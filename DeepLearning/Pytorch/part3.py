# -*- coding: utf-8 -*-
"""Part3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-WZ3VdWmh7d8k9uT49F7NSBKfCVX_emd

### DeepLearning
"""

import torch
from torch import nn as nn
from torch.nn import functional as F
from torchvision import datasets, transforms

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(28*28, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256, 10)
  
  def forward(self, x):
    x = x.view(-1, 28, 28)
    x = self.fc1(x)
    x = F.sigmoid(x)
    x = self.fc2(x)
    x = F.sigmoid(x)
    x = self.fc3(x)
    x = F.log_softmax(x, dim=1)
    return x

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(28*28, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256, 10)
    self.dropout_prob = 0.5
  
  def forward(self, x):
    x = x.view(-1, 28*28)
    x = self.fc1(x)
    x = F.sigmoid(x)
    x = F.dropout(x, training=self.training, p=self.dropout_prob)
    x = self.fc2(x)
    x = F.sigmoid(x)
    x = F.dropout(x, training=self.training, p=self.dropout_prob)
    x = self.fc3(x)
    x = F.log_softmax(x, dim=1)
    return x

BATCH_SIZE = 32

train_dataset = datasets.MNIST(root='../data/MNIST',
                               train=True,
                               download=True,
                               transform=transforms.ToTensor())

test_dataset = datasets.MNIST(root='../data/MNIST',
                              train=False,
                              transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=BATCH_SIZE,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=BATCH_SIZE,
                                          shuffle=False)

def train_loop(dataloader, model, loss_fn, optimizer):
  size = len(dataloader.dataset)
  for batch, (X, y) in enumerate(dataloader):
    pred = model(X)
    loss = loss_fn(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if batch % 100 == 0:
      loss, current = loss.item(), batch * len(X)
      print('loss: {0:.4f}, [{1}/{2}]'.format(loss, current, size))

def test_loop(dataloader, model, loss_fn):
  size = len(dataloader.dataset)
  test_loss, correct = 0, 0

  with torch.no_grad():
    for X, y in dataloader:
      pred = model(X)
      test_loss += loss_fn(pred, y).item()
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()
  
  test_loss /= size
  correct /= size
  print('Test Error: \n Accuracy: {0:.4f}  Avg loss: {1:.4f}'.format(correct, test_loss))

epochs = 30
learning_rate = 0.1

model = Net()
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


for t in range(epochs):
  print('\nEpochs {} \n ==========================='.format(t+1))
  train_loop(train_loader, model, loss_fn, optimizer)
  test_loop(test_loader, model, loss_fn)

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(28*28, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256, 10)
    self.dropout_prob = 0.5
  
  def forward(self, x):
    x = x.view(-1, 28*28)
    x = self.fc1(x)
    x = F.relu(x)
    x = F.dropout(x, training=self.training, p=self.dropout_prob)
    x = self.fc2(x)
    x = F.relu(x)
    x = F.dropout(x, training=self.training, p=self.dropout_prob)
    x = self.fc3(x)
    x = F.log_softmax(x, dim=1)
    return x

epochs = 10
learning_rate = 0.1

model = Net()
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


for t in range(epochs):
  print('\nEpochs {} \n ==========================='.format(t+1))
  train_loop(train_loader, model, loss_fn, optimizer)
  test_loop(test_loader, model, loss_fn)

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(28*28, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256, 10)
    self.dropout_prob = 0.5
    self.batch_norm1 = nn.BatchNorm1d(512)
    self.batch_norm2 = nn.BatchNorm1d(256)
  
  def forward(self, x):
    x = x.view(-1, 28*28)
    x = self.fc1(x)
    x = self.batch_norm1(x)
    x = F.relu(x)
    x = F.dropout(x, training=self.training, p=self.dropout_prob)
    x = self.fc2(x)
    x = self.batch_norm2(x)
    x = F.relu(x)
    x = F.dropout(x, training=self.training, p=self.dropout_prob)
    x = self.fc3(x)
    x = F.log_softmax(x, dim=1)
    return x

epochs = 10
learning_rate = 0.1

model = Net()
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


for t in range(epochs):
  print('\nEpochs {} \n ==========================='.format(t+1))
  train_loop(train_loader, model, loss_fn, optimizer)
  test_loop(test_loader, model, loss_fn)

import torch.nn.init as init

def weight_init(m):
  if isinstance(m, nn.Linear):
    init.kaiming_uniform_(m.weight.data)

model = Net()
model.apply(weight_init)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)
criterion = nn.CrossEntropyLoss()

for t in range(epochs):
  print('\nEpochs {} \n ==========================='.format(t+1))
  train_loop(train_loader, model, loss_fn, optimizer)
  test_loop(test_loader, model, loss_fn)

model = Net()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

for t in range(epochs):
  print('\nEpochs {} \n ==========================='.format(t+1))
  train_loop(train_loader, model, loss_fn, optimizer)
  test_loop(test_loader, model, loss_fn)



"""### AutoEncoder"""

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, datasets

BATCH_SIZE = 32
EPOCHS = 10

train_dataset = datasets.FashionMNIST(
    root='../data/FashionMNIST',
    train=True,
    download=True,
    transform=transforms.ToTensor()
)

test_dataset = datasets.FashionMNIST(
    root='../data/FashionMNIST',
    train=False,
    transform=transforms.ToTensor()
)

train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    dataset=test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False
)

for (X_train, y_train) in train_loader:
  print(X_train.size(), X_train.type())
  print(y_train.size(), y_train.type())
  break

pltsize = 1
plt.figure(figsize=(10 * pltsize, pltsize))

for i in range(10):
  plt.subplot(1, 10, i+1)
  plt.axis('off')
  plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap='gray_r')
  plt.title('class: {}'.format(str(y_train[i].item())))

class AE(nn.Module):
  def __init__(self):
    super(AE, self).__init__()
    
    self.encoder = nn.Sequential(
        nn.Linear(28*28, 512),
        nn.ReLU(),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 32),)
    
    self.decoder = nn.Sequential(
        nn.Linear(32, 256),
        nn.ReLU(),
        nn.Linear(256, 512),
        nn.ReLU(),
        nn.Linear(512, 28*28),)
  
  def forward(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return encoded, decoded

model = AE()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

print(model)

def train(model, train_loader, optimizer, log_interval):
  model.train()
  
  for batch_idx, (image, _) in enumerate(train_loader):
    image = image.view(-1, 28*28)
    target = image.view(-1, 28*28)
    optimizer.zero_grad()
    encoded, decoded = model(image)
    loss = criterion(decoded, target)
    loss.backward()
    optimizer.step()

    if batch_idx % log_interval == 0:
      print('''Train Epoch: {} [{}/{}({:.0f}%)] Train Loss: {:.6f}'''.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))

def evaluate(model, test_loader):
  model.eval()
  test_loss = 0
  real_image = []
  gen_image = []
  
  with torch.no_grad():
    for image, _ in test_loader:
      image = image.view(-1, 28*28)
      target = image.view(-1, 28*28)
      encoded, decoded = model(image)

      test_loss += criterion(decoded, image).item()
      real_image.append(image)
      gen_image.append(decoded)
  
  test_loss /= len(test_loader.dataset)
  return test_loss, real_image, gen_image

for Epoch in range(1, EPOCHS + 1):
  train(model, train_loader, optimizer, log_interval=200)
  test_loss, real_image, gen_image = evaluate(model, test_loader)
  print('\nEPOCH: {0}, \tTest Loss: {1:.4f}'.format(Epoch, test_loss))

  f, a = plt.subplots(2, 10, figsize=(10,4))
  for i in range(10):
    img = np.reshape(real_image[0][i], (28,28))
    a[0][i].imshow(img, cmap='gray_r')
    a[0][i].set_xticks(())
    a[0][i].set_yticks(())
  
  for i in range(10):
    img = np.reshape(gen_image[0][i], (28,28))
    a[1][i].imshow(img, cmap='gray_r')
    a[1][i].set_xticks(())
    a[1][i].set_yticks(())
  
  plt.show()

