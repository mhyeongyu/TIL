{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from numpy.fft import fft, fftshift\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train_features.csv')\n",
    "train_labels=pd.read_csv('./data/train_labels.csv')\n",
    "test=pd.read_csv('./data/test_features.csv')\n",
    "\n",
    "submission=pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "pd.options.display.max_columns=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  가속도, 자이로, (자이로-가속도) 센서값을 에너지로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['acc_Energy']=(train['acc_x']**2+train['acc_y']**2+train['acc_z']**2)**(1/3)\n",
    "test['acc_Energy']=(test['acc_x']**2+test['acc_y']**2+test['acc_z']**2)**(1/3)\n",
    "\n",
    "train['gy_Energy']=(train['gy_x']**2+train['gy_y']**2+train['gy_z']**2)**(1/3)\n",
    "test['gy_Energy']=(test['gy_x']**2+test['gy_y']**2+test['gy_z']**2)**(1/3)\n",
    "\n",
    "train['gy_acc_Energy']=((train['gy_x']-train['acc_x'])**2+(train['gy_y']-train['acc_y'])**2+(train['gy_z']-train['acc_z'])**2)**(1/3)\n",
    "test['gy_acc_Energy']=((test['gy_x']-test['acc_x'])**2+(test['gy_y']-test['acc_y'])**2+(test['gy_z']-test['acc_z'])**2)**(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### id별 데이터는 0.02초마다 측정된 값들이기 때문에 이전 시간 대비 변화량 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.02 \n",
    "def jerk_signal(signal): \n",
    "        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206087</td>\n",
       "      <td>-0.179371</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.591608</td>\n",
       "      <td>-30.549010</td>\n",
       "      <td>-31.676112</td>\n",
       "      <td>1.146962</td>\n",
       "      <td>12.465436</td>\n",
       "      <td>12.427938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287696</td>\n",
       "      <td>-0.198974</td>\n",
       "      <td>-0.182444</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-39.139103</td>\n",
       "      <td>-24.927216</td>\n",
       "      <td>1.200703</td>\n",
       "      <td>12.913284</td>\n",
       "      <td>12.865692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.304609</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.253382</td>\n",
       "      <td>-3.617278</td>\n",
       "      <td>-44.122565</td>\n",
       "      <td>-25.019629</td>\n",
       "      <td>1.217403</td>\n",
       "      <td>13.725729</td>\n",
       "      <td>13.692643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.293095</td>\n",
       "      <td>-0.230366</td>\n",
       "      <td>-0.215210</td>\n",
       "      <td>2.712986</td>\n",
       "      <td>-53.597843</td>\n",
       "      <td>-27.454013</td>\n",
       "      <td>1.209981</td>\n",
       "      <td>15.374021</td>\n",
       "      <td>15.314907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.300887</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>-0.222523</td>\n",
       "      <td>4.286707</td>\n",
       "      <td>-57.906561</td>\n",
       "      <td>-27.961234</td>\n",
       "      <td>1.211254</td>\n",
       "      <td>16.074363</td>\n",
       "      <td>16.017964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>0.985242</td>\n",
       "      <td>-0.326122</td>\n",
       "      <td>-0.354528</td>\n",
       "      <td>-14.903280</td>\n",
       "      <td>20.172339</td>\n",
       "      <td>22.973018</td>\n",
       "      <td>1.063469</td>\n",
       "      <td>10.497476</td>\n",
       "      <td>10.675966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>1.052837</td>\n",
       "      <td>-0.220710</td>\n",
       "      <td>-0.413472</td>\n",
       "      <td>-10.857025</td>\n",
       "      <td>19.786856</td>\n",
       "      <td>23.174597</td>\n",
       "      <td>1.099211</td>\n",
       "      <td>10.152517</td>\n",
       "      <td>10.318246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0</td>\n",
       "      <td>597</td>\n",
       "      <td>1.025643</td>\n",
       "      <td>-0.227845</td>\n",
       "      <td>-0.354516</td>\n",
       "      <td>-2.334243</td>\n",
       "      <td>25.768654</td>\n",
       "      <td>18.932070</td>\n",
       "      <td>1.071307</td>\n",
       "      <td>10.092134</td>\n",
       "      <td>10.193175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>1.031553</td>\n",
       "      <td>-0.387862</td>\n",
       "      <td>-0.277857</td>\n",
       "      <td>-9.710746</td>\n",
       "      <td>28.697694</td>\n",
       "      <td>20.631577</td>\n",
       "      <td>1.089077</td>\n",
       "      <td>11.034378</td>\n",
       "      <td>11.183082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>599</td>\n",
       "      <td>1.138159</td>\n",
       "      <td>-0.426846</td>\n",
       "      <td>-0.430263</td>\n",
       "      <td>-15.891015</td>\n",
       "      <td>21.675950</td>\n",
       "      <td>32.123007</td>\n",
       "      <td>1.184697</td>\n",
       "      <td>12.060479</td>\n",
       "      <td>12.249947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  time     acc_x     acc_y     acc_z       gy_x       gy_y       gy_z  \\\n",
       "0     0     0  1.206087 -0.179371 -0.148447  -0.591608 -30.549010 -31.676112   \n",
       "1     0     1  1.287696 -0.198974 -0.182444   0.303100 -39.139103 -24.927216   \n",
       "2     0     2  1.304609 -0.195114 -0.253382  -3.617278 -44.122565 -25.019629   \n",
       "3     0     3  1.293095 -0.230366 -0.215210   2.712986 -53.597843 -27.454013   \n",
       "4     0     4  1.300887 -0.187757 -0.222523   4.286707 -57.906561 -27.961234   \n",
       "..   ..   ...       ...       ...       ...        ...        ...        ...   \n",
       "595   0   595  0.985242 -0.326122 -0.354528 -14.903280  20.172339  22.973018   \n",
       "596   0   596  1.052837 -0.220710 -0.413472 -10.857025  19.786856  23.174597   \n",
       "597   0   597  1.025643 -0.227845 -0.354516  -2.334243  25.768654  18.932070   \n",
       "598   0   598  1.031553 -0.387862 -0.277857  -9.710746  28.697694  20.631577   \n",
       "599   0   599  1.138159 -0.426846 -0.430263 -15.891015  21.675950  32.123007   \n",
       "\n",
       "     acc_Energy  gy_Energy  gy_acc_Energy  \n",
       "0      1.146962  12.465436      12.427938  \n",
       "1      1.200703  12.913284      12.865692  \n",
       "2      1.217403  13.725729      13.692643  \n",
       "3      1.209981  15.374021      15.314907  \n",
       "4      1.211254  16.074363      16.017964  \n",
       "..          ...        ...            ...  \n",
       "595    1.063469  10.497476      10.675966  \n",
       "596    1.099211  10.152517      10.318246  \n",
       "597    1.071307  10.092134      10.193175  \n",
       "598    1.089077  11.034378      11.183082  \n",
       "599    1.184697  12.060479      12.249947  \n",
       "\n",
       "[600 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.        ,   1.20608659,  -0.17937144,\n",
       "        -0.14844666,  -0.59160785, -30.54900952, -31.67611187,\n",
       "         1.14696246,  12.46543635,  12.427938  ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   5.        ,   0.40804946,  -0.09801141,\n",
       "        -0.16998541,   4.4735403 , -42.95046766,  33.74447932,\n",
       "         0.26870238,   2.23923579,   2.1887693 ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(temp.values[1] - temp.values[0]) / 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  4.08049463, -0.98011409, ...,  4.78098086,\n",
       "       51.30505283, 53.34325901])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.loc[train['id'] == 0]\n",
    "temp = train.loc[train['id'] == 0]\n",
    "\n",
    "# train.columns[2:]\n",
    "v = train.columns[2:]\n",
    "\n",
    "jerk_signal(temp[v].values)\n",
    "values = jerk_signal(temp[v].values)\n",
    "np.insert(values,0,0)      #0번 index에 0추가 후 1차원 array변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jerk 가속도변화의 변화량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:58<00:00, 53.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dt=[]\n",
    "for i in tqdm(train['id'].unique()):\n",
    "    temp=train.loc[train['id']==i]\n",
    "    for v in train.columns[2:]:\n",
    "        values=jerk_signal(temp[v].values)\n",
    "        values=np.insert(values,0,0)\n",
    "        temp.loc[:,v+'_dt']=values\n",
    "    train_dt.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "      <th>acc_x_dt</th>\n",
       "      <th>acc_y_dt</th>\n",
       "      <th>acc_z_dt</th>\n",
       "      <th>gy_x_dt</th>\n",
       "      <th>gy_y_dt</th>\n",
       "      <th>gy_z_dt</th>\n",
       "      <th>acc_Energy_dt</th>\n",
       "      <th>gy_Energy_dt</th>\n",
       "      <th>gy_acc_Energy_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206087</td>\n",
       "      <td>-0.179371</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.591608</td>\n",
       "      <td>-30.549010</td>\n",
       "      <td>-31.676112</td>\n",
       "      <td>1.146962</td>\n",
       "      <td>12.465436</td>\n",
       "      <td>12.427938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287696</td>\n",
       "      <td>-0.198974</td>\n",
       "      <td>-0.182444</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-39.139103</td>\n",
       "      <td>-24.927216</td>\n",
       "      <td>1.200703</td>\n",
       "      <td>12.913284</td>\n",
       "      <td>12.865692</td>\n",
       "      <td>4.080495</td>\n",
       "      <td>-0.980114</td>\n",
       "      <td>-1.699854</td>\n",
       "      <td>44.735403</td>\n",
       "      <td>-429.504677</td>\n",
       "      <td>337.444793</td>\n",
       "      <td>2.687024</td>\n",
       "      <td>22.392358</td>\n",
       "      <td>21.887693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.304609</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.253382</td>\n",
       "      <td>-3.617278</td>\n",
       "      <td>-44.122565</td>\n",
       "      <td>-25.019629</td>\n",
       "      <td>1.217403</td>\n",
       "      <td>13.725729</td>\n",
       "      <td>13.692643</td>\n",
       "      <td>0.845632</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>-3.546937</td>\n",
       "      <td>-196.018888</td>\n",
       "      <td>-249.173073</td>\n",
       "      <td>-4.620631</td>\n",
       "      <td>0.835012</td>\n",
       "      <td>40.622253</td>\n",
       "      <td>41.347563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.293095</td>\n",
       "      <td>-0.230366</td>\n",
       "      <td>-0.215210</td>\n",
       "      <td>2.712986</td>\n",
       "      <td>-53.597843</td>\n",
       "      <td>-27.454013</td>\n",
       "      <td>1.209981</td>\n",
       "      <td>15.374021</td>\n",
       "      <td>15.314907</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-1.762585</td>\n",
       "      <td>1.908626</td>\n",
       "      <td>316.513181</td>\n",
       "      <td>-473.763910</td>\n",
       "      <td>-121.719195</td>\n",
       "      <td>-0.371100</td>\n",
       "      <td>82.414636</td>\n",
       "      <td>81.113199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.300887</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>-0.222523</td>\n",
       "      <td>4.286707</td>\n",
       "      <td>-57.906561</td>\n",
       "      <td>-27.961234</td>\n",
       "      <td>1.211254</td>\n",
       "      <td>16.074363</td>\n",
       "      <td>16.017964</td>\n",
       "      <td>0.389598</td>\n",
       "      <td>2.130453</td>\n",
       "      <td>-0.365665</td>\n",
       "      <td>78.686055</td>\n",
       "      <td>-215.435892</td>\n",
       "      <td>-25.361098</td>\n",
       "      <td>0.063656</td>\n",
       "      <td>35.017060</td>\n",
       "      <td>35.152822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>0.985242</td>\n",
       "      <td>-0.326122</td>\n",
       "      <td>-0.354528</td>\n",
       "      <td>-14.903280</td>\n",
       "      <td>20.172339</td>\n",
       "      <td>22.973018</td>\n",
       "      <td>1.063469</td>\n",
       "      <td>10.497476</td>\n",
       "      <td>10.675966</td>\n",
       "      <td>1.177958</td>\n",
       "      <td>-4.242820</td>\n",
       "      <td>-0.324471</td>\n",
       "      <td>-220.368219</td>\n",
       "      <td>-286.214412</td>\n",
       "      <td>340.765651</td>\n",
       "      <td>1.494244</td>\n",
       "      <td>17.990525</td>\n",
       "      <td>19.714449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>1.052837</td>\n",
       "      <td>-0.220710</td>\n",
       "      <td>-0.413472</td>\n",
       "      <td>-10.857025</td>\n",
       "      <td>19.786856</td>\n",
       "      <td>23.174597</td>\n",
       "      <td>1.099211</td>\n",
       "      <td>10.152517</td>\n",
       "      <td>10.318246</td>\n",
       "      <td>3.379771</td>\n",
       "      <td>5.270607</td>\n",
       "      <td>-2.947208</td>\n",
       "      <td>202.312749</td>\n",
       "      <td>-19.274131</td>\n",
       "      <td>10.078975</td>\n",
       "      <td>1.787111</td>\n",
       "      <td>-17.247953</td>\n",
       "      <td>-17.885963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0</td>\n",
       "      <td>597</td>\n",
       "      <td>1.025643</td>\n",
       "      <td>-0.227845</td>\n",
       "      <td>-0.354516</td>\n",
       "      <td>-2.334243</td>\n",
       "      <td>25.768654</td>\n",
       "      <td>18.932070</td>\n",
       "      <td>1.071307</td>\n",
       "      <td>10.092134</td>\n",
       "      <td>10.193175</td>\n",
       "      <td>-1.359731</td>\n",
       "      <td>-0.356764</td>\n",
       "      <td>2.947780</td>\n",
       "      <td>426.139108</td>\n",
       "      <td>299.089890</td>\n",
       "      <td>-212.126333</td>\n",
       "      <td>-1.395196</td>\n",
       "      <td>-3.019192</td>\n",
       "      <td>-6.253557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>1.031553</td>\n",
       "      <td>-0.387862</td>\n",
       "      <td>-0.277857</td>\n",
       "      <td>-9.710746</td>\n",
       "      <td>28.697694</td>\n",
       "      <td>20.631577</td>\n",
       "      <td>1.089077</td>\n",
       "      <td>11.034378</td>\n",
       "      <td>11.183082</td>\n",
       "      <td>0.295512</td>\n",
       "      <td>-8.000850</td>\n",
       "      <td>3.832979</td>\n",
       "      <td>-368.825182</td>\n",
       "      <td>146.451989</td>\n",
       "      <td>84.975330</td>\n",
       "      <td>0.888507</td>\n",
       "      <td>47.112213</td>\n",
       "      <td>49.495340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>599</td>\n",
       "      <td>1.138159</td>\n",
       "      <td>-0.426846</td>\n",
       "      <td>-0.430263</td>\n",
       "      <td>-15.891015</td>\n",
       "      <td>21.675950</td>\n",
       "      <td>32.123007</td>\n",
       "      <td>1.184697</td>\n",
       "      <td>12.060479</td>\n",
       "      <td>12.249947</td>\n",
       "      <td>5.330310</td>\n",
       "      <td>-1.949185</td>\n",
       "      <td>-7.620324</td>\n",
       "      <td>-309.013460</td>\n",
       "      <td>-351.087173</td>\n",
       "      <td>574.571503</td>\n",
       "      <td>4.780981</td>\n",
       "      <td>51.305053</td>\n",
       "      <td>53.343259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  time     acc_x     acc_y     acc_z       gy_x       gy_y       gy_z  \\\n",
       "0     0     0  1.206087 -0.179371 -0.148447  -0.591608 -30.549010 -31.676112   \n",
       "1     0     1  1.287696 -0.198974 -0.182444   0.303100 -39.139103 -24.927216   \n",
       "2     0     2  1.304609 -0.195114 -0.253382  -3.617278 -44.122565 -25.019629   \n",
       "3     0     3  1.293095 -0.230366 -0.215210   2.712986 -53.597843 -27.454013   \n",
       "4     0     4  1.300887 -0.187757 -0.222523   4.286707 -57.906561 -27.961234   \n",
       "..   ..   ...       ...       ...       ...        ...        ...        ...   \n",
       "595   0   595  0.985242 -0.326122 -0.354528 -14.903280  20.172339  22.973018   \n",
       "596   0   596  1.052837 -0.220710 -0.413472 -10.857025  19.786856  23.174597   \n",
       "597   0   597  1.025643 -0.227845 -0.354516  -2.334243  25.768654  18.932070   \n",
       "598   0   598  1.031553 -0.387862 -0.277857  -9.710746  28.697694  20.631577   \n",
       "599   0   599  1.138159 -0.426846 -0.430263 -15.891015  21.675950  32.123007   \n",
       "\n",
       "     acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  acc_z_dt  \\\n",
       "0      1.146962  12.465436      12.427938  0.000000  0.000000  0.000000   \n",
       "1      1.200703  12.913284      12.865692  4.080495 -0.980114 -1.699854   \n",
       "2      1.217403  13.725729      13.692643  0.845632  0.192961 -3.546937   \n",
       "3      1.209981  15.374021      15.314907 -0.575711 -1.762585  1.908626   \n",
       "4      1.211254  16.074363      16.017964  0.389598  2.130453 -0.365665   \n",
       "..          ...        ...            ...       ...       ...       ...   \n",
       "595    1.063469  10.497476      10.675966  1.177958 -4.242820 -0.324471   \n",
       "596    1.099211  10.152517      10.318246  3.379771  5.270607 -2.947208   \n",
       "597    1.071307  10.092134      10.193175 -1.359731 -0.356764  2.947780   \n",
       "598    1.089077  11.034378      11.183082  0.295512 -8.000850  3.832979   \n",
       "599    1.184697  12.060479      12.249947  5.330310 -1.949185 -7.620324   \n",
       "\n",
       "        gy_x_dt     gy_y_dt     gy_z_dt  acc_Energy_dt  gy_Energy_dt  \\\n",
       "0      0.000000    0.000000    0.000000       0.000000      0.000000   \n",
       "1     44.735403 -429.504677  337.444793       2.687024     22.392358   \n",
       "2   -196.018888 -249.173073   -4.620631       0.835012     40.622253   \n",
       "3    316.513181 -473.763910 -121.719195      -0.371100     82.414636   \n",
       "4     78.686055 -215.435892  -25.361098       0.063656     35.017060   \n",
       "..          ...         ...         ...            ...           ...   \n",
       "595 -220.368219 -286.214412  340.765651       1.494244     17.990525   \n",
       "596  202.312749  -19.274131   10.078975       1.787111    -17.247953   \n",
       "597  426.139108  299.089890 -212.126333      -1.395196     -3.019192   \n",
       "598 -368.825182  146.451989   84.975330       0.888507     47.112213   \n",
       "599 -309.013460 -351.087173  574.571503       4.780981     51.305053   \n",
       "\n",
       "     gy_acc_Energy_dt  \n",
       "0            0.000000  \n",
       "1           21.887693  \n",
       "2           41.347563  \n",
       "3           81.113199  \n",
       "4           35.152822  \n",
       "..                ...  \n",
       "595         19.714449  \n",
       "596        -17.885963  \n",
       "597         -6.253557  \n",
       "598         49.495340  \n",
       "599         53.343259  \n",
       "\n",
       "[600 rows x 20 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 70.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_dt=[]\n",
    "# for i in tqdm(test['id'].unique()):\n",
    "#     temp=test.loc[test['id']==i]\n",
    "#     for v in train.columns[2:]:\n",
    "#         values=jerk_signal(temp[v].values)\n",
    "#         values=np.insert(values,0,0)\n",
    "#         temp.loc[:,v+'_dt']=values\n",
    "#     test_dt.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가속도, 자이로 센서값들을 푸리에 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "from numpy.fft import *\n",
    "\n",
    "def fourier_transform_one_signal(t_signal):\n",
    "    complex_f_signal= fftpack.fft(t_signal)\n",
    "    amplitude_f_signal=np.abs(complex_f_signal)\n",
    "    return amplitude_f_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat(train_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "      <th>acc_x_dt</th>\n",
       "      <th>acc_y_dt</th>\n",
       "      <th>acc_z_dt</th>\n",
       "      <th>gy_x_dt</th>\n",
       "      <th>gy_y_dt</th>\n",
       "      <th>gy_z_dt</th>\n",
       "      <th>acc_Energy_dt</th>\n",
       "      <th>gy_Energy_dt</th>\n",
       "      <th>gy_acc_Energy_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206087</td>\n",
       "      <td>-0.179371</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.591608</td>\n",
       "      <td>-30.549010</td>\n",
       "      <td>-31.676112</td>\n",
       "      <td>1.146962</td>\n",
       "      <td>12.465436</td>\n",
       "      <td>12.427938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287696</td>\n",
       "      <td>-0.198974</td>\n",
       "      <td>-0.182444</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-39.139103</td>\n",
       "      <td>-24.927216</td>\n",
       "      <td>1.200703</td>\n",
       "      <td>12.913284</td>\n",
       "      <td>12.865692</td>\n",
       "      <td>4.080495</td>\n",
       "      <td>-0.980114</td>\n",
       "      <td>-1.699854</td>\n",
       "      <td>44.735403</td>\n",
       "      <td>-429.504677</td>\n",
       "      <td>337.444793</td>\n",
       "      <td>2.687024</td>\n",
       "      <td>22.392358</td>\n",
       "      <td>21.887693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.304609</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.253382</td>\n",
       "      <td>-3.617278</td>\n",
       "      <td>-44.122565</td>\n",
       "      <td>-25.019629</td>\n",
       "      <td>1.217403</td>\n",
       "      <td>13.725729</td>\n",
       "      <td>13.692643</td>\n",
       "      <td>0.845632</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>-3.546937</td>\n",
       "      <td>-196.018888</td>\n",
       "      <td>-249.173073</td>\n",
       "      <td>-4.620631</td>\n",
       "      <td>0.835012</td>\n",
       "      <td>40.622253</td>\n",
       "      <td>41.347563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.293095</td>\n",
       "      <td>-0.230366</td>\n",
       "      <td>-0.215210</td>\n",
       "      <td>2.712986</td>\n",
       "      <td>-53.597843</td>\n",
       "      <td>-27.454013</td>\n",
       "      <td>1.209981</td>\n",
       "      <td>15.374021</td>\n",
       "      <td>15.314907</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-1.762585</td>\n",
       "      <td>1.908626</td>\n",
       "      <td>316.513181</td>\n",
       "      <td>-473.763910</td>\n",
       "      <td>-121.719195</td>\n",
       "      <td>-0.371100</td>\n",
       "      <td>82.414636</td>\n",
       "      <td>81.113199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.300887</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>-0.222523</td>\n",
       "      <td>4.286707</td>\n",
       "      <td>-57.906561</td>\n",
       "      <td>-27.961234</td>\n",
       "      <td>1.211254</td>\n",
       "      <td>16.074363</td>\n",
       "      <td>16.017964</td>\n",
       "      <td>0.389598</td>\n",
       "      <td>2.130453</td>\n",
       "      <td>-0.365665</td>\n",
       "      <td>78.686055</td>\n",
       "      <td>-215.435892</td>\n",
       "      <td>-25.361098</td>\n",
       "      <td>0.063656</td>\n",
       "      <td>35.017060</td>\n",
       "      <td>35.152822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874995</th>\n",
       "      <td>3124</td>\n",
       "      <td>595</td>\n",
       "      <td>-0.712530</td>\n",
       "      <td>-0.658357</td>\n",
       "      <td>0.293707</td>\n",
       "      <td>-29.367857</td>\n",
       "      <td>-104.013664</td>\n",
       "      <td>-76.290437</td>\n",
       "      <td>1.009050</td>\n",
       "      <td>25.963234</td>\n",
       "      <td>25.897316</td>\n",
       "      <td>1.484646</td>\n",
       "      <td>0.303666</td>\n",
       "      <td>0.800069</td>\n",
       "      <td>-150.644663</td>\n",
       "      <td>-34.630282</td>\n",
       "      <td>-8.380088</td>\n",
       "      <td>-0.679712</td>\n",
       "      <td>8.387109</td>\n",
       "      <td>8.432977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874996</th>\n",
       "      <td>3124</td>\n",
       "      <td>596</td>\n",
       "      <td>-0.683037</td>\n",
       "      <td>-0.658466</td>\n",
       "      <td>0.329223</td>\n",
       "      <td>-30.149089</td>\n",
       "      <td>-101.796809</td>\n",
       "      <td>-76.625087</td>\n",
       "      <td>1.002827</td>\n",
       "      <td>25.784692</td>\n",
       "      <td>25.722482</td>\n",
       "      <td>1.474659</td>\n",
       "      <td>-0.005442</td>\n",
       "      <td>1.775771</td>\n",
       "      <td>-39.061611</td>\n",
       "      <td>110.842743</td>\n",
       "      <td>-16.732496</td>\n",
       "      <td>-0.311171</td>\n",
       "      <td>-8.927089</td>\n",
       "      <td>-8.741727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874997</th>\n",
       "      <td>3124</td>\n",
       "      <td>597</td>\n",
       "      <td>-0.664730</td>\n",
       "      <td>-0.666625</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>-27.873095</td>\n",
       "      <td>-98.776072</td>\n",
       "      <td>-79.365125</td>\n",
       "      <td>1.006239</td>\n",
       "      <td>25.628060</td>\n",
       "      <td>25.572145</td>\n",
       "      <td>0.915321</td>\n",
       "      <td>-0.407957</td>\n",
       "      <td>1.744566</td>\n",
       "      <td>113.799702</td>\n",
       "      <td>151.036858</td>\n",
       "      <td>-137.001896</td>\n",
       "      <td>0.170620</td>\n",
       "      <td>-7.831611</td>\n",
       "      <td>-7.516832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874998</th>\n",
       "      <td>3124</td>\n",
       "      <td>598</td>\n",
       "      <td>-0.630534</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.373696</td>\n",
       "      <td>-23.636550</td>\n",
       "      <td>-99.139495</td>\n",
       "      <td>-80.259478</td>\n",
       "      <td>1.001038</td>\n",
       "      <td>25.626266</td>\n",
       "      <td>25.573288</td>\n",
       "      <td>1.709833</td>\n",
       "      <td>-0.796984</td>\n",
       "      <td>0.479107</td>\n",
       "      <td>211.827245</td>\n",
       "      <td>-18.171144</td>\n",
       "      <td>-44.717652</td>\n",
       "      <td>-0.260074</td>\n",
       "      <td>-0.089713</td>\n",
       "      <td>0.057150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874999</th>\n",
       "      <td>3124</td>\n",
       "      <td>599</td>\n",
       "      <td>-0.578351</td>\n",
       "      <td>-0.700235</td>\n",
       "      <td>0.384390</td>\n",
       "      <td>-17.917626</td>\n",
       "      <td>-100.181873</td>\n",
       "      <td>-80.676229</td>\n",
       "      <td>0.990773</td>\n",
       "      <td>25.645131</td>\n",
       "      <td>25.595348</td>\n",
       "      <td>2.609116</td>\n",
       "      <td>-0.883512</td>\n",
       "      <td>0.534668</td>\n",
       "      <td>285.946217</td>\n",
       "      <td>-52.118894</td>\n",
       "      <td>-20.837588</td>\n",
       "      <td>-0.513215</td>\n",
       "      <td>0.943240</td>\n",
       "      <td>1.102985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1875000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  time     acc_x     acc_y     acc_z       gy_x        gy_y  \\\n",
       "0           0     0  1.206087 -0.179371 -0.148447  -0.591608  -30.549010   \n",
       "1           0     1  1.287696 -0.198974 -0.182444   0.303100  -39.139103   \n",
       "2           0     2  1.304609 -0.195114 -0.253382  -3.617278  -44.122565   \n",
       "3           0     3  1.293095 -0.230366 -0.215210   2.712986  -53.597843   \n",
       "4           0     4  1.300887 -0.187757 -0.222523   4.286707  -57.906561   \n",
       "...       ...   ...       ...       ...       ...        ...         ...   \n",
       "1874995  3124   595 -0.712530 -0.658357  0.293707 -29.367857 -104.013664   \n",
       "1874996  3124   596 -0.683037 -0.658466  0.329223 -30.149089 -101.796809   \n",
       "1874997  3124   597 -0.664730 -0.666625  0.364114 -27.873095  -98.776072   \n",
       "1874998  3124   598 -0.630534 -0.682565  0.373696 -23.636550  -99.139495   \n",
       "1874999  3124   599 -0.578351 -0.700235  0.384390 -17.917626 -100.181873   \n",
       "\n",
       "              gy_z  acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  \\\n",
       "0       -31.676112    1.146962  12.465436      12.427938  0.000000  0.000000   \n",
       "1       -24.927216    1.200703  12.913284      12.865692  4.080495 -0.980114   \n",
       "2       -25.019629    1.217403  13.725729      13.692643  0.845632  0.192961   \n",
       "3       -27.454013    1.209981  15.374021      15.314907 -0.575711 -1.762585   \n",
       "4       -27.961234    1.211254  16.074363      16.017964  0.389598  2.130453   \n",
       "...            ...         ...        ...            ...       ...       ...   \n",
       "1874995 -76.290437    1.009050  25.963234      25.897316  1.484646  0.303666   \n",
       "1874996 -76.625087    1.002827  25.784692      25.722482  1.474659 -0.005442   \n",
       "1874997 -79.365125    1.006239  25.628060      25.572145  0.915321 -0.407957   \n",
       "1874998 -80.259478    1.001038  25.626266      25.573288  1.709833 -0.796984   \n",
       "1874999 -80.676229    0.990773  25.645131      25.595348  2.609116 -0.883512   \n",
       "\n",
       "         acc_z_dt     gy_x_dt     gy_y_dt     gy_z_dt  acc_Energy_dt  \\\n",
       "0        0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "1       -1.699854   44.735403 -429.504677  337.444793       2.687024   \n",
       "2       -3.546937 -196.018888 -249.173073   -4.620631       0.835012   \n",
       "3        1.908626  316.513181 -473.763910 -121.719195      -0.371100   \n",
       "4       -0.365665   78.686055 -215.435892  -25.361098       0.063656   \n",
       "...           ...         ...         ...         ...            ...   \n",
       "1874995  0.800069 -150.644663  -34.630282   -8.380088      -0.679712   \n",
       "1874996  1.775771  -39.061611  110.842743  -16.732496      -0.311171   \n",
       "1874997  1.744566  113.799702  151.036858 -137.001896       0.170620   \n",
       "1874998  0.479107  211.827245  -18.171144  -44.717652      -0.260074   \n",
       "1874999  0.534668  285.946217  -52.118894  -20.837588      -0.513215   \n",
       "\n",
       "         gy_Energy_dt  gy_acc_Energy_dt  \n",
       "0            0.000000          0.000000  \n",
       "1           22.392358         21.887693  \n",
       "2           40.622253         41.347563  \n",
       "3           82.414636         81.113199  \n",
       "4           35.017060         35.152822  \n",
       "...               ...               ...  \n",
       "1874995      8.387109          8.432977  \n",
       "1874996     -8.927089         -8.741727  \n",
       "1874997     -7.831611         -7.516832  \n",
       "1874998     -0.089713          0.057150  \n",
       "1874999      0.943240          1.102985  \n",
       "\n",
       "[1875000 rows x 20 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.59948117e+02, 9.25608169e+01, 8.24514477e+01, 7.47521477e+01,\n",
       "       1.25984057e+01, 2.32009942e+01, 3.38340045e+01, 3.57384381e+01,\n",
       "       3.25181599e+01, 2.72695878e+01, 1.96043134e+01, 2.83065024e+01,\n",
       "       7.10484054e+00, 4.64911870e+01, 1.22654171e+01, 4.03864326e+01,\n",
       "       5.48711520e+01, 3.58434966e+01, 4.01096484e+01, 8.13353642e+00,\n",
       "       2.26283263e+01, 1.50267859e+01, 5.73678985e+00, 6.75854211e+00,\n",
       "       4.44425347e+00, 1.98479809e+01, 3.12689056e+01, 1.97452495e+01,\n",
       "       2.45438786e+00, 1.87215449e+01, 1.87444878e+01, 8.05689547e+00,\n",
       "       1.24566504e+01, 1.45357274e+01, 5.90177056e+00, 4.41989314e+00,\n",
       "       1.31966168e+01, 7.55829445e+00, 1.43144487e+01, 6.82121259e+00,\n",
       "       8.31921374e+00, 1.54774677e+01, 6.33143922e+00, 8.58947597e+00,\n",
       "       5.85265922e+00, 5.56930207e+00, 2.05894082e+00, 3.91629216e+00,\n",
       "       3.86496940e+00, 4.19078597e+00, 1.33517384e+01, 1.12939306e+01,\n",
       "       5.07096790e+00, 5.26580172e+00, 2.28906958e+00, 4.35318126e+00,\n",
       "       3.44241696e+00, 2.72646479e+00, 4.99365178e+00, 6.84868009e+00,\n",
       "       5.37806692e+00, 6.41972484e+00, 5.73106323e+00, 4.64306658e+00,\n",
       "       5.21258630e+00, 7.01501634e+00, 6.33189730e+00, 1.06581133e+01,\n",
       "       3.05764819e+00, 5.40455474e+00, 5.68597232e+00, 4.22793681e+00,\n",
       "       9.38367297e+00, 4.52698336e+00, 4.94675838e+00, 5.38993801e-01,\n",
       "       4.82866632e+00, 4.29290953e+00, 6.81156068e+00, 4.23113603e+00,\n",
       "       3.30564494e+00, 8.62354107e+00, 6.65403181e+00, 5.90603645e+00,\n",
       "       4.47806589e+00, 6.64896533e+00, 4.38695831e+00, 3.99117050e+00,\n",
       "       4.05805385e+00, 4.01369190e+00, 1.29852019e+00, 2.82719218e+00,\n",
       "       1.68632656e+00, 2.41367782e-01, 1.77591586e+00, 3.40271907e+00,\n",
       "       5.36530548e+00, 2.01246863e+00, 3.59333840e+00, 5.03165202e+00,\n",
       "       3.00935970e+00, 7.82674174e+00, 5.04976796e+00, 5.45904067e+00,\n",
       "       3.84715252e+00, 4.95092731e+00, 3.07150917e+00, 4.60951172e+00,\n",
       "       2.89225948e+00, 4.51074093e+00, 4.24975903e+00, 1.68615796e+00,\n",
       "       2.79852495e+00, 6.13132162e-01, 2.99205780e+00, 4.99317379e+00,\n",
       "       2.46974517e+00, 6.55862651e+00, 5.44436817e+00, 6.24616347e+00,\n",
       "       5.07384986e+00, 2.84441198e+00, 5.52846461e+00, 2.98769859e+00,\n",
       "       4.19134950e+00, 3.75718817e+00, 3.78923347e+00, 1.81798894e+00,\n",
       "       2.95776180e+00, 1.55720587e+00, 1.73165593e+00, 1.76683147e+00,\n",
       "       1.83160117e+00, 2.61899143e+00, 3.40462321e+00, 4.10608716e+00,\n",
       "       2.51281742e+00, 3.49748489e+00, 2.98690164e+00, 3.20009524e+00,\n",
       "       2.29625810e+00, 2.92333291e+00, 2.59206058e+00, 2.01727543e+00,\n",
       "       3.37925939e+00, 9.13373712e-01, 1.78099293e+00, 1.77136257e+00,\n",
       "       9.82138658e-01, 1.63264711e+00, 1.42301469e+00, 1.31890217e+00,\n",
       "       1.05686034e+00, 1.19189613e+00, 2.20526217e+00, 9.93788380e-01,\n",
       "       1.04283292e+00, 2.04519459e+00, 7.60708578e-01, 2.22474459e-01,\n",
       "       1.31205179e+00, 2.38788395e-01, 8.50328601e-01, 1.10536254e-01,\n",
       "       1.76066819e+00, 6.43137562e-01, 7.03500181e-01, 5.88984185e-01,\n",
       "       7.90015648e-01, 1.75936655e+00, 6.67856459e-01, 1.07720489e+00,\n",
       "       5.55003581e-01, 1.09444431e+00, 9.90555086e-01, 1.53642131e+00,\n",
       "       3.46440460e-01, 1.11426372e+00, 1.10276095e+00, 8.15049247e-01,\n",
       "       7.82870603e-01, 7.57925695e-01, 4.03574097e-01, 4.97499835e-01,\n",
       "       6.18229764e-01, 7.66756245e-01, 2.66288536e-01, 1.07221209e+00,\n",
       "       1.02424065e+00, 8.62055324e-01, 1.24570622e+00, 1.95113708e-01,\n",
       "       9.48396701e-01, 1.40674714e+00, 8.38169875e-01, 9.99038279e-01,\n",
       "       1.10524269e+00, 7.47566808e-01, 1.15597400e+00, 4.81323862e-01,\n",
       "       4.87408942e-01, 8.83901873e-01, 5.39141901e-01, 7.00932590e-01,\n",
       "       5.30087041e-01, 2.36413790e-01, 7.20296278e-01, 5.13178533e-02,\n",
       "       3.06410969e-01, 6.35637363e-01, 8.87819562e-02, 7.53990336e-01,\n",
       "       3.95329640e-01, 1.72879081e-01, 7.94284841e-01, 2.39146665e-01,\n",
       "       7.74208323e-01, 3.88147740e-01, 5.11682006e-01, 4.26698928e-01,\n",
       "       5.11961516e-01, 2.59837951e-01, 7.44833197e-02, 4.40725960e-01,\n",
       "       2.67768735e-01, 4.79888184e-02, 2.95772516e-01, 4.90706134e-02,\n",
       "       5.54778129e-01, 4.68052616e-01, 1.67780632e-01, 1.92507759e-01,\n",
       "       1.19165688e-01, 4.12804889e-01, 1.43014380e-01, 4.72577931e-01,\n",
       "       3.89139362e-01, 3.56244253e-01, 6.80165411e-01, 3.24397724e-01,\n",
       "       4.10781946e-01, 4.56456180e-01, 3.86770175e-01, 4.50560795e-01,\n",
       "       3.60657718e-01, 4.40448096e-01, 4.58038664e-01, 3.86422610e-01,\n",
       "       1.84008675e-01, 5.07944415e-01, 3.52520520e-01, 4.65042873e-01,\n",
       "       5.84472287e-01, 3.46381583e-01, 5.75061905e-01, 4.51040230e-01,\n",
       "       1.65083825e-01, 4.66510049e-01, 3.57886710e-01, 2.47365004e-01,\n",
       "       3.74901992e-01, 2.47005582e-01, 4.94010186e-01, 2.13135705e-01,\n",
       "       1.68180859e-01, 2.78481703e-01, 2.46066996e-01, 3.58269733e-01,\n",
       "       1.75485399e-01, 3.60459220e-01, 2.64530927e-01, 2.29385101e-01,\n",
       "       2.32304289e-01, 1.20272646e-01, 2.71005878e-01, 1.05673920e-01,\n",
       "       3.61307525e-01, 1.46023493e-01, 2.32641814e-01, 1.52407020e-01,\n",
       "       1.82070612e-01, 9.24013729e-02, 2.41708625e-01, 1.55355539e-01,\n",
       "       2.73734292e-02, 1.64607222e-01, 3.59862694e-02, 1.73012582e-01,\n",
       "       1.54203165e-01, 3.09630924e-01, 1.96506456e-01, 1.74951943e-01,\n",
       "       1.27161701e-01, 1.95756233e-01, 1.50729283e-01, 1.43899896e-01,\n",
       "       3.18299839e-01, 1.78915599e-01, 1.28414327e-01, 1.66988905e-01,\n",
       "       2.72652189e-02, 1.66988905e-01, 1.28414327e-01, 1.78915599e-01,\n",
       "       3.18299839e-01, 1.43899896e-01, 1.50729283e-01, 1.95756233e-01,\n",
       "       1.27161701e-01, 1.74951943e-01, 1.96506456e-01, 3.09630924e-01,\n",
       "       1.54203165e-01, 1.73012582e-01, 3.59862694e-02, 1.64607222e-01,\n",
       "       2.73734292e-02, 1.55355539e-01, 2.41708625e-01, 9.24013729e-02,\n",
       "       1.82070612e-01, 1.52407020e-01, 2.32641814e-01, 1.46023493e-01,\n",
       "       3.61307525e-01, 1.05673920e-01, 2.71005878e-01, 1.20272646e-01,\n",
       "       2.32304289e-01, 2.29385101e-01, 2.64530927e-01, 3.60459220e-01,\n",
       "       1.75485399e-01, 3.58269733e-01, 2.46066996e-01, 2.78481703e-01,\n",
       "       1.68180859e-01, 2.13135705e-01, 4.94010186e-01, 2.47005582e-01,\n",
       "       3.74901992e-01, 2.47365004e-01, 3.57886710e-01, 4.66510049e-01,\n",
       "       1.65083825e-01, 4.51040230e-01, 5.75061905e-01, 3.46381583e-01,\n",
       "       5.84472287e-01, 4.65042873e-01, 3.52520520e-01, 5.07944415e-01,\n",
       "       1.84008675e-01, 3.86422610e-01, 4.58038664e-01, 4.40448096e-01,\n",
       "       3.60657718e-01, 4.50560795e-01, 3.86770175e-01, 4.56456180e-01,\n",
       "       4.10781946e-01, 3.24397724e-01, 6.80165411e-01, 3.56244253e-01,\n",
       "       3.89139362e-01, 4.72577931e-01, 1.43014380e-01, 4.12804889e-01,\n",
       "       1.19165688e-01, 1.92507759e-01, 1.67780632e-01, 4.68052616e-01,\n",
       "       5.54778129e-01, 4.90706134e-02, 2.95772516e-01, 4.79888184e-02,\n",
       "       2.67768735e-01, 4.40725960e-01, 7.44833197e-02, 2.59837951e-01,\n",
       "       5.11961516e-01, 4.26698928e-01, 5.11682006e-01, 3.88147740e-01,\n",
       "       7.74208323e-01, 2.39146665e-01, 7.94284841e-01, 1.72879081e-01,\n",
       "       3.95329640e-01, 7.53990336e-01, 8.87819562e-02, 6.35637363e-01,\n",
       "       3.06410969e-01, 5.13178533e-02, 7.20296278e-01, 2.36413790e-01,\n",
       "       5.30087041e-01, 7.00932590e-01, 5.39141901e-01, 8.83901873e-01,\n",
       "       4.87408942e-01, 4.81323862e-01, 1.15597400e+00, 7.47566808e-01,\n",
       "       1.10524269e+00, 9.99038279e-01, 8.38169875e-01, 1.40674714e+00,\n",
       "       9.48396701e-01, 1.95113708e-01, 1.24570622e+00, 8.62055324e-01,\n",
       "       1.02424065e+00, 1.07221209e+00, 2.66288536e-01, 7.66756245e-01,\n",
       "       6.18229764e-01, 4.97499835e-01, 4.03574097e-01, 7.57925695e-01,\n",
       "       7.82870603e-01, 8.15049247e-01, 1.10276095e+00, 1.11426372e+00,\n",
       "       3.46440460e-01, 1.53642131e+00, 9.90555086e-01, 1.09444431e+00,\n",
       "       5.55003581e-01, 1.07720489e+00, 6.67856459e-01, 1.75936655e+00,\n",
       "       7.90015648e-01, 5.88984185e-01, 7.03500181e-01, 6.43137562e-01,\n",
       "       1.76066819e+00, 1.10536254e-01, 8.50328601e-01, 2.38788395e-01,\n",
       "       1.31205179e+00, 2.22474459e-01, 7.60708578e-01, 2.04519459e+00,\n",
       "       1.04283292e+00, 9.93788380e-01, 2.20526217e+00, 1.19189613e+00,\n",
       "       1.05686034e+00, 1.31890217e+00, 1.42301469e+00, 1.63264711e+00,\n",
       "       9.82138658e-01, 1.77136257e+00, 1.78099293e+00, 9.13373712e-01,\n",
       "       3.37925939e+00, 2.01727543e+00, 2.59206058e+00, 2.92333291e+00,\n",
       "       2.29625810e+00, 3.20009524e+00, 2.98690164e+00, 3.49748489e+00,\n",
       "       2.51281742e+00, 4.10608716e+00, 3.40462321e+00, 2.61899143e+00,\n",
       "       1.83160117e+00, 1.76683147e+00, 1.73165593e+00, 1.55720587e+00,\n",
       "       2.95776180e+00, 1.81798894e+00, 3.78923347e+00, 3.75718817e+00,\n",
       "       4.19134950e+00, 2.98769859e+00, 5.52846461e+00, 2.84441198e+00,\n",
       "       5.07384986e+00, 6.24616347e+00, 5.44436817e+00, 6.55862651e+00,\n",
       "       2.46974517e+00, 4.99317379e+00, 2.99205780e+00, 6.13132162e-01,\n",
       "       2.79852495e+00, 1.68615796e+00, 4.24975903e+00, 4.51074093e+00,\n",
       "       2.89225948e+00, 4.60951172e+00, 3.07150917e+00, 4.95092731e+00,\n",
       "       3.84715252e+00, 5.45904067e+00, 5.04976796e+00, 7.82674174e+00,\n",
       "       3.00935970e+00, 5.03165202e+00, 3.59333840e+00, 2.01246863e+00,\n",
       "       5.36530548e+00, 3.40271907e+00, 1.77591586e+00, 2.41367782e-01,\n",
       "       1.68632656e+00, 2.82719218e+00, 1.29852019e+00, 4.01369190e+00,\n",
       "       4.05805385e+00, 3.99117050e+00, 4.38695831e+00, 6.64896533e+00,\n",
       "       4.47806589e+00, 5.90603645e+00, 6.65403181e+00, 8.62354107e+00,\n",
       "       3.30564494e+00, 4.23113603e+00, 6.81156068e+00, 4.29290953e+00,\n",
       "       4.82866632e+00, 5.38993801e-01, 4.94675838e+00, 4.52698336e+00,\n",
       "       9.38367297e+00, 4.22793681e+00, 5.68597232e+00, 5.40455474e+00,\n",
       "       3.05764819e+00, 1.06581133e+01, 6.33189730e+00, 7.01501634e+00,\n",
       "       5.21258630e+00, 4.64306658e+00, 5.73106323e+00, 6.41972484e+00,\n",
       "       5.37806692e+00, 6.84868009e+00, 4.99365178e+00, 2.72646479e+00,\n",
       "       3.44241696e+00, 4.35318126e+00, 2.28906958e+00, 5.26580172e+00,\n",
       "       5.07096790e+00, 1.12939306e+01, 1.33517384e+01, 4.19078597e+00,\n",
       "       3.86496940e+00, 3.91629216e+00, 2.05894082e+00, 5.56930207e+00,\n",
       "       5.85265922e+00, 8.58947597e+00, 6.33143922e+00, 1.54774677e+01,\n",
       "       8.31921374e+00, 6.82121259e+00, 1.43144487e+01, 7.55829445e+00,\n",
       "       1.31966168e+01, 4.41989314e+00, 5.90177056e+00, 1.45357274e+01,\n",
       "       1.24566504e+01, 8.05689547e+00, 1.87444878e+01, 1.87215449e+01,\n",
       "       2.45438786e+00, 1.97452495e+01, 3.12689056e+01, 1.98479809e+01,\n",
       "       4.44425347e+00, 6.75854211e+00, 5.73678985e+00, 1.50267859e+01,\n",
       "       2.26283263e+01, 8.13353642e+00, 4.01096484e+01, 3.58434966e+01,\n",
       "       5.48711520e+01, 4.03864326e+01, 1.22654171e+01, 4.64911870e+01,\n",
       "       7.10484054e+00, 2.83065024e+01, 1.96043134e+01, 2.72695878e+01,\n",
       "       3.25181599e+01, 3.57384381e+01, 3.38340045e+01, 2.32009942e+01,\n",
       "       1.25984057e+01, 7.47521477e+01, 8.24514477e+01, 9.25608169e+01])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train.loc[train['id'] == 1]\n",
    "\n",
    "# temp['acc_x'].values\n",
    "\n",
    "fourier_transform_one_signal(temp['acc_x'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:13<00:00, 235.98it/s]\n"
     ]
    }
   ],
   "source": [
    "fft=[]\n",
    "for i in tqdm(train['id'].unique()):\n",
    "    temp=train.loc[train['id']==i]\n",
    "    for i in train.columns[2:8]:\n",
    "        temp[i]=fourier_transform_one_signal(temp[i].values)\n",
    "    fft.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_train=pd.concat(fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=pd.concat(test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 782/782 [00:01<00:00, 421.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# fft_t=[]\n",
    "# for i in tqdm(test['id'].unique()):\n",
    "#     temp=test.loc[test['id']==i]\n",
    "#     for i in test.columns[2:8]:\n",
    "#         temp[i]=fourier_transform_one_signal(temp[i].values)\n",
    "#     fft_t.append(temp)\n",
    "# test=pd.concat(fft_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "      <th>acc_x_dt</th>\n",
       "      <th>acc_y_dt</th>\n",
       "      <th>acc_z_dt</th>\n",
       "      <th>gy_x_dt</th>\n",
       "      <th>gy_y_dt</th>\n",
       "      <th>gy_z_dt</th>\n",
       "      <th>acc_Energy_dt</th>\n",
       "      <th>gy_Energy_dt</th>\n",
       "      <th>gy_acc_Energy_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206087</td>\n",
       "      <td>-0.179371</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.591608</td>\n",
       "      <td>-30.549010</td>\n",
       "      <td>-31.676112</td>\n",
       "      <td>1.146962</td>\n",
       "      <td>12.465436</td>\n",
       "      <td>12.427938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287696</td>\n",
       "      <td>-0.198974</td>\n",
       "      <td>-0.182444</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-39.139103</td>\n",
       "      <td>-24.927216</td>\n",
       "      <td>1.200703</td>\n",
       "      <td>12.913284</td>\n",
       "      <td>12.865692</td>\n",
       "      <td>4.080495</td>\n",
       "      <td>-0.980114</td>\n",
       "      <td>-1.699854</td>\n",
       "      <td>44.735403</td>\n",
       "      <td>-429.504677</td>\n",
       "      <td>337.444793</td>\n",
       "      <td>2.687024</td>\n",
       "      <td>22.392358</td>\n",
       "      <td>21.887693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.304609</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.253382</td>\n",
       "      <td>-3.617278</td>\n",
       "      <td>-44.122565</td>\n",
       "      <td>-25.019629</td>\n",
       "      <td>1.217403</td>\n",
       "      <td>13.725729</td>\n",
       "      <td>13.692643</td>\n",
       "      <td>0.845632</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>-3.546937</td>\n",
       "      <td>-196.018888</td>\n",
       "      <td>-249.173073</td>\n",
       "      <td>-4.620631</td>\n",
       "      <td>0.835012</td>\n",
       "      <td>40.622253</td>\n",
       "      <td>41.347563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.293095</td>\n",
       "      <td>-0.230366</td>\n",
       "      <td>-0.215210</td>\n",
       "      <td>2.712986</td>\n",
       "      <td>-53.597843</td>\n",
       "      <td>-27.454013</td>\n",
       "      <td>1.209981</td>\n",
       "      <td>15.374021</td>\n",
       "      <td>15.314907</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-1.762585</td>\n",
       "      <td>1.908626</td>\n",
       "      <td>316.513181</td>\n",
       "      <td>-473.763910</td>\n",
       "      <td>-121.719195</td>\n",
       "      <td>-0.371100</td>\n",
       "      <td>82.414636</td>\n",
       "      <td>81.113199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.300887</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>-0.222523</td>\n",
       "      <td>4.286707</td>\n",
       "      <td>-57.906561</td>\n",
       "      <td>-27.961234</td>\n",
       "      <td>1.211254</td>\n",
       "      <td>16.074363</td>\n",
       "      <td>16.017964</td>\n",
       "      <td>0.389598</td>\n",
       "      <td>2.130453</td>\n",
       "      <td>-0.365665</td>\n",
       "      <td>78.686055</td>\n",
       "      <td>-215.435892</td>\n",
       "      <td>-25.361098</td>\n",
       "      <td>0.063656</td>\n",
       "      <td>35.017060</td>\n",
       "      <td>35.152822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874995</th>\n",
       "      <td>3124</td>\n",
       "      <td>595</td>\n",
       "      <td>-0.712530</td>\n",
       "      <td>-0.658357</td>\n",
       "      <td>0.293707</td>\n",
       "      <td>-29.367857</td>\n",
       "      <td>-104.013664</td>\n",
       "      <td>-76.290437</td>\n",
       "      <td>1.009050</td>\n",
       "      <td>25.963234</td>\n",
       "      <td>25.897316</td>\n",
       "      <td>1.484646</td>\n",
       "      <td>0.303666</td>\n",
       "      <td>0.800069</td>\n",
       "      <td>-150.644663</td>\n",
       "      <td>-34.630282</td>\n",
       "      <td>-8.380088</td>\n",
       "      <td>-0.679712</td>\n",
       "      <td>8.387109</td>\n",
       "      <td>8.432977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874996</th>\n",
       "      <td>3124</td>\n",
       "      <td>596</td>\n",
       "      <td>-0.683037</td>\n",
       "      <td>-0.658466</td>\n",
       "      <td>0.329223</td>\n",
       "      <td>-30.149089</td>\n",
       "      <td>-101.796809</td>\n",
       "      <td>-76.625087</td>\n",
       "      <td>1.002827</td>\n",
       "      <td>25.784692</td>\n",
       "      <td>25.722482</td>\n",
       "      <td>1.474659</td>\n",
       "      <td>-0.005442</td>\n",
       "      <td>1.775771</td>\n",
       "      <td>-39.061611</td>\n",
       "      <td>110.842743</td>\n",
       "      <td>-16.732496</td>\n",
       "      <td>-0.311171</td>\n",
       "      <td>-8.927089</td>\n",
       "      <td>-8.741727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874997</th>\n",
       "      <td>3124</td>\n",
       "      <td>597</td>\n",
       "      <td>-0.664730</td>\n",
       "      <td>-0.666625</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>-27.873095</td>\n",
       "      <td>-98.776072</td>\n",
       "      <td>-79.365125</td>\n",
       "      <td>1.006239</td>\n",
       "      <td>25.628060</td>\n",
       "      <td>25.572145</td>\n",
       "      <td>0.915321</td>\n",
       "      <td>-0.407957</td>\n",
       "      <td>1.744566</td>\n",
       "      <td>113.799702</td>\n",
       "      <td>151.036858</td>\n",
       "      <td>-137.001896</td>\n",
       "      <td>0.170620</td>\n",
       "      <td>-7.831611</td>\n",
       "      <td>-7.516832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874998</th>\n",
       "      <td>3124</td>\n",
       "      <td>598</td>\n",
       "      <td>-0.630534</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.373696</td>\n",
       "      <td>-23.636550</td>\n",
       "      <td>-99.139495</td>\n",
       "      <td>-80.259478</td>\n",
       "      <td>1.001038</td>\n",
       "      <td>25.626266</td>\n",
       "      <td>25.573288</td>\n",
       "      <td>1.709833</td>\n",
       "      <td>-0.796984</td>\n",
       "      <td>0.479107</td>\n",
       "      <td>211.827245</td>\n",
       "      <td>-18.171144</td>\n",
       "      <td>-44.717652</td>\n",
       "      <td>-0.260074</td>\n",
       "      <td>-0.089713</td>\n",
       "      <td>0.057150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874999</th>\n",
       "      <td>3124</td>\n",
       "      <td>599</td>\n",
       "      <td>-0.578351</td>\n",
       "      <td>-0.700235</td>\n",
       "      <td>0.384390</td>\n",
       "      <td>-17.917626</td>\n",
       "      <td>-100.181873</td>\n",
       "      <td>-80.676229</td>\n",
       "      <td>0.990773</td>\n",
       "      <td>25.645131</td>\n",
       "      <td>25.595348</td>\n",
       "      <td>2.609116</td>\n",
       "      <td>-0.883512</td>\n",
       "      <td>0.534668</td>\n",
       "      <td>285.946217</td>\n",
       "      <td>-52.118894</td>\n",
       "      <td>-20.837588</td>\n",
       "      <td>-0.513215</td>\n",
       "      <td>0.943240</td>\n",
       "      <td>1.102985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1875000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  time     acc_x     acc_y     acc_z       gy_x        gy_y  \\\n",
       "0           0     0  1.206087 -0.179371 -0.148447  -0.591608  -30.549010   \n",
       "1           0     1  1.287696 -0.198974 -0.182444   0.303100  -39.139103   \n",
       "2           0     2  1.304609 -0.195114 -0.253382  -3.617278  -44.122565   \n",
       "3           0     3  1.293095 -0.230366 -0.215210   2.712986  -53.597843   \n",
       "4           0     4  1.300887 -0.187757 -0.222523   4.286707  -57.906561   \n",
       "...       ...   ...       ...       ...       ...        ...         ...   \n",
       "1874995  3124   595 -0.712530 -0.658357  0.293707 -29.367857 -104.013664   \n",
       "1874996  3124   596 -0.683037 -0.658466  0.329223 -30.149089 -101.796809   \n",
       "1874997  3124   597 -0.664730 -0.666625  0.364114 -27.873095  -98.776072   \n",
       "1874998  3124   598 -0.630534 -0.682565  0.373696 -23.636550  -99.139495   \n",
       "1874999  3124   599 -0.578351 -0.700235  0.384390 -17.917626 -100.181873   \n",
       "\n",
       "              gy_z  acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  \\\n",
       "0       -31.676112    1.146962  12.465436      12.427938  0.000000  0.000000   \n",
       "1       -24.927216    1.200703  12.913284      12.865692  4.080495 -0.980114   \n",
       "2       -25.019629    1.217403  13.725729      13.692643  0.845632  0.192961   \n",
       "3       -27.454013    1.209981  15.374021      15.314907 -0.575711 -1.762585   \n",
       "4       -27.961234    1.211254  16.074363      16.017964  0.389598  2.130453   \n",
       "...            ...         ...        ...            ...       ...       ...   \n",
       "1874995 -76.290437    1.009050  25.963234      25.897316  1.484646  0.303666   \n",
       "1874996 -76.625087    1.002827  25.784692      25.722482  1.474659 -0.005442   \n",
       "1874997 -79.365125    1.006239  25.628060      25.572145  0.915321 -0.407957   \n",
       "1874998 -80.259478    1.001038  25.626266      25.573288  1.709833 -0.796984   \n",
       "1874999 -80.676229    0.990773  25.645131      25.595348  2.609116 -0.883512   \n",
       "\n",
       "         acc_z_dt     gy_x_dt     gy_y_dt     gy_z_dt  acc_Energy_dt  \\\n",
       "0        0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "1       -1.699854   44.735403 -429.504677  337.444793       2.687024   \n",
       "2       -3.546937 -196.018888 -249.173073   -4.620631       0.835012   \n",
       "3        1.908626  316.513181 -473.763910 -121.719195      -0.371100   \n",
       "4       -0.365665   78.686055 -215.435892  -25.361098       0.063656   \n",
       "...           ...         ...         ...         ...            ...   \n",
       "1874995  0.800069 -150.644663  -34.630282   -8.380088      -0.679712   \n",
       "1874996  1.775771  -39.061611  110.842743  -16.732496      -0.311171   \n",
       "1874997  1.744566  113.799702  151.036858 -137.001896       0.170620   \n",
       "1874998  0.479107  211.827245  -18.171144  -44.717652      -0.260074   \n",
       "1874999  0.534668  285.946217  -52.118894  -20.837588      -0.513215   \n",
       "\n",
       "         gy_Energy_dt  gy_acc_Energy_dt  \n",
       "0            0.000000          0.000000  \n",
       "1           22.392358         21.887693  \n",
       "2           40.622253         41.347563  \n",
       "3           82.414636         81.113199  \n",
       "4           35.017060         35.152822  \n",
       "...               ...               ...  \n",
       "1874995      8.387109          8.432977  \n",
       "1874996     -8.927089         -8.741727  \n",
       "1874997     -7.831611         -7.516832  \n",
       "1874998     -0.089713          0.057150  \n",
       "1874999      0.943240          1.102985  \n",
       "\n",
       "[1875000 rows x 20 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  1.20608659e+00, ...,\n",
       "        -5.91607848e-01, -3.05490095e+01, -3.16761119e+01],\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  1.28769648e+00, ...,\n",
       "         3.03100213e-01, -3.91391031e+01, -2.49272160e+01],\n",
       "       [ 0.00000000e+00,  2.00000000e+00,  1.30460912e+00, ...,\n",
       "        -3.61727754e+00, -4.41225645e+01, -2.50196286e+01],\n",
       "       ...,\n",
       "       [ 3.12400000e+03,  5.97000000e+02, -6.64730300e-01, ...,\n",
       "        -2.78730951e+01, -9.87760722e+01, -7.93651247e+01],\n",
       "       [ 3.12400000e+03,  5.98000000e+02, -6.30533630e-01, ...,\n",
       "        -2.36365502e+01, -9.91394951e+01, -8.02594777e+01],\n",
       "       [ 3.12400000e+03,  5.99000000e+02, -5.78351314e-01, ...,\n",
       "        -1.79176259e+01, -1.00181873e+02, -8.06762295e+01]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_train = train.iloc[:, :8].values\n",
    "before_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'time',\n",
       " 'before_acc_x',\n",
       " 'before_acc_y',\n",
       " 'before_acc_z',\n",
       " 'before_gy_x',\n",
       " 'before_gy_y',\n",
       " 'before_gy_z']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_lst = ['id', 'time']\n",
    "for i in train.columns[2:8]:\n",
    "    column_lst.append('before_'+i)\n",
    "column_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(before_train, columns=column_lst)\n",
    "before_train = df.astype({'id':'int64', 'time':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = before_train.merge(fft_train, how='left', on=['id', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>before_acc_x</th>\n",
       "      <th>before_acc_y</th>\n",
       "      <th>before_acc_z</th>\n",
       "      <th>before_gy_x</th>\n",
       "      <th>before_gy_y</th>\n",
       "      <th>before_gy_z</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "      <th>acc_x_dt</th>\n",
       "      <th>acc_y_dt</th>\n",
       "      <th>acc_z_dt</th>\n",
       "      <th>gy_x_dt</th>\n",
       "      <th>gy_y_dt</th>\n",
       "      <th>gy_z_dt</th>\n",
       "      <th>acc_Energy_dt</th>\n",
       "      <th>gy_Energy_dt</th>\n",
       "      <th>gy_acc_Energy_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206087</td>\n",
       "      <td>-0.179371</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.591608</td>\n",
       "      <td>-30.549010</td>\n",
       "      <td>-31.676112</td>\n",
       "      <td>558.797337</td>\n",
       "      <td>131.082711</td>\n",
       "      <td>222.252919</td>\n",
       "      <td>1119.161589</td>\n",
       "      <td>2015.703683</td>\n",
       "      <td>709.264425</td>\n",
       "      <td>1.146962</td>\n",
       "      <td>12.465436</td>\n",
       "      <td>12.427938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287696</td>\n",
       "      <td>-0.198974</td>\n",
       "      <td>-0.182444</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-39.139103</td>\n",
       "      <td>-24.927216</td>\n",
       "      <td>3.233175</td>\n",
       "      <td>15.689279</td>\n",
       "      <td>12.229014</td>\n",
       "      <td>221.599635</td>\n",
       "      <td>361.903330</td>\n",
       "      <td>477.080942</td>\n",
       "      <td>1.200703</td>\n",
       "      <td>12.913284</td>\n",
       "      <td>12.865692</td>\n",
       "      <td>4.080495</td>\n",
       "      <td>-0.980114</td>\n",
       "      <td>-1.699854</td>\n",
       "      <td>44.735403</td>\n",
       "      <td>-429.504677</td>\n",
       "      <td>337.444793</td>\n",
       "      <td>2.687024</td>\n",
       "      <td>22.392358</td>\n",
       "      <td>21.887693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.304609</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.253382</td>\n",
       "      <td>-3.617278</td>\n",
       "      <td>-44.122565</td>\n",
       "      <td>-25.019629</td>\n",
       "      <td>4.832535</td>\n",
       "      <td>8.199566</td>\n",
       "      <td>3.901211</td>\n",
       "      <td>357.200415</td>\n",
       "      <td>430.568986</td>\n",
       "      <td>452.096143</td>\n",
       "      <td>1.217403</td>\n",
       "      <td>13.725729</td>\n",
       "      <td>13.692643</td>\n",
       "      <td>0.845632</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>-3.546937</td>\n",
       "      <td>-196.018888</td>\n",
       "      <td>-249.173073</td>\n",
       "      <td>-4.620631</td>\n",
       "      <td>0.835012</td>\n",
       "      <td>40.622253</td>\n",
       "      <td>41.347563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.293095</td>\n",
       "      <td>-0.230366</td>\n",
       "      <td>-0.215210</td>\n",
       "      <td>2.712986</td>\n",
       "      <td>-53.597843</td>\n",
       "      <td>-27.454013</td>\n",
       "      <td>5.675383</td>\n",
       "      <td>5.330015</td>\n",
       "      <td>2.527445</td>\n",
       "      <td>340.433376</td>\n",
       "      <td>787.558320</td>\n",
       "      <td>467.307109</td>\n",
       "      <td>1.209981</td>\n",
       "      <td>15.374021</td>\n",
       "      <td>15.314907</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-1.762585</td>\n",
       "      <td>1.908626</td>\n",
       "      <td>316.513181</td>\n",
       "      <td>-473.763910</td>\n",
       "      <td>-121.719195</td>\n",
       "      <td>-0.371100</td>\n",
       "      <td>82.414636</td>\n",
       "      <td>81.113199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.300887</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>-0.222523</td>\n",
       "      <td>4.286707</td>\n",
       "      <td>-57.906561</td>\n",
       "      <td>-27.961234</td>\n",
       "      <td>7.415275</td>\n",
       "      <td>7.980024</td>\n",
       "      <td>6.566908</td>\n",
       "      <td>128.188871</td>\n",
       "      <td>1372.095224</td>\n",
       "      <td>715.824074</td>\n",
       "      <td>1.211254</td>\n",
       "      <td>16.074363</td>\n",
       "      <td>16.017964</td>\n",
       "      <td>0.389598</td>\n",
       "      <td>2.130453</td>\n",
       "      <td>-0.365665</td>\n",
       "      <td>78.686055</td>\n",
       "      <td>-215.435892</td>\n",
       "      <td>-25.361098</td>\n",
       "      <td>0.063656</td>\n",
       "      <td>35.017060</td>\n",
       "      <td>35.152822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874995</th>\n",
       "      <td>3124</td>\n",
       "      <td>595</td>\n",
       "      <td>-0.712530</td>\n",
       "      <td>-0.658357</td>\n",
       "      <td>0.293707</td>\n",
       "      <td>-29.367857</td>\n",
       "      <td>-104.013664</td>\n",
       "      <td>-76.290437</td>\n",
       "      <td>11.743654</td>\n",
       "      <td>3.796333</td>\n",
       "      <td>12.513870</td>\n",
       "      <td>715.873677</td>\n",
       "      <td>1124.494889</td>\n",
       "      <td>645.627066</td>\n",
       "      <td>1.009050</td>\n",
       "      <td>25.963234</td>\n",
       "      <td>25.897316</td>\n",
       "      <td>1.484646</td>\n",
       "      <td>0.303666</td>\n",
       "      <td>0.800069</td>\n",
       "      <td>-150.644663</td>\n",
       "      <td>-34.630282</td>\n",
       "      <td>-8.380088</td>\n",
       "      <td>-0.679712</td>\n",
       "      <td>8.387109</td>\n",
       "      <td>8.432977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874996</th>\n",
       "      <td>3124</td>\n",
       "      <td>596</td>\n",
       "      <td>-0.683037</td>\n",
       "      <td>-0.658466</td>\n",
       "      <td>0.329223</td>\n",
       "      <td>-30.149089</td>\n",
       "      <td>-101.796809</td>\n",
       "      <td>-76.625087</td>\n",
       "      <td>211.498089</td>\n",
       "      <td>82.888508</td>\n",
       "      <td>86.807874</td>\n",
       "      <td>5515.261695</td>\n",
       "      <td>28917.564390</td>\n",
       "      <td>20218.747027</td>\n",
       "      <td>1.002827</td>\n",
       "      <td>25.784692</td>\n",
       "      <td>25.722482</td>\n",
       "      <td>1.474659</td>\n",
       "      <td>-0.005442</td>\n",
       "      <td>1.775771</td>\n",
       "      <td>-39.061611</td>\n",
       "      <td>110.842743</td>\n",
       "      <td>-16.732496</td>\n",
       "      <td>-0.311171</td>\n",
       "      <td>-8.927089</td>\n",
       "      <td>-8.741727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874997</th>\n",
       "      <td>3124</td>\n",
       "      <td>597</td>\n",
       "      <td>-0.664730</td>\n",
       "      <td>-0.666625</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>-27.873095</td>\n",
       "      <td>-98.776072</td>\n",
       "      <td>-79.365125</td>\n",
       "      <td>12.175349</td>\n",
       "      <td>6.200258</td>\n",
       "      <td>2.084554</td>\n",
       "      <td>343.695161</td>\n",
       "      <td>464.375112</td>\n",
       "      <td>78.097163</td>\n",
       "      <td>1.006239</td>\n",
       "      <td>25.628060</td>\n",
       "      <td>25.572145</td>\n",
       "      <td>0.915321</td>\n",
       "      <td>-0.407957</td>\n",
       "      <td>1.744566</td>\n",
       "      <td>113.799702</td>\n",
       "      <td>151.036858</td>\n",
       "      <td>-137.001896</td>\n",
       "      <td>0.170620</td>\n",
       "      <td>-7.831611</td>\n",
       "      <td>-7.516832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874998</th>\n",
       "      <td>3124</td>\n",
       "      <td>598</td>\n",
       "      <td>-0.630534</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.373696</td>\n",
       "      <td>-23.636550</td>\n",
       "      <td>-99.139495</td>\n",
       "      <td>-80.259478</td>\n",
       "      <td>19.116783</td>\n",
       "      <td>3.830800</td>\n",
       "      <td>6.938661</td>\n",
       "      <td>791.376179</td>\n",
       "      <td>2724.373764</td>\n",
       "      <td>1131.590078</td>\n",
       "      <td>1.001038</td>\n",
       "      <td>25.626266</td>\n",
       "      <td>25.573288</td>\n",
       "      <td>1.709833</td>\n",
       "      <td>-0.796984</td>\n",
       "      <td>0.479107</td>\n",
       "      <td>211.827245</td>\n",
       "      <td>-18.171144</td>\n",
       "      <td>-44.717652</td>\n",
       "      <td>-0.260074</td>\n",
       "      <td>-0.089713</td>\n",
       "      <td>0.057150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874999</th>\n",
       "      <td>3124</td>\n",
       "      <td>599</td>\n",
       "      <td>-0.578351</td>\n",
       "      <td>-0.700235</td>\n",
       "      <td>0.384390</td>\n",
       "      <td>-17.917626</td>\n",
       "      <td>-100.181873</td>\n",
       "      <td>-80.676229</td>\n",
       "      <td>22.306532</td>\n",
       "      <td>4.721920</td>\n",
       "      <td>15.463388</td>\n",
       "      <td>357.639418</td>\n",
       "      <td>2058.364675</td>\n",
       "      <td>977.868201</td>\n",
       "      <td>0.990773</td>\n",
       "      <td>25.645131</td>\n",
       "      <td>25.595348</td>\n",
       "      <td>2.609116</td>\n",
       "      <td>-0.883512</td>\n",
       "      <td>0.534668</td>\n",
       "      <td>285.946217</td>\n",
       "      <td>-52.118894</td>\n",
       "      <td>-20.837588</td>\n",
       "      <td>-0.513215</td>\n",
       "      <td>0.943240</td>\n",
       "      <td>1.102985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1875000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  time  before_acc_x  before_acc_y  before_acc_z  before_gy_x  \\\n",
       "0           0     0      1.206087     -0.179371     -0.148447    -0.591608   \n",
       "1           0     1      1.287696     -0.198974     -0.182444     0.303100   \n",
       "2           0     2      1.304609     -0.195114     -0.253382    -3.617278   \n",
       "3           0     3      1.293095     -0.230366     -0.215210     2.712986   \n",
       "4           0     4      1.300887     -0.187757     -0.222523     4.286707   \n",
       "...       ...   ...           ...           ...           ...          ...   \n",
       "1874995  3124   595     -0.712530     -0.658357      0.293707   -29.367857   \n",
       "1874996  3124   596     -0.683037     -0.658466      0.329223   -30.149089   \n",
       "1874997  3124   597     -0.664730     -0.666625      0.364114   -27.873095   \n",
       "1874998  3124   598     -0.630534     -0.682565      0.373696   -23.636550   \n",
       "1874999  3124   599     -0.578351     -0.700235      0.384390   -17.917626   \n",
       "\n",
       "         before_gy_y  before_gy_z       acc_x       acc_y       acc_z  \\\n",
       "0         -30.549010   -31.676112  558.797337  131.082711  222.252919   \n",
       "1         -39.139103   -24.927216    3.233175   15.689279   12.229014   \n",
       "2         -44.122565   -25.019629    4.832535    8.199566    3.901211   \n",
       "3         -53.597843   -27.454013    5.675383    5.330015    2.527445   \n",
       "4         -57.906561   -27.961234    7.415275    7.980024    6.566908   \n",
       "...              ...          ...         ...         ...         ...   \n",
       "1874995  -104.013664   -76.290437   11.743654    3.796333   12.513870   \n",
       "1874996  -101.796809   -76.625087  211.498089   82.888508   86.807874   \n",
       "1874997   -98.776072   -79.365125   12.175349    6.200258    2.084554   \n",
       "1874998   -99.139495   -80.259478   19.116783    3.830800    6.938661   \n",
       "1874999  -100.181873   -80.676229   22.306532    4.721920   15.463388   \n",
       "\n",
       "                gy_x          gy_y          gy_z  acc_Energy  gy_Energy  \\\n",
       "0        1119.161589   2015.703683    709.264425    1.146962  12.465436   \n",
       "1         221.599635    361.903330    477.080942    1.200703  12.913284   \n",
       "2         357.200415    430.568986    452.096143    1.217403  13.725729   \n",
       "3         340.433376    787.558320    467.307109    1.209981  15.374021   \n",
       "4         128.188871   1372.095224    715.824074    1.211254  16.074363   \n",
       "...              ...           ...           ...         ...        ...   \n",
       "1874995   715.873677   1124.494889    645.627066    1.009050  25.963234   \n",
       "1874996  5515.261695  28917.564390  20218.747027    1.002827  25.784692   \n",
       "1874997   343.695161    464.375112     78.097163    1.006239  25.628060   \n",
       "1874998   791.376179   2724.373764   1131.590078    1.001038  25.626266   \n",
       "1874999   357.639418   2058.364675    977.868201    0.990773  25.645131   \n",
       "\n",
       "         gy_acc_Energy  acc_x_dt  acc_y_dt  acc_z_dt     gy_x_dt     gy_y_dt  \\\n",
       "0            12.427938  0.000000  0.000000  0.000000    0.000000    0.000000   \n",
       "1            12.865692  4.080495 -0.980114 -1.699854   44.735403 -429.504677   \n",
       "2            13.692643  0.845632  0.192961 -3.546937 -196.018888 -249.173073   \n",
       "3            15.314907 -0.575711 -1.762585  1.908626  316.513181 -473.763910   \n",
       "4            16.017964  0.389598  2.130453 -0.365665   78.686055 -215.435892   \n",
       "...                ...       ...       ...       ...         ...         ...   \n",
       "1874995      25.897316  1.484646  0.303666  0.800069 -150.644663  -34.630282   \n",
       "1874996      25.722482  1.474659 -0.005442  1.775771  -39.061611  110.842743   \n",
       "1874997      25.572145  0.915321 -0.407957  1.744566  113.799702  151.036858   \n",
       "1874998      25.573288  1.709833 -0.796984  0.479107  211.827245  -18.171144   \n",
       "1874999      25.595348  2.609116 -0.883512  0.534668  285.946217  -52.118894   \n",
       "\n",
       "            gy_z_dt  acc_Energy_dt  gy_Energy_dt  gy_acc_Energy_dt  \n",
       "0          0.000000       0.000000      0.000000          0.000000  \n",
       "1        337.444793       2.687024     22.392358         21.887693  \n",
       "2         -4.620631       0.835012     40.622253         41.347563  \n",
       "3       -121.719195      -0.371100     82.414636         81.113199  \n",
       "4        -25.361098       0.063656     35.017060         35.152822  \n",
       "...             ...            ...           ...               ...  \n",
       "1874995   -8.380088      -0.679712      8.387109          8.432977  \n",
       "1874996  -16.732496      -0.311171     -8.927089         -8.741727  \n",
       "1874997 -137.001896       0.170620     -7.831611         -7.516832  \n",
       "1874998  -44.717652      -0.260074     -0.089713          0.057150  \n",
       "1874999  -20.837588      -0.513215      0.943240          1.102985  \n",
       "\n",
       "[1875000 rows x 26 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>before_acc_x</th>\n",
       "      <th>before_acc_y</th>\n",
       "      <th>before_acc_z</th>\n",
       "      <th>before_gy_x</th>\n",
       "      <th>before_gy_y</th>\n",
       "      <th>before_gy_z</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "      <th>acc_x_dt</th>\n",
       "      <th>acc_y_dt</th>\n",
       "      <th>acc_z_dt</th>\n",
       "      <th>gy_x_dt</th>\n",
       "      <th>gy_y_dt</th>\n",
       "      <th>gy_z_dt</th>\n",
       "      <th>acc_Energy_dt</th>\n",
       "      <th>gy_Energy_dt</th>\n",
       "      <th>gy_acc_Energy_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.307314</td>\n",
       "      <td>-0.029939</td>\n",
       "      <td>-0.432104</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>-0.410859</td>\n",
       "      <td>-0.461007</td>\n",
       "      <td>27.356382</td>\n",
       "      <td>8.807207</td>\n",
       "      <td>19.465910</td>\n",
       "      <td>0.376992</td>\n",
       "      <td>0.869226</td>\n",
       "      <td>0.150423</td>\n",
       "      <td>0.495681</td>\n",
       "      <td>-0.272719</td>\n",
       "      <td>-0.276391</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.421086</td>\n",
       "      <td>-0.063321</td>\n",
       "      <td>-0.504058</td>\n",
       "      <td>0.026484</td>\n",
       "      <td>-0.528052</td>\n",
       "      <td>-0.365459</td>\n",
       "      <td>-0.054866</td>\n",
       "      <td>0.833464</td>\n",
       "      <td>0.820412</td>\n",
       "      <td>-0.282128</td>\n",
       "      <td>-0.093560</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.742974</td>\n",
       "      <td>-0.236152</td>\n",
       "      <td>-0.240632</td>\n",
       "      <td>0.416836</td>\n",
       "      <td>-0.118821</td>\n",
       "      <td>-0.255054</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>-0.349095</td>\n",
       "      <td>0.377085</td>\n",
       "      <td>0.564992</td>\n",
       "      <td>0.166566</td>\n",
       "      <td>0.162871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.444664</td>\n",
       "      <td>-0.056749</td>\n",
       "      <td>-0.654199</td>\n",
       "      <td>-0.037962</td>\n",
       "      <td>-0.596041</td>\n",
       "      <td>-0.366767</td>\n",
       "      <td>0.024046</td>\n",
       "      <td>0.315921</td>\n",
       "      <td>0.081086</td>\n",
       "      <td>-0.182551</td>\n",
       "      <td>-0.053585</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>0.819822</td>\n",
       "      <td>-0.169815</td>\n",
       "      <td>-0.173080</td>\n",
       "      <td>0.086405</td>\n",
       "      <td>0.023750</td>\n",
       "      <td>-0.531727</td>\n",
       "      <td>-0.141582</td>\n",
       "      <td>-0.202368</td>\n",
       "      <td>-0.004887</td>\n",
       "      <td>0.175645</td>\n",
       "      <td>0.300944</td>\n",
       "      <td>0.306341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.428612</td>\n",
       "      <td>-0.116782</td>\n",
       "      <td>-0.573407</td>\n",
       "      <td>0.066099</td>\n",
       "      <td>-0.725311</td>\n",
       "      <td>-0.401232</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>0.117634</td>\n",
       "      <td>-0.040874</td>\n",
       "      <td>-0.194863</td>\n",
       "      <td>0.154242</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.785669</td>\n",
       "      <td>-0.035229</td>\n",
       "      <td>-0.040560</td>\n",
       "      <td>-0.058780</td>\n",
       "      <td>-0.213920</td>\n",
       "      <td>0.285459</td>\n",
       "      <td>0.229520</td>\n",
       "      <td>-0.385106</td>\n",
       "      <td>-0.135647</td>\n",
       "      <td>-0.077915</td>\n",
       "      <td>0.609008</td>\n",
       "      <td>0.599518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.439475</td>\n",
       "      <td>-0.044220</td>\n",
       "      <td>-0.588886</td>\n",
       "      <td>0.091969</td>\n",
       "      <td>-0.784094</td>\n",
       "      <td>-0.408413</td>\n",
       "      <td>0.151477</td>\n",
       "      <td>0.300751</td>\n",
       "      <td>0.317742</td>\n",
       "      <td>-0.350724</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.154354</td>\n",
       "      <td>0.791528</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>0.039823</td>\n",
       "      <td>0.259227</td>\n",
       "      <td>-0.055206</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>-0.174917</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.013483</td>\n",
       "      <td>0.259626</td>\n",
       "      <td>0.260669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874995</th>\n",
       "      <td>3124</td>\n",
       "      <td>595</td>\n",
       "      <td>-0.367432</td>\n",
       "      <td>-0.845648</td>\n",
       "      <td>0.503707</td>\n",
       "      <td>-0.461266</td>\n",
       "      <td>-1.413128</td>\n",
       "      <td>-1.092639</td>\n",
       "      <td>0.365037</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.845701</td>\n",
       "      <td>0.080839</td>\n",
       "      <td>0.350395</td>\n",
       "      <td>0.112282</td>\n",
       "      <td>-0.138940</td>\n",
       "      <td>0.829394</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>0.151679</td>\n",
       "      <td>0.037205</td>\n",
       "      <td>0.119409</td>\n",
       "      <td>-0.108728</td>\n",
       "      <td>-0.027804</td>\n",
       "      <td>-0.009085</td>\n",
       "      <td>-0.142794</td>\n",
       "      <td>0.063329</td>\n",
       "      <td>0.063674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874996</th>\n",
       "      <td>3124</td>\n",
       "      <td>596</td>\n",
       "      <td>-0.326315</td>\n",
       "      <td>-0.845833</td>\n",
       "      <td>0.578875</td>\n",
       "      <td>-0.474109</td>\n",
       "      <td>-1.382884</td>\n",
       "      <td>-1.097377</td>\n",
       "      <td>10.220817</td>\n",
       "      <td>5.476964</td>\n",
       "      <td>7.441373</td>\n",
       "      <td>3.605246</td>\n",
       "      <td>16.530576</td>\n",
       "      <td>11.843241</td>\n",
       "      <td>-0.167578</td>\n",
       "      <td>0.814816</td>\n",
       "      <td>0.809618</td>\n",
       "      <td>0.150658</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.265559</td>\n",
       "      <td>-0.027936</td>\n",
       "      <td>0.090560</td>\n",
       "      <td>-0.018412</td>\n",
       "      <td>-0.065316</td>\n",
       "      <td>-0.064300</td>\n",
       "      <td>-0.062949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874997</th>\n",
       "      <td>3124</td>\n",
       "      <td>597</td>\n",
       "      <td>-0.300794</td>\n",
       "      <td>-0.859728</td>\n",
       "      <td>0.652722</td>\n",
       "      <td>-0.436694</td>\n",
       "      <td>-1.341672</td>\n",
       "      <td>-1.136169</td>\n",
       "      <td>0.386337</td>\n",
       "      <td>0.177768</td>\n",
       "      <td>-0.080193</td>\n",
       "      <td>-0.192468</td>\n",
       "      <td>-0.033904</td>\n",
       "      <td>-0.227861</td>\n",
       "      <td>-0.151875</td>\n",
       "      <td>0.802027</td>\n",
       "      <td>0.797338</td>\n",
       "      <td>0.093524</td>\n",
       "      <td>-0.049283</td>\n",
       "      <td>0.260884</td>\n",
       "      <td>0.082744</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>-0.152712</td>\n",
       "      <td>0.035970</td>\n",
       "      <td>-0.056225</td>\n",
       "      <td>-0.053918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874998</th>\n",
       "      <td>3124</td>\n",
       "      <td>598</td>\n",
       "      <td>-0.253120</td>\n",
       "      <td>-0.886873</td>\n",
       "      <td>0.673002</td>\n",
       "      <td>-0.367051</td>\n",
       "      <td>-1.346630</td>\n",
       "      <td>-1.148831</td>\n",
       "      <td>0.728823</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.350745</td>\n",
       "      <td>0.136284</td>\n",
       "      <td>1.281790</td>\n",
       "      <td>0.403540</td>\n",
       "      <td>-0.175811</td>\n",
       "      <td>0.801880</td>\n",
       "      <td>0.797431</td>\n",
       "      <td>0.174681</td>\n",
       "      <td>-0.096564</td>\n",
       "      <td>0.071332</td>\n",
       "      <td>0.153722</td>\n",
       "      <td>-0.014412</td>\n",
       "      <td>-0.049662</td>\n",
       "      <td>-0.054574</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874999</th>\n",
       "      <td>3124</td>\n",
       "      <td>599</td>\n",
       "      <td>-0.180373</td>\n",
       "      <td>-0.916966</td>\n",
       "      <td>0.695634</td>\n",
       "      <td>-0.273040</td>\n",
       "      <td>-1.360851</td>\n",
       "      <td>-1.154731</td>\n",
       "      <td>0.886204</td>\n",
       "      <td>0.075614</td>\n",
       "      <td>1.107553</td>\n",
       "      <td>-0.182228</td>\n",
       "      <td>0.894062</td>\n",
       "      <td>0.311408</td>\n",
       "      <td>-0.223043</td>\n",
       "      <td>0.803421</td>\n",
       "      <td>0.799233</td>\n",
       "      <td>0.266539</td>\n",
       "      <td>-0.107081</td>\n",
       "      <td>0.079654</td>\n",
       "      <td>0.207388</td>\n",
       "      <td>-0.042034</td>\n",
       "      <td>-0.022996</td>\n",
       "      <td>-0.107792</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1875000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  time  before_acc_x  before_acc_y  before_acc_z  before_gy_x  \\\n",
       "0           0     0      2.307314     -0.029939     -0.432104     0.011776   \n",
       "1           0     1      2.421086     -0.063321     -0.504058     0.026484   \n",
       "2           0     2      2.444664     -0.056749     -0.654199    -0.037962   \n",
       "3           0     3      2.428612     -0.116782     -0.573407     0.066099   \n",
       "4           0     4      2.439475     -0.044220     -0.588886     0.091969   \n",
       "...       ...   ...           ...           ...           ...          ...   \n",
       "1874995  3124   595     -0.367432     -0.845648      0.503707    -0.461266   \n",
       "1874996  3124   596     -0.326315     -0.845833      0.578875    -0.474109   \n",
       "1874997  3124   597     -0.300794     -0.859728      0.652722    -0.436694   \n",
       "1874998  3124   598     -0.253120     -0.886873      0.673002    -0.367051   \n",
       "1874999  3124   599     -0.180373     -0.916966      0.695634    -0.273040   \n",
       "\n",
       "         before_gy_y  before_gy_z      acc_x     acc_y      acc_z      gy_x  \\\n",
       "0          -0.410859    -0.461007  27.356382  8.807207  19.465910  0.376992   \n",
       "1          -0.528052    -0.365459  -0.054866  0.833464   0.820412 -0.282128   \n",
       "2          -0.596041    -0.366767   0.024046  0.315921   0.081086 -0.182551   \n",
       "3          -0.725311    -0.401232   0.065632  0.117634  -0.040874 -0.194863   \n",
       "4          -0.784094    -0.408413   0.151477  0.300751   0.317742 -0.350724   \n",
       "...              ...          ...        ...       ...        ...       ...   \n",
       "1874995    -1.413128    -1.092639   0.365037  0.011656   0.845701  0.080839   \n",
       "1874996    -1.382884    -1.097377  10.220817  5.476964   7.441373  3.605246   \n",
       "1874997    -1.341672    -1.136169   0.386337  0.177768  -0.080193 -0.192468   \n",
       "1874998    -1.346630    -1.148831   0.728823  0.014037   0.350745  0.136284   \n",
       "1874999    -1.360851    -1.154731   0.886204  0.075614   1.107553 -0.182228   \n",
       "\n",
       "              gy_y       gy_z  acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  \\\n",
       "0         0.869226   0.150423    0.495681  -0.272719      -0.276391  0.000027   \n",
       "1        -0.093560   0.011266    0.742974  -0.236152      -0.240632  0.416836   \n",
       "2        -0.053585  -0.003708    0.819822  -0.169815      -0.173080  0.086405   \n",
       "3         0.154242   0.005408    0.785669  -0.035229      -0.040560 -0.058780   \n",
       "4         0.494539   0.154354    0.791528   0.021954       0.016872  0.039823   \n",
       "...            ...        ...         ...        ...            ...       ...   \n",
       "1874995   0.350395   0.112282   -0.138940   0.829394       0.823900  0.151679   \n",
       "1874996  16.530576  11.843241   -0.167578   0.814816       0.809618  0.150658   \n",
       "1874997  -0.033904  -0.227861   -0.151875   0.802027       0.797338  0.093524   \n",
       "1874998   1.281790   0.403540   -0.175811   0.801880       0.797431  0.174681   \n",
       "1874999   0.894062   0.311408   -0.223043   0.803421       0.799233  0.266539   \n",
       "\n",
       "         acc_y_dt  acc_z_dt   gy_x_dt   gy_y_dt   gy_z_dt  acc_Energy_dt  \\\n",
       "0        0.000298 -0.000433  0.000347  0.000373  0.000273       0.000101   \n",
       "1       -0.118821 -0.255054  0.032738 -0.349095  0.377085       0.564992   \n",
       "2        0.023750 -0.531727 -0.141582 -0.202368 -0.004887       0.175645   \n",
       "3       -0.213920  0.285459  0.229520 -0.385106 -0.135647      -0.077915   \n",
       "4        0.259227 -0.055206  0.057320 -0.174917 -0.028047       0.013483   \n",
       "...           ...       ...       ...       ...       ...            ...   \n",
       "1874995  0.037205  0.119409 -0.108728 -0.027804 -0.009085      -0.142794   \n",
       "1874996 -0.000363  0.265559 -0.027936  0.090560 -0.018412      -0.065316   \n",
       "1874997 -0.049283  0.260884  0.082744  0.123264 -0.152712       0.035970   \n",
       "1874998 -0.096564  0.071332  0.153722 -0.014412 -0.049662      -0.054574   \n",
       "1874999 -0.107081  0.079654  0.207388 -0.042034 -0.022996      -0.107792   \n",
       "\n",
       "         gy_Energy_dt  gy_acc_Energy_dt  \n",
       "0            0.001505          0.001501  \n",
       "1            0.166566          0.162871  \n",
       "2            0.300944          0.306341  \n",
       "3            0.609008          0.599518  \n",
       "4            0.259626          0.260669  \n",
       "...               ...               ...  \n",
       "1874995      0.063329          0.063674  \n",
       "1874996     -0.064300         -0.062949  \n",
       "1874997     -0.056225         -0.053918  \n",
       "1874998      0.000843          0.001922  \n",
       "1874999      0.008458          0.009633  \n",
       "\n",
       "[1875000 rows x 26 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.iloc[:, 2:])\n",
    "scaled_data = scaler.transform(df.iloc[:, 2:])\n",
    "\n",
    "df.iloc[:, 2:] = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "# plt.plot(df[df['id']==2]['before_acc_x'])\n",
    "plt.plot(df[df['id']==2]['acc_x'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "# plt.plot(df[df['id']==0]['before_acc_x'])\n",
    "plt.plot(df[df['id']==0]['acc_x'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "plt.plot(df[df['id']==0]['before_acc_x'])\n",
    "plt.plot(df[df['id']==120]['acc_x'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard scaling 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=train.columns\n",
    "train_s=train.copy()\n",
    "test_s=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_s.iloc[:,2:]= scaler.fit_transform(train_s.iloc[:,2:])\n",
    "train_sc = pd.DataFrame(data = train_s,columns =col)\n",
    "\n",
    "test_s.iloc[:,2:]= scaler.transform(test_s.iloc[:,2:])\n",
    "test_sc = pd.DataFrame(data = test_s,columns =col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "      <th>acc_Energy</th>\n",
       "      <th>gy_Energy</th>\n",
       "      <th>gy_acc_Energy</th>\n",
       "      <th>acc_x_dt</th>\n",
       "      <th>acc_y_dt</th>\n",
       "      <th>acc_z_dt</th>\n",
       "      <th>gy_x_dt</th>\n",
       "      <th>gy_y_dt</th>\n",
       "      <th>gy_z_dt</th>\n",
       "      <th>acc_Energy_dt</th>\n",
       "      <th>gy_Energy_dt</th>\n",
       "      <th>gy_acc_Energy_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.356382</td>\n",
       "      <td>8.807207</td>\n",
       "      <td>19.465910</td>\n",
       "      <td>0.376992</td>\n",
       "      <td>0.869226</td>\n",
       "      <td>0.150423</td>\n",
       "      <td>0.495681</td>\n",
       "      <td>-0.272719</td>\n",
       "      <td>-0.276391</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.054866</td>\n",
       "      <td>0.833464</td>\n",
       "      <td>0.820412</td>\n",
       "      <td>-0.282128</td>\n",
       "      <td>-0.093560</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.742974</td>\n",
       "      <td>-0.236152</td>\n",
       "      <td>-0.240632</td>\n",
       "      <td>0.416836</td>\n",
       "      <td>-0.118821</td>\n",
       "      <td>-0.255054</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>-0.349095</td>\n",
       "      <td>0.377085</td>\n",
       "      <td>0.564992</td>\n",
       "      <td>0.166566</td>\n",
       "      <td>0.162871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.024046</td>\n",
       "      <td>0.315921</td>\n",
       "      <td>0.081086</td>\n",
       "      <td>-0.182551</td>\n",
       "      <td>-0.053585</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>0.819822</td>\n",
       "      <td>-0.169815</td>\n",
       "      <td>-0.173080</td>\n",
       "      <td>0.086405</td>\n",
       "      <td>0.023750</td>\n",
       "      <td>-0.531727</td>\n",
       "      <td>-0.141582</td>\n",
       "      <td>-0.202368</td>\n",
       "      <td>-0.004887</td>\n",
       "      <td>0.175645</td>\n",
       "      <td>0.300944</td>\n",
       "      <td>0.306341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>0.117634</td>\n",
       "      <td>-0.040874</td>\n",
       "      <td>-0.194863</td>\n",
       "      <td>0.154242</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.785669</td>\n",
       "      <td>-0.035229</td>\n",
       "      <td>-0.040560</td>\n",
       "      <td>-0.058780</td>\n",
       "      <td>-0.213920</td>\n",
       "      <td>0.285459</td>\n",
       "      <td>0.229520</td>\n",
       "      <td>-0.385106</td>\n",
       "      <td>-0.135647</td>\n",
       "      <td>-0.077915</td>\n",
       "      <td>0.609008</td>\n",
       "      <td>0.599518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.151477</td>\n",
       "      <td>0.300751</td>\n",
       "      <td>0.317742</td>\n",
       "      <td>-0.350724</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.154354</td>\n",
       "      <td>0.791528</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>0.039823</td>\n",
       "      <td>0.259227</td>\n",
       "      <td>-0.055206</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>-0.174917</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.013483</td>\n",
       "      <td>0.259626</td>\n",
       "      <td>0.260669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874995</th>\n",
       "      <td>3124</td>\n",
       "      <td>595</td>\n",
       "      <td>0.365037</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.845701</td>\n",
       "      <td>0.080839</td>\n",
       "      <td>0.350395</td>\n",
       "      <td>0.112282</td>\n",
       "      <td>-0.138940</td>\n",
       "      <td>0.829394</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>0.151679</td>\n",
       "      <td>0.037205</td>\n",
       "      <td>0.119409</td>\n",
       "      <td>-0.108728</td>\n",
       "      <td>-0.027804</td>\n",
       "      <td>-0.009085</td>\n",
       "      <td>-0.142794</td>\n",
       "      <td>0.063329</td>\n",
       "      <td>0.063674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874996</th>\n",
       "      <td>3124</td>\n",
       "      <td>596</td>\n",
       "      <td>10.220817</td>\n",
       "      <td>5.476964</td>\n",
       "      <td>7.441373</td>\n",
       "      <td>3.605246</td>\n",
       "      <td>16.530576</td>\n",
       "      <td>11.843241</td>\n",
       "      <td>-0.167578</td>\n",
       "      <td>0.814816</td>\n",
       "      <td>0.809618</td>\n",
       "      <td>0.150658</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.265559</td>\n",
       "      <td>-0.027936</td>\n",
       "      <td>0.090560</td>\n",
       "      <td>-0.018412</td>\n",
       "      <td>-0.065316</td>\n",
       "      <td>-0.064300</td>\n",
       "      <td>-0.062949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874997</th>\n",
       "      <td>3124</td>\n",
       "      <td>597</td>\n",
       "      <td>0.386337</td>\n",
       "      <td>0.177768</td>\n",
       "      <td>-0.080193</td>\n",
       "      <td>-0.192468</td>\n",
       "      <td>-0.033904</td>\n",
       "      <td>-0.227861</td>\n",
       "      <td>-0.151875</td>\n",
       "      <td>0.802027</td>\n",
       "      <td>0.797338</td>\n",
       "      <td>0.093524</td>\n",
       "      <td>-0.049283</td>\n",
       "      <td>0.260884</td>\n",
       "      <td>0.082744</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>-0.152712</td>\n",
       "      <td>0.035970</td>\n",
       "      <td>-0.056225</td>\n",
       "      <td>-0.053918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874998</th>\n",
       "      <td>3124</td>\n",
       "      <td>598</td>\n",
       "      <td>0.728823</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.350745</td>\n",
       "      <td>0.136284</td>\n",
       "      <td>1.281790</td>\n",
       "      <td>0.403540</td>\n",
       "      <td>-0.175811</td>\n",
       "      <td>0.801880</td>\n",
       "      <td>0.797431</td>\n",
       "      <td>0.174681</td>\n",
       "      <td>-0.096564</td>\n",
       "      <td>0.071332</td>\n",
       "      <td>0.153722</td>\n",
       "      <td>-0.014412</td>\n",
       "      <td>-0.049662</td>\n",
       "      <td>-0.054574</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874999</th>\n",
       "      <td>3124</td>\n",
       "      <td>599</td>\n",
       "      <td>0.886204</td>\n",
       "      <td>0.075614</td>\n",
       "      <td>1.107553</td>\n",
       "      <td>-0.182228</td>\n",
       "      <td>0.894062</td>\n",
       "      <td>0.311408</td>\n",
       "      <td>-0.223043</td>\n",
       "      <td>0.803421</td>\n",
       "      <td>0.799233</td>\n",
       "      <td>0.266539</td>\n",
       "      <td>-0.107081</td>\n",
       "      <td>0.079654</td>\n",
       "      <td>0.207388</td>\n",
       "      <td>-0.042034</td>\n",
       "      <td>-0.022996</td>\n",
       "      <td>-0.107792</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1875000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  time      acc_x     acc_y      acc_z      gy_x       gy_y  \\\n",
       "0           0     0  27.356382  8.807207  19.465910  0.376992   0.869226   \n",
       "1           0     1  -0.054866  0.833464   0.820412 -0.282128  -0.093560   \n",
       "2           0     2   0.024046  0.315921   0.081086 -0.182551  -0.053585   \n",
       "3           0     3   0.065632  0.117634  -0.040874 -0.194863   0.154242   \n",
       "4           0     4   0.151477  0.300751   0.317742 -0.350724   0.494539   \n",
       "...       ...   ...        ...       ...        ...       ...        ...   \n",
       "1874995  3124   595   0.365037  0.011656   0.845701  0.080839   0.350395   \n",
       "1874996  3124   596  10.220817  5.476964   7.441373  3.605246  16.530576   \n",
       "1874997  3124   597   0.386337  0.177768  -0.080193 -0.192468  -0.033904   \n",
       "1874998  3124   598   0.728823  0.014037   0.350745  0.136284   1.281790   \n",
       "1874999  3124   599   0.886204  0.075614   1.107553 -0.182228   0.894062   \n",
       "\n",
       "              gy_z  acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  \\\n",
       "0         0.150423    0.495681  -0.272719      -0.276391  0.000027  0.000298   \n",
       "1         0.011266    0.742974  -0.236152      -0.240632  0.416836 -0.118821   \n",
       "2        -0.003708    0.819822  -0.169815      -0.173080  0.086405  0.023750   \n",
       "3         0.005408    0.785669  -0.035229      -0.040560 -0.058780 -0.213920   \n",
       "4         0.154354    0.791528   0.021954       0.016872  0.039823  0.259227   \n",
       "...            ...         ...        ...            ...       ...       ...   \n",
       "1874995   0.112282   -0.138940   0.829394       0.823900  0.151679  0.037205   \n",
       "1874996  11.843241   -0.167578   0.814816       0.809618  0.150658 -0.000363   \n",
       "1874997  -0.227861   -0.151875   0.802027       0.797338  0.093524 -0.049283   \n",
       "1874998   0.403540   -0.175811   0.801880       0.797431  0.174681 -0.096564   \n",
       "1874999   0.311408   -0.223043   0.803421       0.799233  0.266539 -0.107081   \n",
       "\n",
       "         acc_z_dt   gy_x_dt   gy_y_dt   gy_z_dt  acc_Energy_dt  gy_Energy_dt  \\\n",
       "0       -0.000433  0.000347  0.000373  0.000273       0.000101      0.001505   \n",
       "1       -0.255054  0.032738 -0.349095  0.377085       0.564992      0.166566   \n",
       "2       -0.531727 -0.141582 -0.202368 -0.004887       0.175645      0.300944   \n",
       "3        0.285459  0.229520 -0.385106 -0.135647      -0.077915      0.609008   \n",
       "4       -0.055206  0.057320 -0.174917 -0.028047       0.013483      0.259626   \n",
       "...           ...       ...       ...       ...            ...           ...   \n",
       "1874995  0.119409 -0.108728 -0.027804 -0.009085      -0.142794      0.063329   \n",
       "1874996  0.265559 -0.027936  0.090560 -0.018412      -0.065316     -0.064300   \n",
       "1874997  0.260884  0.082744  0.123264 -0.152712       0.035970     -0.056225   \n",
       "1874998  0.071332  0.153722 -0.014412 -0.049662      -0.054574      0.000843   \n",
       "1874999  0.079654  0.207388 -0.042034 -0.022996      -0.107792      0.008458   \n",
       "\n",
       "         gy_acc_Energy_dt  \n",
       "0                0.001501  \n",
       "1                0.162871  \n",
       "2                0.306341  \n",
       "3                0.599518  \n",
       "4                0.260669  \n",
       "...                   ...  \n",
       "1874995          0.063674  \n",
       "1874996         -0.062949  \n",
       "1874997         -0.053918  \n",
       "1874998          0.001922  \n",
       "1874999          0.009633  \n",
       "\n",
       "[1875000 rows x 20 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델링\n",
    "\n",
    "+ CNN, LSTM, CNN+LSTM 등 여러 구조 적용해보다가 CNN에서 Flatten 없이 Global average pooling 한 구조가 가장 성능이 좋아 채택했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM,Bidirectional,Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from numpy.random import seed\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 600, 18)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array(train_sc.iloc[:,2:]).reshape(3125, 600, -1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 600, 18)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x=np.array(test_sc.iloc[:,2:]).reshape(782, 600, -1)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 61)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_labels['label'].values\n",
    "y = tf.keras.utils.to_categorical(train_labels['label']) \n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 구조 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_shape, classes):\n",
    "    seed(2021)\n",
    "    tf.random.set_seed(2021)\n",
    "    \n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\n",
    "    \n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n",
    "    \n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    \n",
    "    output_layer = keras.layers.Dense(classes, activation='softmax')(gap)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10-fold StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Fold_1--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 185ms/step - loss: 3.3854 - accuracy: 0.3490 - val_loss: 3.1161 - val_accuracy: 0.3099\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.9729 - accuracy: 0.5509 - val_loss: 1.9228 - val_accuracy: 0.5304\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.7849 - accuracy: 0.5650 - val_loss: 2.0362 - val_accuracy: 0.5495\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.5552 - accuracy: 0.6141 - val_loss: 1.9158 - val_accuracy: 0.5623\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.3835 - accuracy: 0.6450 - val_loss: 1.5137 - val_accuracy: 0.6070\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.2684 - accuracy: 0.6751 - val_loss: 1.4040 - val_accuracy: 0.6230\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.1646 - accuracy: 0.6891 - val_loss: 1.2561 - val_accuracy: 0.6550\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.0603 - accuracy: 0.7168 - val_loss: 1.1957 - val_accuracy: 0.6773\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.9702 - accuracy: 0.7390 - val_loss: 1.1095 - val_accuracy: 0.7093\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.9055 - accuracy: 0.7567 - val_loss: 0.9851 - val_accuracy: 0.7572\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.8772 - accuracy: 0.7569 - val_loss: 0.9508 - val_accuracy: 0.7732\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.7877 - accuracy: 0.7860 - val_loss: 1.1010 - val_accuracy: 0.7316\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7497 - accuracy: 0.7816 - val_loss: 0.9534 - val_accuracy: 0.7732\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.7095 - accuracy: 0.8003 - val_loss: 0.8475 - val_accuracy: 0.7732\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.6783 - accuracy: 0.8029 - val_loss: 0.7799 - val_accuracy: 0.8083\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.6177 - accuracy: 0.8349 - val_loss: 0.7713 - val_accuracy: 0.8179\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5864 - accuracy: 0.8382 - val_loss: 0.7485 - val_accuracy: 0.7891\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5729 - accuracy: 0.8265 - val_loss: 0.7256 - val_accuracy: 0.8083\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5377 - accuracy: 0.8494 - val_loss: 0.7081 - val_accuracy: 0.8051\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4999 - accuracy: 0.8579 - val_loss: 0.6763 - val_accuracy: 0.8051\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5073 - accuracy: 0.8556 - val_loss: 0.7458 - val_accuracy: 0.8147\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4766 - accuracy: 0.8569 - val_loss: 0.6444 - val_accuracy: 0.8019\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4479 - accuracy: 0.8716 - val_loss: 0.6840 - val_accuracy: 0.7955\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4305 - accuracy: 0.8771 - val_loss: 0.6818 - val_accuracy: 0.8211\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4523 - accuracy: 0.8773 - val_loss: 0.6613 - val_accuracy: 0.8115\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 178ms/step - loss: 0.4024 - accuracy: 0.8820 - val_loss: 0.7498 - val_accuracy: 0.7764\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3788 - accuracy: 0.8973 - val_loss: 0.6059 - val_accuracy: 0.8307\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3580 - accuracy: 0.9026 - val_loss: 0.6103 - val_accuracy: 0.8307\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3496 - accuracy: 0.8991 - val_loss: 0.6190 - val_accuracy: 0.8211\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3652 - accuracy: 0.9048 - val_loss: 0.5941 - val_accuracy: 0.8211\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3484 - accuracy: 0.9052 - val_loss: 0.6028 - val_accuracy: 0.8339\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3325 - accuracy: 0.9054 - val_loss: 0.5876 - val_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3063 - accuracy: 0.9182 - val_loss: 0.6011 - val_accuracy: 0.8179\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3194 - accuracy: 0.9144 - val_loss: 0.5761 - val_accuracy: 0.8339\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3116 - accuracy: 0.9118 - val_loss: 0.6350 - val_accuracy: 0.8243\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3211 - accuracy: 0.9072 - val_loss: 0.5904 - val_accuracy: 0.8275\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3028 - accuracy: 0.9183 - val_loss: 0.5898 - val_accuracy: 0.8371\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.2931 - accuracy: 0.9176 - val_loss: 0.5957 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2801 - accuracy: 0.9305 - val_loss: 0.5583 - val_accuracy: 0.8403\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2811 - accuracy: 0.9214 - val_loss: 0.5523 - val_accuracy: 0.8562\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2610 - accuracy: 0.9373 - val_loss: 0.5557 - val_accuracy: 0.8371\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2750 - accuracy: 0.9306 - val_loss: 0.5534 - val_accuracy: 0.8530\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2767 - accuracy: 0.9286 - val_loss: 0.5693 - val_accuracy: 0.8339\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2533 - accuracy: 0.9424 - val_loss: 0.5564 - val_accuracy: 0.8466\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2422 - accuracy: 0.9414 - val_loss: 0.5611 - val_accuracy: 0.8435\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2499 - accuracy: 0.9401 - val_loss: 0.5546 - val_accuracy: 0.8435\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2555 - accuracy: 0.9397 - val_loss: 0.5453 - val_accuracy: 0.8466\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2540 - accuracy: 0.9354 - val_loss: 0.5480 - val_accuracy: 0.8435\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2461 - accuracy: 0.9341 - val_loss: 0.5491 - val_accuracy: 0.8403\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2496 - accuracy: 0.9324 - val_loss: 0.5485 - val_accuracy: 0.8339\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2358 - accuracy: 0.9444 - val_loss: 0.5484 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2754 - accuracy: 0.9279 - val_loss: 0.5412 - val_accuracy: 0.8466\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2346 - accuracy: 0.9443 - val_loss: 0.5386 - val_accuracy: 0.8530\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2356 - accuracy: 0.9446 - val_loss: 0.5421 - val_accuracy: 0.8466\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2563 - accuracy: 0.9405 - val_loss: 0.5407 - val_accuracy: 0.8435\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2562 - accuracy: 0.9359 - val_loss: 0.5375 - val_accuracy: 0.8466\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2383 - accuracy: 0.9392 - val_loss: 0.5416 - val_accuracy: 0.8498\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2440 - accuracy: 0.9389 - val_loss: 0.5423 - val_accuracy: 0.8498\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2208 - accuracy: 0.9490 - val_loss: 0.5383 - val_accuracy: 0.8466\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2503 - accuracy: 0.9347 - val_loss: 0.5397 - val_accuracy: 0.8562\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2480 - accuracy: 0.9395 - val_loss: 0.5383 - val_accuracy: 0.8466\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2378 - accuracy: 0.9472 - val_loss: 0.5382 - val_accuracy: 0.8466\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2290 - accuracy: 0.9416 - val_loss: 0.5374 - val_accuracy: 0.8435\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2339 - accuracy: 0.9422 - val_loss: 0.5380 - val_accuracy: 0.8530\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2181 - accuracy: 0.9483 - val_loss: 0.5376 - val_accuracy: 0.8530\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2257 - accuracy: 0.9516 - val_loss: 0.5380 - val_accuracy: 0.8498\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2389 - accuracy: 0.9484 - val_loss: 0.5388 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2349 - accuracy: 0.9403 - val_loss: 0.5367 - val_accuracy: 0.8530\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2331 - accuracy: 0.9515 - val_loss: 0.5369 - val_accuracy: 0.8498\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2231 - accuracy: 0.9447 - val_loss: 0.5368 - val_accuracy: 0.8498\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2284 - accuracy: 0.9447 - val_loss: 0.5377 - val_accuracy: 0.8498\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2134 - accuracy: 0.9516 - val_loss: 0.5371 - val_accuracy: 0.8562\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2547 - accuracy: 0.9403 - val_loss: 0.5374 - val_accuracy: 0.8530\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2351 - accuracy: 0.9413 - val_loss: 0.5372 - val_accuracy: 0.8498\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2234 - accuracy: 0.9460 - val_loss: 0.5364 - val_accuracy: 0.8530\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2282 - accuracy: 0.9344 - val_loss: 0.5362 - val_accuracy: 0.8530\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2287 - accuracy: 0.9387 - val_loss: 0.5366 - val_accuracy: 0.8562\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2490 - accuracy: 0.9350 - val_loss: 0.5372 - val_accuracy: 0.8498\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2233 - accuracy: 0.9461 - val_loss: 0.5365 - val_accuracy: 0.8498\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2231 - accuracy: 0.9430 - val_loss: 0.5368 - val_accuracy: 0.8466\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2360 - accuracy: 0.9453 - val_loss: 0.5365 - val_accuracy: 0.8466\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2301 - accuracy: 0.9430 - val_loss: 0.5364 - val_accuracy: 0.8466\n",
      "Epoch 83/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2491 - accuracy: 0.9348 - val_loss: 0.5363 - val_accuracy: 0.8562\n",
      "Epoch 84/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2242 - accuracy: 0.9492 - val_loss: 0.5361 - val_accuracy: 0.8530\n",
      "Epoch 85/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2314 - accuracy: 0.9452 - val_loss: 0.5362 - val_accuracy: 0.8562\n",
      "Epoch 86/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2424 - accuracy: 0.9401 - val_loss: 0.5358 - val_accuracy: 0.8562\n",
      "Epoch 87/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2344 - accuracy: 0.9421 - val_loss: 0.5358 - val_accuracy: 0.8498\n",
      "Epoch 88/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2440 - accuracy: 0.9402 - val_loss: 0.5354 - val_accuracy: 0.8530\n",
      "Epoch 89/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2208 - accuracy: 0.9478 - val_loss: 0.5350 - val_accuracy: 0.8562\n",
      "Epoch 90/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2139 - accuracy: 0.9511 - val_loss: 0.5355 - val_accuracy: 0.8594\n",
      "Epoch 91/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2120 - accuracy: 0.9487 - val_loss: 0.5360 - val_accuracy: 0.8530\n",
      "Epoch 92/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2451 - accuracy: 0.9394 - val_loss: 0.5361 - val_accuracy: 0.8530\n",
      "Epoch 93/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2361 - accuracy: 0.9440 - val_loss: 0.5362 - val_accuracy: 0.8530\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 94/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2450 - accuracy: 0.9396 - val_loss: 0.5362 - val_accuracy: 0.8562\n",
      "Epoch 95/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2265 - accuracy: 0.9500 - val_loss: 0.5360 - val_accuracy: 0.8530\n",
      "Epoch 96/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2161 - accuracy: 0.9508 - val_loss: 0.5359 - val_accuracy: 0.8562\n",
      "Epoch 97/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2201 - accuracy: 0.9533 - val_loss: 0.5360 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.5350 - accuracy: 0.8562\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.5350 - accuracy: 0.8562\n",
      "--------------------Fold_2--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 185ms/step - loss: 3.4087 - accuracy: 0.3354 - val_loss: 3.2235 - val_accuracy: 0.3099\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 2.0135 - accuracy: 0.5430 - val_loss: 1.8948 - val_accuracy: 0.5431\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.7351 - accuracy: 0.5815 - val_loss: 1.9706 - val_accuracy: 0.5431\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.5307 - accuracy: 0.6188 - val_loss: 1.6252 - val_accuracy: 0.5751\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.4244 - accuracy: 0.6295 - val_loss: 1.5192 - val_accuracy: 0.6006\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.2449 - accuracy: 0.6791 - val_loss: 1.4169 - val_accuracy: 0.6166\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.1708 - accuracy: 0.6863 - val_loss: 1.3115 - val_accuracy: 0.6486\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.1031 - accuracy: 0.7061 - val_loss: 1.2211 - val_accuracy: 0.6741\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.0043 - accuracy: 0.7249 - val_loss: 1.0456 - val_accuracy: 0.7284\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.9419 - accuracy: 0.7443 - val_loss: 0.9392 - val_accuracy: 0.7827\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.8386 - accuracy: 0.7699 - val_loss: 0.8931 - val_accuracy: 0.7508\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7998 - accuracy: 0.7750 - val_loss: 0.8766 - val_accuracy: 0.7540\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.7521 - accuracy: 0.7948 - val_loss: 0.7987 - val_accuracy: 0.7604\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.7254 - accuracy: 0.7906 - val_loss: 0.7729 - val_accuracy: 0.7955\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 186ms/step - loss: 0.6685 - accuracy: 0.8101 - val_loss: 0.7397 - val_accuracy: 0.7827\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6274 - accuracy: 0.8190 - val_loss: 0.8465 - val_accuracy: 0.7955\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6117 - accuracy: 0.8322 - val_loss: 0.6865 - val_accuracy: 0.7955\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5728 - accuracy: 0.8383 - val_loss: 0.6846 - val_accuracy: 0.8211\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.5298 - accuracy: 0.8597 - val_loss: 0.6226 - val_accuracy: 0.8211\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5011 - accuracy: 0.8489 - val_loss: 0.6331 - val_accuracy: 0.8147\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4866 - accuracy: 0.8646 - val_loss: 0.6286 - val_accuracy: 0.8339\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4680 - accuracy: 0.8746 - val_loss: 0.6417 - val_accuracy: 0.8275\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4753 - accuracy: 0.8648 - val_loss: 0.6173 - val_accuracy: 0.8115\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4288 - accuracy: 0.8803 - val_loss: 0.6333 - val_accuracy: 0.7923\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 188ms/step - loss: 0.4296 - accuracy: 0.8705 - val_loss: 0.6220 - val_accuracy: 0.8147\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4099 - accuracy: 0.8836 - val_loss: 0.6293 - val_accuracy: 0.8083\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4127 - accuracy: 0.8866 - val_loss: 0.5709 - val_accuracy: 0.8051\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4006 - accuracy: 0.8891 - val_loss: 0.5567 - val_accuracy: 0.8403\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3776 - accuracy: 0.8888 - val_loss: 0.5581 - val_accuracy: 0.8307\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3808 - accuracy: 0.8930 - val_loss: 0.5948 - val_accuracy: 0.8275\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3598 - accuracy: 0.9034 - val_loss: 0.5556 - val_accuracy: 0.8243\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3411 - accuracy: 0.8982 - val_loss: 0.6639 - val_accuracy: 0.7891\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3480 - accuracy: 0.8964 - val_loss: 0.5686 - val_accuracy: 0.8275\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3155 - accuracy: 0.9131 - val_loss: 0.5616 - val_accuracy: 0.8403\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3155 - accuracy: 0.9220 - val_loss: 0.5170 - val_accuracy: 0.8403\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3247 - accuracy: 0.9141 - val_loss: 0.5410 - val_accuracy: 0.8403\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3020 - accuracy: 0.9159 - val_loss: 0.5532 - val_accuracy: 0.8211\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3131 - accuracy: 0.9052 - val_loss: 0.5720 - val_accuracy: 0.8243\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2854 - accuracy: 0.9204 - val_loss: 0.5679 - val_accuracy: 0.8115\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2832 - accuracy: 0.9213 - val_loss: 0.5431 - val_accuracy: 0.8243\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2679 - accuracy: 0.9200 - val_loss: 0.5185 - val_accuracy: 0.8371\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2805 - accuracy: 0.9242 - val_loss: 0.5026 - val_accuracy: 0.8339\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2378 - accuracy: 0.9424 - val_loss: 0.5049 - val_accuracy: 0.8307\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2617 - accuracy: 0.9250 - val_loss: 0.5113 - val_accuracy: 0.8403\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2568 - accuracy: 0.9312 - val_loss: 0.5401 - val_accuracy: 0.8307\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2419 - accuracy: 0.9366 - val_loss: 0.5186 - val_accuracy: 0.8403\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2356 - accuracy: 0.9427 - val_loss: 0.4889 - val_accuracy: 0.8530\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2099 - accuracy: 0.9500 - val_loss: 0.4758 - val_accuracy: 0.8466\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2376 - accuracy: 0.9379 - val_loss: 0.4826 - val_accuracy: 0.8562\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2010 - accuracy: 0.9500 - val_loss: 0.5051 - val_accuracy: 0.8371\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2085 - accuracy: 0.9518 - val_loss: 0.4931 - val_accuracy: 0.8371\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2029 - accuracy: 0.9488 - val_loss: 0.4882 - val_accuracy: 0.8530\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.1980 - accuracy: 0.9525 - val_loss: 0.4832 - val_accuracy: 0.8466\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1844 - accuracy: 0.9590 - val_loss: 0.4919 - val_accuracy: 0.8435\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1980 - accuracy: 0.9522 - val_loss: 0.4759 - val_accuracy: 0.8498\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1919 - accuracy: 0.9563 - val_loss: 0.4871 - val_accuracy: 0.8466\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4758 - accuracy: 0.8466\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4758 - accuracy: 0.8466\n",
      "--------------------Fold_3--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 187ms/step - loss: 3.3707 - accuracy: 0.3413 - val_loss: 3.1703 - val_accuracy: 0.3323\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.9793 - accuracy: 0.5496 - val_loss: 1.8589 - val_accuracy: 0.5527\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.7007 - accuracy: 0.5952 - val_loss: 1.7976 - val_accuracy: 0.5527\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.5332 - accuracy: 0.6167 - val_loss: 1.5757 - val_accuracy: 0.5751\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.3839 - accuracy: 0.6454 - val_loss: 1.5885 - val_accuracy: 0.6070\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.2320 - accuracy: 0.6813 - val_loss: 1.3246 - val_accuracy: 0.6326\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.1608 - accuracy: 0.6954 - val_loss: 1.1648 - val_accuracy: 0.6901\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.0542 - accuracy: 0.7217 - val_loss: 1.0786 - val_accuracy: 0.7220\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.0407 - accuracy: 0.7128 - val_loss: 1.0472 - val_accuracy: 0.7380\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.8956 - accuracy: 0.7591 - val_loss: 0.9337 - val_accuracy: 0.7508\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.8656 - accuracy: 0.7490 - val_loss: 0.9020 - val_accuracy: 0.7764\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.7914 - accuracy: 0.7917 - val_loss: 0.8772 - val_accuracy: 0.7796\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7463 - accuracy: 0.7862 - val_loss: 0.7643 - val_accuracy: 0.7955\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.7211 - accuracy: 0.7968 - val_loss: 0.7634 - val_accuracy: 0.7764\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6327 - accuracy: 0.8098 - val_loss: 0.7711 - val_accuracy: 0.7732\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.6020 - accuracy: 0.8289 - val_loss: 0.7412 - val_accuracy: 0.7827\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6137 - accuracy: 0.8178 - val_loss: 0.7279 - val_accuracy: 0.7955\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.5618 - accuracy: 0.8358 - val_loss: 0.6913 - val_accuracy: 0.8115\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.5359 - accuracy: 0.8587 - val_loss: 0.6123 - val_accuracy: 0.8339\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5035 - accuracy: 0.8522 - val_loss: 0.6428 - val_accuracy: 0.8243\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.4999 - accuracy: 0.8540 - val_loss: 0.6539 - val_accuracy: 0.8498\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4630 - accuracy: 0.8726 - val_loss: 0.6243 - val_accuracy: 0.8211\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4761 - accuracy: 0.8609 - val_loss: 0.5717 - val_accuracy: 0.8211\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4630 - accuracy: 0.8621 - val_loss: 0.5867 - val_accuracy: 0.8371\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4230 - accuracy: 0.8709 - val_loss: 0.5808 - val_accuracy: 0.8147\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4008 - accuracy: 0.8808 - val_loss: 0.5329 - val_accuracy: 0.8530\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4312 - accuracy: 0.8709 - val_loss: 0.5058 - val_accuracy: 0.8498\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3935 - accuracy: 0.8863 - val_loss: 0.5552 - val_accuracy: 0.8339\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3774 - accuracy: 0.8904 - val_loss: 0.5389 - val_accuracy: 0.8339\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3765 - accuracy: 0.8887 - val_loss: 0.6882 - val_accuracy: 0.7891\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 189ms/step - loss: 0.3775 - accuracy: 0.8885 - val_loss: 0.5027 - val_accuracy: 0.8339\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.3447 - accuracy: 0.9050 - val_loss: 0.5355 - val_accuracy: 0.8498\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3496 - accuracy: 0.8972 - val_loss: 0.5862 - val_accuracy: 0.8115\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3453 - accuracy: 0.8954 - val_loss: 0.6494 - val_accuracy: 0.8051\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3640 - accuracy: 0.8975 - val_loss: 0.5230 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2908 - accuracy: 0.9207 - val_loss: 0.4986 - val_accuracy: 0.8339\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2750 - accuracy: 0.9285 - val_loss: 0.4826 - val_accuracy: 0.8403\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2764 - accuracy: 0.9203 - val_loss: 0.4692 - val_accuracy: 0.8466\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2670 - accuracy: 0.9243 - val_loss: 0.4576 - val_accuracy: 0.8466\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2723 - accuracy: 0.9171 - val_loss: 0.4698 - val_accuracy: 0.8594\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2759 - accuracy: 0.9263 - val_loss: 0.4611 - val_accuracy: 0.8562\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2813 - accuracy: 0.9179 - val_loss: 0.4983 - val_accuracy: 0.8403\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2715 - accuracy: 0.9326 - val_loss: 0.4953 - val_accuracy: 0.8403\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2428 - accuracy: 0.9357 - val_loss: 0.4471 - val_accuracy: 0.8498\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2364 - accuracy: 0.9393 - val_loss: 0.4531 - val_accuracy: 0.8594\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2424 - accuracy: 0.9323 - val_loss: 0.4456 - val_accuracy: 0.8562\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.2274 - accuracy: 0.9403 - val_loss: 0.4389 - val_accuracy: 0.8498\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2398 - accuracy: 0.9371 - val_loss: 0.4363 - val_accuracy: 0.8626\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 186ms/step - loss: 0.2291 - accuracy: 0.9438 - val_loss: 0.4325 - val_accuracy: 0.8658\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 186ms/step - loss: 0.2295 - accuracy: 0.9362 - val_loss: 0.4354 - val_accuracy: 0.8658\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2243 - accuracy: 0.9385 - val_loss: 0.4489 - val_accuracy: 0.8530\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2577 - accuracy: 0.9284 - val_loss: 0.4308 - val_accuracy: 0.8626\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.2270 - accuracy: 0.9383 - val_loss: 0.4364 - val_accuracy: 0.8498\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2430 - accuracy: 0.9327 - val_loss: 0.4508 - val_accuracy: 0.8466\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2189 - accuracy: 0.9462 - val_loss: 0.4331 - val_accuracy: 0.8658\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2236 - accuracy: 0.9396 - val_loss: 0.4307 - val_accuracy: 0.8594\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2248 - accuracy: 0.9387 - val_loss: 0.4437 - val_accuracy: 0.8562\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2135 - accuracy: 0.9402 - val_loss: 0.4482 - val_accuracy: 0.8594\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2162 - accuracy: 0.9399 - val_loss: 0.4335 - val_accuracy: 0.8594\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2263 - accuracy: 0.9412 - val_loss: 0.4398 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.1910 - accuracy: 0.9524 - val_loss: 0.4310 - val_accuracy: 0.8594\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2102 - accuracy: 0.9418 - val_loss: 0.4277 - val_accuracy: 0.8626\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2097 - accuracy: 0.9498 - val_loss: 0.4274 - val_accuracy: 0.8594\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1766 - accuracy: 0.9544 - val_loss: 0.4246 - val_accuracy: 0.8690\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2054 - accuracy: 0.9526 - val_loss: 0.4281 - val_accuracy: 0.8722\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2100 - accuracy: 0.9511 - val_loss: 0.4312 - val_accuracy: 0.8626\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.1923 - accuracy: 0.9466 - val_loss: 0.4374 - val_accuracy: 0.8626\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1891 - accuracy: 0.9556 - val_loss: 0.4293 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1789 - accuracy: 0.9521 - val_loss: 0.4256 - val_accuracy: 0.8690\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1929 - accuracy: 0.9507 - val_loss: 0.4231 - val_accuracy: 0.8626\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1800 - accuracy: 0.9566 - val_loss: 0.4249 - val_accuracy: 0.8658\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1862 - accuracy: 0.9520 - val_loss: 0.4206 - val_accuracy: 0.8658\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1891 - accuracy: 0.9561 - val_loss: 0.4212 - val_accuracy: 0.8626\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1877 - accuracy: 0.9607 - val_loss: 0.4204 - val_accuracy: 0.8690\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 8s 186ms/step - loss: 0.1918 - accuracy: 0.9479 - val_loss: 0.4192 - val_accuracy: 0.8722\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1799 - accuracy: 0.9487 - val_loss: 0.4199 - val_accuracy: 0.8658\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1848 - accuracy: 0.9539 - val_loss: 0.4212 - val_accuracy: 0.8626\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.1967 - accuracy: 0.9463 - val_loss: 0.4224 - val_accuracy: 0.8594\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.1709 - accuracy: 0.9638 - val_loss: 0.4195 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.1849 - accuracy: 0.9563 - val_loss: 0.4214 - val_accuracy: 0.8690\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1910 - accuracy: 0.9533 - val_loss: 0.4207 - val_accuracy: 0.8690\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1838 - accuracy: 0.9545 - val_loss: 0.4208 - val_accuracy: 0.8690\n",
      "Epoch 83/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.1830 - accuracy: 0.9552 - val_loss: 0.4215 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4192 - accuracy: 0.8722\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4192 - accuracy: 0.8722\n",
      "--------------------Fold_4--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 183ms/step - loss: 3.3705 - accuracy: 0.3537 - val_loss: 3.0873 - val_accuracy: 0.3642\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.9608 - accuracy: 0.5554 - val_loss: 1.9848 - val_accuracy: 0.5431\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.7312 - accuracy: 0.5809 - val_loss: 1.9145 - val_accuracy: 0.5367\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.5519 - accuracy: 0.6207 - val_loss: 1.9543 - val_accuracy: 0.5240\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.3650 - accuracy: 0.6500 - val_loss: 1.6601 - val_accuracy: 0.5783\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.2733 - accuracy: 0.6592 - val_loss: 1.4666 - val_accuracy: 0.6326\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.1637 - accuracy: 0.6978 - val_loss: 1.3127 - val_accuracy: 0.6677\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.0988 - accuracy: 0.7065 - val_loss: 1.4022 - val_accuracy: 0.6454\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.9743 - accuracy: 0.7414 - val_loss: 1.1884 - val_accuracy: 0.6837\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.8994 - accuracy: 0.7401 - val_loss: 1.1106 - val_accuracy: 0.6997\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.8423 - accuracy: 0.7700 - val_loss: 1.0385 - val_accuracy: 0.7061\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7764 - accuracy: 0.7834 - val_loss: 1.0650 - val_accuracy: 0.7093\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7298 - accuracy: 0.7859 - val_loss: 1.0388 - val_accuracy: 0.7220\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6825 - accuracy: 0.8057 - val_loss: 0.9738 - val_accuracy: 0.7412\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6237 - accuracy: 0.8299 - val_loss: 0.9837 - val_accuracy: 0.7572\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6079 - accuracy: 0.8283 - val_loss: 0.8605 - val_accuracy: 0.7444\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5798 - accuracy: 0.8350 - val_loss: 0.8514 - val_accuracy: 0.7508\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5676 - accuracy: 0.8354 - val_loss: 0.8599 - val_accuracy: 0.7636\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5500 - accuracy: 0.8415 - val_loss: 0.8228 - val_accuracy: 0.7700\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.5329 - accuracy: 0.8458 - val_loss: 0.8255 - val_accuracy: 0.7732\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4898 - accuracy: 0.8700 - val_loss: 0.7658 - val_accuracy: 0.7923\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4554 - accuracy: 0.8632 - val_loss: 0.8014 - val_accuracy: 0.7668\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4943 - accuracy: 0.8523 - val_loss: 0.7794 - val_accuracy: 0.7891\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4388 - accuracy: 0.8760 - val_loss: 0.7431 - val_accuracy: 0.7955\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4394 - accuracy: 0.8754 - val_loss: 0.7632 - val_accuracy: 0.7796\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3971 - accuracy: 0.8957 - val_loss: 0.7526 - val_accuracy: 0.7827\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4238 - accuracy: 0.8705 - val_loss: 0.7913 - val_accuracy: 0.7668\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3999 - accuracy: 0.8777 - val_loss: 0.7204 - val_accuracy: 0.7827\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3721 - accuracy: 0.8900 - val_loss: 0.7637 - val_accuracy: 0.7891\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3727 - accuracy: 0.8964 - val_loss: 0.9099 - val_accuracy: 0.7764\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3743 - accuracy: 0.9016 - val_loss: 0.7481 - val_accuracy: 0.7955\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3619 - accuracy: 0.9061 - val_loss: 0.7348 - val_accuracy: 0.7732\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3369 - accuracy: 0.8985 - val_loss: 0.7024 - val_accuracy: 0.7827\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3113 - accuracy: 0.9130 - val_loss: 0.6557 - val_accuracy: 0.8051\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 178ms/step - loss: 0.3153 - accuracy: 0.9157 - val_loss: 0.6585 - val_accuracy: 0.8179\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2845 - accuracy: 0.9261 - val_loss: 0.6696 - val_accuracy: 0.7987\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2895 - accuracy: 0.9211 - val_loss: 0.6736 - val_accuracy: 0.8019\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2769 - accuracy: 0.9294 - val_loss: 0.6936 - val_accuracy: 0.8115\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2613 - accuracy: 0.9299 - val_loss: 0.6276 - val_accuracy: 0.8115\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2759 - accuracy: 0.9209 - val_loss: 0.6474 - val_accuracy: 0.8147\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2461 - accuracy: 0.9376 - val_loss: 0.6317 - val_accuracy: 0.8147\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2554 - accuracy: 0.9352 - val_loss: 0.6447 - val_accuracy: 0.8179\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2707 - accuracy: 0.9327 - val_loss: 0.6353 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2651 - accuracy: 0.9332 - val_loss: 0.6248 - val_accuracy: 0.8275\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2393 - accuracy: 0.9394 - val_loss: 0.6268 - val_accuracy: 0.8147\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2385 - accuracy: 0.9414 - val_loss: 0.6271 - val_accuracy: 0.8083\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2335 - accuracy: 0.9408 - val_loss: 0.6224 - val_accuracy: 0.8275\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2396 - accuracy: 0.9377 - val_loss: 0.6381 - val_accuracy: 0.8211\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2497 - accuracy: 0.9350 - val_loss: 0.6262 - val_accuracy: 0.8147\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2354 - accuracy: 0.9338 - val_loss: 0.6315 - val_accuracy: 0.8083\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2268 - accuracy: 0.9452 - val_loss: 0.6171 - val_accuracy: 0.8211\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2360 - accuracy: 0.9370 - val_loss: 0.6252 - val_accuracy: 0.8307\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2368 - accuracy: 0.9396 - val_loss: 0.6260 - val_accuracy: 0.8275\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2154 - accuracy: 0.9470 - val_loss: 0.6198 - val_accuracy: 0.8307\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2262 - accuracy: 0.9456 - val_loss: 0.6233 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2191 - accuracy: 0.9453 - val_loss: 0.6234 - val_accuracy: 0.8243\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2173 - accuracy: 0.9435 - val_loss: 0.6190 - val_accuracy: 0.8243\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2191 - accuracy: 0.9454 - val_loss: 0.6133 - val_accuracy: 0.8243\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2235 - accuracy: 0.9430 - val_loss: 0.6212 - val_accuracy: 0.8307\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2315 - accuracy: 0.9451 - val_loss: 0.6178 - val_accuracy: 0.8211\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2310 - accuracy: 0.9398 - val_loss: 0.6127 - val_accuracy: 0.8307\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2162 - accuracy: 0.9457 - val_loss: 0.6184 - val_accuracy: 0.8179\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2288 - accuracy: 0.9411 - val_loss: 0.6102 - val_accuracy: 0.8211\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2134 - accuracy: 0.9449 - val_loss: 0.6139 - val_accuracy: 0.8275\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2114 - accuracy: 0.9493 - val_loss: 0.6123 - val_accuracy: 0.8307\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2240 - accuracy: 0.9455 - val_loss: 0.6162 - val_accuracy: 0.8243\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2058 - accuracy: 0.9506 - val_loss: 0.6084 - val_accuracy: 0.8307\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2186 - accuracy: 0.9487 - val_loss: 0.6108 - val_accuracy: 0.8275\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2238 - accuracy: 0.9439 - val_loss: 0.6133 - val_accuracy: 0.8243\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2281 - accuracy: 0.9363 - val_loss: 0.6083 - val_accuracy: 0.8211\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2031 - accuracy: 0.9443 - val_loss: 0.6136 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2328 - accuracy: 0.9414 - val_loss: 0.6105 - val_accuracy: 0.8275\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2170 - accuracy: 0.9458 - val_loss: 0.6108 - val_accuracy: 0.8179\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2209 - accuracy: 0.9475 - val_loss: 0.6061 - val_accuracy: 0.8307\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2002 - accuracy: 0.9476 - val_loss: 0.6122 - val_accuracy: 0.8339\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2223 - accuracy: 0.9454 - val_loss: 0.6157 - val_accuracy: 0.8275\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2033 - accuracy: 0.9518 - val_loss: 0.6073 - val_accuracy: 0.8307\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2226 - accuracy: 0.9395 - val_loss: 0.6067 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2154 - accuracy: 0.9464 - val_loss: 0.6098 - val_accuracy: 0.8275\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2168 - accuracy: 0.9486 - val_loss: 0.6082 - val_accuracy: 0.8243\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2164 - accuracy: 0.9435 - val_loss: 0.6099 - val_accuracy: 0.8275\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2098 - accuracy: 0.9467 - val_loss: 0.6096 - val_accuracy: 0.8243\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.6061 - accuracy: 0.8307\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.6061 - accuracy: 0.8307\n",
      "--------------------Fold_5--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 183ms/step - loss: 3.4280 - accuracy: 0.3378 - val_loss: 3.4393 - val_accuracy: 0.2396\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 2.0556 - accuracy: 0.5336 - val_loss: 1.9430 - val_accuracy: 0.5463\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 1.7248 - accuracy: 0.5903 - val_loss: 2.1378 - val_accuracy: 0.5304\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.5707 - accuracy: 0.6246 - val_loss: 1.7869 - val_accuracy: 0.5527\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.3816 - accuracy: 0.6496 - val_loss: 1.5200 - val_accuracy: 0.5974\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.2676 - accuracy: 0.6763 - val_loss: 1.4007 - val_accuracy: 0.6230\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.1305 - accuracy: 0.7121 - val_loss: 1.2460 - val_accuracy: 0.6677\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.0768 - accuracy: 0.7092 - val_loss: 1.1612 - val_accuracy: 0.6901\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.9718 - accuracy: 0.7429 - val_loss: 1.0911 - val_accuracy: 0.7125\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.9313 - accuracy: 0.7524 - val_loss: 1.0051 - val_accuracy: 0.7188\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.8545 - accuracy: 0.7763 - val_loss: 0.9631 - val_accuracy: 0.7188\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.8050 - accuracy: 0.7759 - val_loss: 0.8820 - val_accuracy: 0.7540\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7213 - accuracy: 0.8033 - val_loss: 0.8155 - val_accuracy: 0.7796\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6971 - accuracy: 0.8063 - val_loss: 0.7620 - val_accuracy: 0.7827\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6521 - accuracy: 0.8230 - val_loss: 0.7350 - val_accuracy: 0.7955\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6420 - accuracy: 0.8105 - val_loss: 0.7100 - val_accuracy: 0.8019\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5669 - accuracy: 0.8405 - val_loss: 0.8417 - val_accuracy: 0.7476\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5699 - accuracy: 0.8232 - val_loss: 0.7480 - val_accuracy: 0.7955\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5413 - accuracy: 0.8475 - val_loss: 0.6902 - val_accuracy: 0.8083\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5385 - accuracy: 0.8374 - val_loss: 0.6434 - val_accuracy: 0.8243\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4914 - accuracy: 0.8522 - val_loss: 0.6675 - val_accuracy: 0.7891\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5073 - accuracy: 0.8561 - val_loss: 0.7400 - val_accuracy: 0.8147\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4607 - accuracy: 0.8662 - val_loss: 0.6328 - val_accuracy: 0.7891\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4829 - accuracy: 0.8561 - val_loss: 0.5920 - val_accuracy: 0.8147\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4482 - accuracy: 0.8700 - val_loss: 0.5627 - val_accuracy: 0.8147\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4531 - accuracy: 0.8638 - val_loss: 0.6285 - val_accuracy: 0.7955\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4168 - accuracy: 0.8757 - val_loss: 0.6083 - val_accuracy: 0.8179\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3940 - accuracy: 0.8820 - val_loss: 0.6033 - val_accuracy: 0.8019\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3920 - accuracy: 0.8846 - val_loss: 0.5705 - val_accuracy: 0.8243\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3404 - accuracy: 0.9044 - val_loss: 0.5270 - val_accuracy: 0.8307\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3628 - accuracy: 0.8972 - val_loss: 0.5058 - val_accuracy: 0.8371\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3467 - accuracy: 0.9075 - val_loss: 0.4960 - val_accuracy: 0.8307\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3262 - accuracy: 0.9062 - val_loss: 0.4962 - val_accuracy: 0.8339\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3161 - accuracy: 0.9143 - val_loss: 0.5253 - val_accuracy: 0.8403\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.3118 - accuracy: 0.9114 - val_loss: 0.6038 - val_accuracy: 0.8019\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3333 - accuracy: 0.9054 - val_loss: 0.5104 - val_accuracy: 0.8371\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2973 - accuracy: 0.9256 - val_loss: 0.4898 - val_accuracy: 0.8562\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3011 - accuracy: 0.9206 - val_loss: 0.4947 - val_accuracy: 0.8307\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2992 - accuracy: 0.9211 - val_loss: 0.4888 - val_accuracy: 0.8435\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2899 - accuracy: 0.9108 - val_loss: 0.4900 - val_accuracy: 0.8435\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2858 - accuracy: 0.9258 - val_loss: 0.4876 - val_accuracy: 0.8498\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2717 - accuracy: 0.9250 - val_loss: 0.4733 - val_accuracy: 0.8530\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2796 - accuracy: 0.9283 - val_loss: 0.4616 - val_accuracy: 0.8594\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2681 - accuracy: 0.9305 - val_loss: 0.4873 - val_accuracy: 0.8562\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2731 - accuracy: 0.9243 - val_loss: 0.4636 - val_accuracy: 0.8530\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2713 - accuracy: 0.9327 - val_loss: 0.4700 - val_accuracy: 0.8498\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2477 - accuracy: 0.9402 - val_loss: 0.4748 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2477 - accuracy: 0.9354 - val_loss: 0.4670 - val_accuracy: 0.8562\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2334 - accuracy: 0.9428 - val_loss: 0.4676 - val_accuracy: 0.8466\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2781 - accuracy: 0.9292 - val_loss: 0.4570 - val_accuracy: 0.8562\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2453 - accuracy: 0.9337 - val_loss: 0.4547 - val_accuracy: 0.8626\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2539 - accuracy: 0.9336 - val_loss: 0.4651 - val_accuracy: 0.8658\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2593 - accuracy: 0.9353 - val_loss: 0.4560 - val_accuracy: 0.8594\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2570 - accuracy: 0.9352 - val_loss: 0.4533 - val_accuracy: 0.8562\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2415 - accuracy: 0.9386 - val_loss: 0.4520 - val_accuracy: 0.8658\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2402 - accuracy: 0.9394 - val_loss: 0.4577 - val_accuracy: 0.8530\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2352 - accuracy: 0.9421 - val_loss: 0.4534 - val_accuracy: 0.8594\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2499 - accuracy: 0.9358 - val_loss: 0.4576 - val_accuracy: 0.8466\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2252 - accuracy: 0.9357 - val_loss: 0.4620 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2213 - accuracy: 0.9466 - val_loss: 0.4471 - val_accuracy: 0.8626\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2400 - accuracy: 0.9386 - val_loss: 0.4486 - val_accuracy: 0.8594\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2285 - accuracy: 0.9388 - val_loss: 0.4515 - val_accuracy: 0.8658\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2235 - accuracy: 0.9379 - val_loss: 0.4599 - val_accuracy: 0.8562\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2249 - accuracy: 0.9486 - val_loss: 0.4561 - val_accuracy: 0.8594\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2265 - accuracy: 0.9397 - val_loss: 0.4542 - val_accuracy: 0.8562\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2171 - accuracy: 0.9501 - val_loss: 0.4543 - val_accuracy: 0.8594\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2191 - accuracy: 0.9505 - val_loss: 0.4509 - val_accuracy: 0.8626\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2356 - accuracy: 0.9348 - val_loss: 0.4472 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4471 - accuracy: 0.8626\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4471 - accuracy: 0.8626\n",
      "--------------------Fold_6--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 184ms/step - loss: 3.4189 - accuracy: 0.3562 - val_loss: 3.3166 - val_accuracy: 0.2949\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.9764 - accuracy: 0.5473 - val_loss: 1.9983 - val_accuracy: 0.5417\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.7944 - accuracy: 0.5669 - val_loss: 2.0605 - val_accuracy: 0.5385\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.5198 - accuracy: 0.6231 - val_loss: 2.0388 - val_accuracy: 0.5481\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.4263 - accuracy: 0.6382 - val_loss: 1.7125 - val_accuracy: 0.5833\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.2330 - accuracy: 0.6804 - val_loss: 1.6308 - val_accuracy: 0.5929\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.1801 - accuracy: 0.6826 - val_loss: 1.4636 - val_accuracy: 0.6186\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.0298 - accuracy: 0.7256 - val_loss: 1.2574 - val_accuracy: 0.6699\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.9718 - accuracy: 0.7392 - val_loss: 1.1678 - val_accuracy: 0.6987\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.9239 - accuracy: 0.7509 - val_loss: 1.1308 - val_accuracy: 0.7115\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.8644 - accuracy: 0.7627 - val_loss: 1.0906 - val_accuracy: 0.7308\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.7988 - accuracy: 0.7784 - val_loss: 0.9962 - val_accuracy: 0.7244\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.7607 - accuracy: 0.7848 - val_loss: 0.9616 - val_accuracy: 0.7372\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6818 - accuracy: 0.8068 - val_loss: 0.9201 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6851 - accuracy: 0.7813 - val_loss: 0.8458 - val_accuracy: 0.7853\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.6436 - accuracy: 0.8106 - val_loss: 0.8556 - val_accuracy: 0.7756\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5787 - accuracy: 0.8317 - val_loss: 0.8241 - val_accuracy: 0.7628\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.5265 - accuracy: 0.8527 - val_loss: 0.7859 - val_accuracy: 0.7853\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5466 - accuracy: 0.8440 - val_loss: 0.8072 - val_accuracy: 0.7660\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.5264 - accuracy: 0.8389 - val_loss: 0.7157 - val_accuracy: 0.8077\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.5301 - accuracy: 0.8423 - val_loss: 0.8158 - val_accuracy: 0.7692\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.5019 - accuracy: 0.8451 - val_loss: 0.6916 - val_accuracy: 0.8237\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.4580 - accuracy: 0.8587 - val_loss: 0.7483 - val_accuracy: 0.7853\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4441 - accuracy: 0.8612 - val_loss: 0.6744 - val_accuracy: 0.8269\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4171 - accuracy: 0.8763 - val_loss: 0.7086 - val_accuracy: 0.8045\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4237 - accuracy: 0.8671 - val_loss: 0.6816 - val_accuracy: 0.8141\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4196 - accuracy: 0.8794 - val_loss: 0.7092 - val_accuracy: 0.8045\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4300 - accuracy: 0.8671 - val_loss: 0.6759 - val_accuracy: 0.8173\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3999 - accuracy: 0.8836 - val_loss: 0.6336 - val_accuracy: 0.8301\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3460 - accuracy: 0.9085 - val_loss: 0.6318 - val_accuracy: 0.8269\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3504 - accuracy: 0.9026 - val_loss: 0.6294 - val_accuracy: 0.8333\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3414 - accuracy: 0.9031 - val_loss: 0.6806 - val_accuracy: 0.8077\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3350 - accuracy: 0.8995 - val_loss: 0.6112 - val_accuracy: 0.8365\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3135 - accuracy: 0.9111 - val_loss: 0.6319 - val_accuracy: 0.8237\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3016 - accuracy: 0.9188 - val_loss: 0.6328 - val_accuracy: 0.8237\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3172 - accuracy: 0.9150 - val_loss: 0.6856 - val_accuracy: 0.8141\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3330 - accuracy: 0.9093 - val_loss: 0.6125 - val_accuracy: 0.8397\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3031 - accuracy: 0.9178 - val_loss: 0.6112 - val_accuracy: 0.8333\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.2892 - accuracy: 0.9186 - val_loss: 0.6185 - val_accuracy: 0.8365\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2721 - accuracy: 0.9292 - val_loss: 0.6117 - val_accuracy: 0.8429\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2786 - accuracy: 0.9253 - val_loss: 0.6412 - val_accuracy: 0.8365\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.6112 - accuracy: 0.8365\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.6112 - accuracy: 0.8365\n",
      "--------------------Fold_7--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 184ms/step - loss: 3.4262 - accuracy: 0.3292 - val_loss: 3.4112 - val_accuracy: 0.2212\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.9775 - accuracy: 0.5514 - val_loss: 1.8765 - val_accuracy: 0.5385\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.7195 - accuracy: 0.5812 - val_loss: 1.9843 - val_accuracy: 0.5449\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.5073 - accuracy: 0.6315 - val_loss: 1.7828 - val_accuracy: 0.5513\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.3913 - accuracy: 0.6516 - val_loss: 1.6510 - val_accuracy: 0.5737\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.2870 - accuracy: 0.6613 - val_loss: 1.3778 - val_accuracy: 0.6314\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.1337 - accuracy: 0.6999 - val_loss: 1.3402 - val_accuracy: 0.6378\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.0869 - accuracy: 0.7074 - val_loss: 1.2187 - val_accuracy: 0.6635\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.0000 - accuracy: 0.7427 - val_loss: 1.0770 - val_accuracy: 0.6987\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.9167 - accuracy: 0.7501 - val_loss: 1.0119 - val_accuracy: 0.7340\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.8363 - accuracy: 0.7754 - val_loss: 0.9565 - val_accuracy: 0.7147\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7700 - accuracy: 0.7837 - val_loss: 0.9048 - val_accuracy: 0.7692\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.7410 - accuracy: 0.7914 - val_loss: 0.8613 - val_accuracy: 0.7340\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7019 - accuracy: 0.8061 - val_loss: 0.8285 - val_accuracy: 0.7468\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6931 - accuracy: 0.8077 - val_loss: 0.7562 - val_accuracy: 0.7821\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6254 - accuracy: 0.8129 - val_loss: 0.7584 - val_accuracy: 0.7917\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5935 - accuracy: 0.8303 - val_loss: 0.7200 - val_accuracy: 0.8173\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6075 - accuracy: 0.8197 - val_loss: 0.7311 - val_accuracy: 0.8045\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5245 - accuracy: 0.8542 - val_loss: 0.7543 - val_accuracy: 0.7788\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5277 - accuracy: 0.8502 - val_loss: 0.6576 - val_accuracy: 0.8173\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.5120 - accuracy: 0.8572 - val_loss: 0.6416 - val_accuracy: 0.8109\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4948 - accuracy: 0.8537 - val_loss: 0.7707 - val_accuracy: 0.7788\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4808 - accuracy: 0.8615 - val_loss: 0.6281 - val_accuracy: 0.8077\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.4742 - accuracy: 0.8575 - val_loss: 0.6607 - val_accuracy: 0.7917\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4749 - accuracy: 0.8687 - val_loss: 0.6100 - val_accuracy: 0.8205\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.3953 - accuracy: 0.8872 - val_loss: 0.5698 - val_accuracy: 0.8462\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4083 - accuracy: 0.8765 - val_loss: 0.5728 - val_accuracy: 0.8462\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3918 - accuracy: 0.8911 - val_loss: 0.5473 - val_accuracy: 0.7981\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3982 - accuracy: 0.8895 - val_loss: 0.5657 - val_accuracy: 0.8462\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3758 - accuracy: 0.8927 - val_loss: 0.5812 - val_accuracy: 0.8205\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3630 - accuracy: 0.8957 - val_loss: 0.5508 - val_accuracy: 0.8365\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3525 - accuracy: 0.8940 - val_loss: 0.5964 - val_accuracy: 0.8269\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.3128 - accuracy: 0.9161 - val_loss: 0.5147 - val_accuracy: 0.8429\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.3194 - accuracy: 0.9098 - val_loss: 0.5536 - val_accuracy: 0.8397\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3289 - accuracy: 0.9068 - val_loss: 0.5416 - val_accuracy: 0.8462\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2872 - accuracy: 0.9259 - val_loss: 0.5391 - val_accuracy: 0.8301\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3088 - accuracy: 0.9147 - val_loss: 0.4995 - val_accuracy: 0.8429\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 186ms/step - loss: 0.3201 - accuracy: 0.9092 - val_loss: 0.5396 - val_accuracy: 0.8237\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2767 - accuracy: 0.9239 - val_loss: 0.5355 - val_accuracy: 0.8333\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2785 - accuracy: 0.9284 - val_loss: 0.5197 - val_accuracy: 0.8494\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2925 - accuracy: 0.9130 - val_loss: 0.5237 - val_accuracy: 0.8365\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2505 - accuracy: 0.9395 - val_loss: 0.4959 - val_accuracy: 0.8429\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2538 - accuracy: 0.9378 - val_loss: 0.4973 - val_accuracy: 0.8365\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2596 - accuracy: 0.9331 - val_loss: 0.4880 - val_accuracy: 0.8429\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2475 - accuracy: 0.9342 - val_loss: 0.4879 - val_accuracy: 0.8494\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2612 - accuracy: 0.9288 - val_loss: 0.4812 - val_accuracy: 0.8526\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2393 - accuracy: 0.9385 - val_loss: 0.4787 - val_accuracy: 0.8462\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2448 - accuracy: 0.9360 - val_loss: 0.4892 - val_accuracy: 0.8429\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2370 - accuracy: 0.9350 - val_loss: 0.5135 - val_accuracy: 0.8462\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2252 - accuracy: 0.9432 - val_loss: 0.4839 - val_accuracy: 0.8526\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2253 - accuracy: 0.9413 - val_loss: 0.4822 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2311 - accuracy: 0.9449 - val_loss: 0.4811 - val_accuracy: 0.8558\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2349 - accuracy: 0.9405 - val_loss: 0.4690 - val_accuracy: 0.8462\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2339 - accuracy: 0.9376 - val_loss: 0.4783 - val_accuracy: 0.8526\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2261 - accuracy: 0.9417 - val_loss: 0.4750 - val_accuracy: 0.8526\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2195 - accuracy: 0.9426 - val_loss: 0.4674 - val_accuracy: 0.8558\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2395 - accuracy: 0.9381 - val_loss: 0.4670 - val_accuracy: 0.8526\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2313 - accuracy: 0.9382 - val_loss: 0.4553 - val_accuracy: 0.8558\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2323 - accuracy: 0.9427 - val_loss: 0.4645 - val_accuracy: 0.8558\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2289 - accuracy: 0.9459 - val_loss: 0.4574 - val_accuracy: 0.8558\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2067 - accuracy: 0.9497 - val_loss: 0.4662 - val_accuracy: 0.8462\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2172 - accuracy: 0.9468 - val_loss: 0.4708 - val_accuracy: 0.8526\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2126 - accuracy: 0.9443 - val_loss: 0.4670 - val_accuracy: 0.8526\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2220 - accuracy: 0.9397 - val_loss: 0.4585 - val_accuracy: 0.8558\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.1986 - accuracy: 0.9542 - val_loss: 0.4633 - val_accuracy: 0.8526\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2105 - accuracy: 0.9483 - val_loss: 0.4608 - val_accuracy: 0.8526\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4553 - accuracy: 0.8558\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4553 - accuracy: 0.8558\n",
      "--------------------Fold_8--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 186ms/step - loss: 3.3943 - accuracy: 0.3472 - val_loss: 3.2599 - val_accuracy: 0.3173\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 2.0112 - accuracy: 0.5408 - val_loss: 1.8584 - val_accuracy: 0.5385\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.7211 - accuracy: 0.5914 - val_loss: 1.8750 - val_accuracy: 0.5641\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.5069 - accuracy: 0.6342 - val_loss: 1.8492 - val_accuracy: 0.5705\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 1.4494 - accuracy: 0.6297 - val_loss: 1.5701 - val_accuracy: 0.6058\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.2839 - accuracy: 0.6591 - val_loss: 1.4310 - val_accuracy: 0.6378\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 1.1322 - accuracy: 0.7070 - val_loss: 1.2462 - val_accuracy: 0.6410\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.0399 - accuracy: 0.7190 - val_loss: 1.2043 - val_accuracy: 0.6635\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.9885 - accuracy: 0.7357 - val_loss: 1.0543 - val_accuracy: 0.7019\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.8679 - accuracy: 0.7674 - val_loss: 1.0066 - val_accuracy: 0.7147\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.8301 - accuracy: 0.7681 - val_loss: 0.9485 - val_accuracy: 0.7179\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.7946 - accuracy: 0.7870 - val_loss: 0.9041 - val_accuracy: 0.7276\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7052 - accuracy: 0.8114 - val_loss: 0.8144 - val_accuracy: 0.7340\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7033 - accuracy: 0.8061 - val_loss: 0.9005 - val_accuracy: 0.7724\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6795 - accuracy: 0.8035 - val_loss: 0.7892 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.6201 - accuracy: 0.8251 - val_loss: 0.8321 - val_accuracy: 0.7756\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.6084 - accuracy: 0.8241 - val_loss: 0.7088 - val_accuracy: 0.7756\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5697 - accuracy: 0.8356 - val_loss: 0.6970 - val_accuracy: 0.8109\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5500 - accuracy: 0.8325 - val_loss: 0.6611 - val_accuracy: 0.7821\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5438 - accuracy: 0.8333 - val_loss: 0.6099 - val_accuracy: 0.8205\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4886 - accuracy: 0.8582 - val_loss: 0.6175 - val_accuracy: 0.7853\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4849 - accuracy: 0.8632 - val_loss: 0.6175 - val_accuracy: 0.7724\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4607 - accuracy: 0.8641 - val_loss: 0.6343 - val_accuracy: 0.7692\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4709 - accuracy: 0.8681 - val_loss: 0.5986 - val_accuracy: 0.8141\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4200 - accuracy: 0.8791 - val_loss: 0.6160 - val_accuracy: 0.7821\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4259 - accuracy: 0.8794 - val_loss: 0.5795 - val_accuracy: 0.8141\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.4105 - accuracy: 0.8842 - val_loss: 0.5776 - val_accuracy: 0.8141\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3961 - accuracy: 0.8860 - val_loss: 0.5648 - val_accuracy: 0.8141\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3605 - accuracy: 0.8946 - val_loss: 0.6027 - val_accuracy: 0.7917\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3808 - accuracy: 0.8911 - val_loss: 0.5889 - val_accuracy: 0.8173\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3552 - accuracy: 0.8938 - val_loss: 0.6089 - val_accuracy: 0.8109\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3884 - accuracy: 0.8858 - val_loss: 0.5764 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3218 - accuracy: 0.9101 - val_loss: 0.5410 - val_accuracy: 0.8397\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.3190 - accuracy: 0.9071 - val_loss: 0.5031 - val_accuracy: 0.8365\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2981 - accuracy: 0.9222 - val_loss: 0.5216 - val_accuracy: 0.8237\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2877 - accuracy: 0.9295 - val_loss: 0.5116 - val_accuracy: 0.8301\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3139 - accuracy: 0.9097 - val_loss: 0.5179 - val_accuracy: 0.8333\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2916 - accuracy: 0.9188 - val_loss: 0.5442 - val_accuracy: 0.7885\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2848 - accuracy: 0.9253 - val_loss: 0.4576 - val_accuracy: 0.8462\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2748 - accuracy: 0.9311 - val_loss: 0.4609 - val_accuracy: 0.8622\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2582 - accuracy: 0.9377 - val_loss: 0.4569 - val_accuracy: 0.8622\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2552 - accuracy: 0.9343 - val_loss: 0.4536 - val_accuracy: 0.8558\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2667 - accuracy: 0.9298 - val_loss: 0.4430 - val_accuracy: 0.8654\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2567 - accuracy: 0.9390 - val_loss: 0.4443 - val_accuracy: 0.8686\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2722 - accuracy: 0.9261 - val_loss: 0.4680 - val_accuracy: 0.8365\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2528 - accuracy: 0.9339 - val_loss: 0.4655 - val_accuracy: 0.8494\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2478 - accuracy: 0.9357 - val_loss: 0.4621 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.2489 - accuracy: 0.9380 - val_loss: 0.4472 - val_accuracy: 0.8622\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2338 - accuracy: 0.9420 - val_loss: 0.4437 - val_accuracy: 0.8590\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2332 - accuracy: 0.9439 - val_loss: 0.4420 - val_accuracy: 0.8590\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2271 - accuracy: 0.9403 - val_loss: 0.4526 - val_accuracy: 0.8526\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2351 - accuracy: 0.9389 - val_loss: 0.4422 - val_accuracy: 0.8622\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2529 - accuracy: 0.9370 - val_loss: 0.4367 - val_accuracy: 0.8590\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2298 - accuracy: 0.9455 - val_loss: 0.4372 - val_accuracy: 0.8686\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2424 - accuracy: 0.9341 - val_loss: 0.4410 - val_accuracy: 0.8590\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2616 - accuracy: 0.9294 - val_loss: 0.4409 - val_accuracy: 0.8654\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 187ms/step - loss: 0.2505 - accuracy: 0.9361 - val_loss: 0.4443 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2275 - accuracy: 0.9437 - val_loss: 0.4351 - val_accuracy: 0.8590\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2314 - accuracy: 0.9433 - val_loss: 0.4328 - val_accuracy: 0.8622\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2202 - accuracy: 0.9417 - val_loss: 0.4337 - val_accuracy: 0.8654\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2107 - accuracy: 0.9510 - val_loss: 0.4313 - val_accuracy: 0.8686\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2332 - accuracy: 0.9404 - val_loss: 0.4305 - val_accuracy: 0.8654\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2392 - accuracy: 0.9386 - val_loss: 0.4321 - val_accuracy: 0.8654\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2281 - accuracy: 0.9371 - val_loss: 0.4342 - val_accuracy: 0.8622\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2237 - accuracy: 0.9469 - val_loss: 0.4351 - val_accuracy: 0.8718\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2331 - accuracy: 0.9410 - val_loss: 0.4341 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2341 - accuracy: 0.9390 - val_loss: 0.4307 - val_accuracy: 0.8686\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2294 - accuracy: 0.9373 - val_loss: 0.4312 - val_accuracy: 0.8654\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2112 - accuracy: 0.9465 - val_loss: 0.4315 - val_accuracy: 0.8686\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2300 - accuracy: 0.9372 - val_loss: 0.4303 - val_accuracy: 0.8622\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.2243 - accuracy: 0.9438 - val_loss: 0.4296 - val_accuracy: 0.8622\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2239 - accuracy: 0.9414 - val_loss: 0.4273 - val_accuracy: 0.8686\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2191 - accuracy: 0.9496 - val_loss: 0.4265 - val_accuracy: 0.8686\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2216 - accuracy: 0.9453 - val_loss: 0.4267 - val_accuracy: 0.8654\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2275 - accuracy: 0.9426 - val_loss: 0.4265 - val_accuracy: 0.8654\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2251 - accuracy: 0.9385 - val_loss: 0.4258 - val_accuracy: 0.8654\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2181 - accuracy: 0.9390 - val_loss: 0.4288 - val_accuracy: 0.8718\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2086 - accuracy: 0.9552 - val_loss: 0.4277 - val_accuracy: 0.8718\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2182 - accuracy: 0.9463 - val_loss: 0.4275 - val_accuracy: 0.8750\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2229 - accuracy: 0.9448 - val_loss: 0.4274 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2254 - accuracy: 0.9460 - val_loss: 0.4263 - val_accuracy: 0.8654\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2038 - accuracy: 0.9525 - val_loss: 0.4267 - val_accuracy: 0.8654\n",
      "Epoch 83/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2245 - accuracy: 0.9492 - val_loss: 0.4257 - val_accuracy: 0.8654\n",
      "Epoch 84/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2237 - accuracy: 0.9395 - val_loss: 0.4252 - val_accuracy: 0.8654\n",
      "Epoch 85/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2255 - accuracy: 0.9416 - val_loss: 0.4258 - val_accuracy: 0.8686\n",
      "Epoch 86/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2193 - accuracy: 0.9455 - val_loss: 0.4253 - val_accuracy: 0.8654\n",
      "Epoch 87/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2099 - accuracy: 0.9437 - val_loss: 0.4252 - val_accuracy: 0.8686\n",
      "Epoch 88/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2128 - accuracy: 0.9445 - val_loss: 0.4249 - val_accuracy: 0.8654\n",
      "Epoch 89/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2186 - accuracy: 0.9460 - val_loss: 0.4249 - val_accuracy: 0.8686\n",
      "Epoch 90/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2171 - accuracy: 0.9462 - val_loss: 0.4251 - val_accuracy: 0.8654\n",
      "Epoch 91/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2070 - accuracy: 0.9479 - val_loss: 0.4255 - val_accuracy: 0.8686\n",
      "Epoch 92/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2135 - accuracy: 0.9421 - val_loss: 0.4253 - val_accuracy: 0.8686\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 93/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2158 - accuracy: 0.9466 - val_loss: 0.4253 - val_accuracy: 0.8654\n",
      "Epoch 94/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2068 - accuracy: 0.9497 - val_loss: 0.4258 - val_accuracy: 0.8654\n",
      "Epoch 95/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2133 - accuracy: 0.9448 - val_loss: 0.4252 - val_accuracy: 0.8654\n",
      "Epoch 96/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2051 - accuracy: 0.9466 - val_loss: 0.4256 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4249 - accuracy: 0.8654\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4249 - accuracy: 0.8654\n",
      "--------------------Fold_9--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 183ms/step - loss: 3.3783 - accuracy: 0.3489 - val_loss: 2.9276 - val_accuracy: 0.4551\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.9826 - accuracy: 0.5604 - val_loss: 1.9077 - val_accuracy: 0.5449\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.7716 - accuracy: 0.5709 - val_loss: 1.8158 - val_accuracy: 0.5609\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.5370 - accuracy: 0.6251 - val_loss: 1.6927 - val_accuracy: 0.5609\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 1.3952 - accuracy: 0.6416 - val_loss: 1.5730 - val_accuracy: 0.5833\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.2643 - accuracy: 0.6657 - val_loss: 1.3282 - val_accuracy: 0.6218\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.1647 - accuracy: 0.6902 - val_loss: 1.2566 - val_accuracy: 0.6538\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.0608 - accuracy: 0.7219 - val_loss: 1.1629 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.0091 - accuracy: 0.7301 - val_loss: 1.0633 - val_accuracy: 0.7179\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.9020 - accuracy: 0.7562 - val_loss: 1.0346 - val_accuracy: 0.7115\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.8648 - accuracy: 0.7585 - val_loss: 1.0035 - val_accuracy: 0.6987\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7519 - accuracy: 0.7962 - val_loss: 0.8986 - val_accuracy: 0.7468\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7723 - accuracy: 0.7746 - val_loss: 0.8348 - val_accuracy: 0.7468\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.6736 - accuracy: 0.8136 - val_loss: 0.8409 - val_accuracy: 0.7692\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.6625 - accuracy: 0.8096 - val_loss: 0.7517 - val_accuracy: 0.7917\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6032 - accuracy: 0.8225 - val_loss: 0.7323 - val_accuracy: 0.7821\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.6164 - accuracy: 0.8127 - val_loss: 0.7655 - val_accuracy: 0.7308\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.5579 - accuracy: 0.8352 - val_loss: 0.6725 - val_accuracy: 0.7885\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5341 - accuracy: 0.8449 - val_loss: 0.6899 - val_accuracy: 0.7917\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.5310 - accuracy: 0.8426 - val_loss: 0.6850 - val_accuracy: 0.7756\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.4796 - accuracy: 0.8560 - val_loss: 0.6845 - val_accuracy: 0.8141\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.4967 - accuracy: 0.8466 - val_loss: 0.6169 - val_accuracy: 0.8109\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4601 - accuracy: 0.8738 - val_loss: 0.5979 - val_accuracy: 0.8141\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4355 - accuracy: 0.8695 - val_loss: 0.5814 - val_accuracy: 0.8141\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.4511 - accuracy: 0.8638 - val_loss: 0.5766 - val_accuracy: 0.8205\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3942 - accuracy: 0.8931 - val_loss: 0.5817 - val_accuracy: 0.8173\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4032 - accuracy: 0.8814 - val_loss: 0.5770 - val_accuracy: 0.8173\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.4382 - accuracy: 0.8677 - val_loss: 0.7069 - val_accuracy: 0.7660\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 179ms/step - loss: 0.3894 - accuracy: 0.8941 - val_loss: 0.5766 - val_accuracy: 0.8013\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3686 - accuracy: 0.8951 - val_loss: 0.5157 - val_accuracy: 0.8269\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3564 - accuracy: 0.8994 - val_loss: 0.5174 - val_accuracy: 0.8365\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3170 - accuracy: 0.9155 - val_loss: 0.5086 - val_accuracy: 0.8365\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3380 - accuracy: 0.9067 - val_loss: 0.5103 - val_accuracy: 0.8141\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.3070 - accuracy: 0.9140 - val_loss: 0.4884 - val_accuracy: 0.8333\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3238 - accuracy: 0.9026 - val_loss: 0.4924 - val_accuracy: 0.8397\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2925 - accuracy: 0.9193 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3219 - accuracy: 0.9062 - val_loss: 0.4964 - val_accuracy: 0.8237\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3114 - accuracy: 0.9100 - val_loss: 0.4725 - val_accuracy: 0.8269\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2949 - accuracy: 0.9169 - val_loss: 0.4758 - val_accuracy: 0.8397\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3110 - accuracy: 0.9167 - val_loss: 0.5190 - val_accuracy: 0.8333\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2735 - accuracy: 0.9259 - val_loss: 0.4621 - val_accuracy: 0.8365\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3084 - accuracy: 0.9129 - val_loss: 0.4818 - val_accuracy: 0.8301\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3023 - accuracy: 0.9086 - val_loss: 0.4979 - val_accuracy: 0.8237\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2596 - accuracy: 0.9329 - val_loss: 0.5195 - val_accuracy: 0.8205\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2735 - accuracy: 0.9223 - val_loss: 0.4901 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2674 - accuracy: 0.9263 - val_loss: 0.4668 - val_accuracy: 0.8365\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 186ms/step - loss: 0.2441 - accuracy: 0.9326 - val_loss: 0.4638 - val_accuracy: 0.8333\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2596 - accuracy: 0.9320 - val_loss: 0.4638 - val_accuracy: 0.8429\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2550 - accuracy: 0.9318 - val_loss: 0.4599 - val_accuracy: 0.8397\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2341 - accuracy: 0.9370 - val_loss: 0.4552 - val_accuracy: 0.8462\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2228 - accuracy: 0.9429 - val_loss: 0.4725 - val_accuracy: 0.8429\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2580 - accuracy: 0.9299 - val_loss: 0.4369 - val_accuracy: 0.8526\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2516 - accuracy: 0.9324 - val_loss: 0.4869 - val_accuracy: 0.8462\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2343 - accuracy: 0.9314 - val_loss: 0.4540 - val_accuracy: 0.8365\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2158 - accuracy: 0.9470 - val_loss: 0.4458 - val_accuracy: 0.8365\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2193 - accuracy: 0.9392 - val_loss: 0.4775 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2138 - accuracy: 0.9422 - val_loss: 0.4411 - val_accuracy: 0.8365\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2219 - accuracy: 0.9468 - val_loss: 0.4440 - val_accuracy: 0.8494\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2081 - accuracy: 0.9475 - val_loss: 0.4471 - val_accuracy: 0.8494\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 0.2171 - accuracy: 0.9430 - val_loss: 0.4472 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4369 - accuracy: 0.8526\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4369 - accuracy: 0.8526\n",
      "--------------------Fold_10--------------------\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 183ms/step - loss: 3.4090 - accuracy: 0.3351 - val_loss: 2.9942 - val_accuracy: 0.3814\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.9950 - accuracy: 0.5441 - val_loss: 1.8966 - val_accuracy: 0.5449\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.6929 - accuracy: 0.5890 - val_loss: 1.8593 - val_accuracy: 0.5417\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 1.5276 - accuracy: 0.6258 - val_loss: 1.6897 - val_accuracy: 0.5769\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 1.3816 - accuracy: 0.6517 - val_loss: 1.5503 - val_accuracy: 0.6026\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.3005 - accuracy: 0.6587 - val_loss: 1.3712 - val_accuracy: 0.6410\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 1.1707 - accuracy: 0.6903 - val_loss: 1.2846 - val_accuracy: 0.6538\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 1.0711 - accuracy: 0.7139 - val_loss: 1.1105 - val_accuracy: 0.7115\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 8s 184ms/step - loss: 1.0148 - accuracy: 0.7113 - val_loss: 1.0171 - val_accuracy: 0.7115\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.8945 - accuracy: 0.7654 - val_loss: 0.9971 - val_accuracy: 0.7404\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.8492 - accuracy: 0.7610 - val_loss: 0.8766 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7745 - accuracy: 0.7834 - val_loss: 0.8687 - val_accuracy: 0.7596\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.7804 - accuracy: 0.7886 - val_loss: 0.8666 - val_accuracy: 0.7692\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.7094 - accuracy: 0.8049 - val_loss: 0.8277 - val_accuracy: 0.7724\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6759 - accuracy: 0.8145 - val_loss: 0.7555 - val_accuracy: 0.8141\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.6615 - accuracy: 0.8233 - val_loss: 0.7008 - val_accuracy: 0.8077\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.6073 - accuracy: 0.8199 - val_loss: 0.6731 - val_accuracy: 0.8109\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5522 - accuracy: 0.8348 - val_loss: 0.6461 - val_accuracy: 0.8269\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 0.5383 - accuracy: 0.8463 - val_loss: 0.6329 - val_accuracy: 0.8205\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5298 - accuracy: 0.8544 - val_loss: 0.6212 - val_accuracy: 0.8365\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5350 - accuracy: 0.8486 - val_loss: 0.7384 - val_accuracy: 0.7885\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5122 - accuracy: 0.8514 - val_loss: 0.5705 - val_accuracy: 0.8397\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.5070 - accuracy: 0.8528 - val_loss: 0.6085 - val_accuracy: 0.8141\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.4421 - accuracy: 0.8736 - val_loss: 0.7051 - val_accuracy: 0.7949\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4600 - accuracy: 0.8655 - val_loss: 0.5246 - val_accuracy: 0.8109\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4115 - accuracy: 0.8775 - val_loss: 0.6344 - val_accuracy: 0.8237\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.4435 - accuracy: 0.8647 - val_loss: 0.5483 - val_accuracy: 0.8301\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.4168 - accuracy: 0.8750 - val_loss: 0.5620 - val_accuracy: 0.8301\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3954 - accuracy: 0.8829 - val_loss: 0.5526 - val_accuracy: 0.8269\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3629 - accuracy: 0.8944 - val_loss: 0.5091 - val_accuracy: 0.8269\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3395 - accuracy: 0.9105 - val_loss: 0.4939 - val_accuracy: 0.8429\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3169 - accuracy: 0.9181 - val_loss: 0.5328 - val_accuracy: 0.8173\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3267 - accuracy: 0.9030 - val_loss: 0.4844 - val_accuracy: 0.8397\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3425 - accuracy: 0.9026 - val_loss: 0.5376 - val_accuracy: 0.8173\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.3120 - accuracy: 0.9223 - val_loss: 0.5188 - val_accuracy: 0.8397\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.3025 - accuracy: 0.9229 - val_loss: 0.4938 - val_accuracy: 0.8397\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3213 - accuracy: 0.9137 - val_loss: 0.5077 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3000 - accuracy: 0.9233 - val_loss: 0.4701 - val_accuracy: 0.8526\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2890 - accuracy: 0.9261 - val_loss: 0.4789 - val_accuracy: 0.8397\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.3104 - accuracy: 0.9193 - val_loss: 0.4497 - val_accuracy: 0.8494\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2679 - accuracy: 0.9350 - val_loss: 0.4620 - val_accuracy: 0.8429\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2819 - accuracy: 0.9242 - val_loss: 0.4759 - val_accuracy: 0.8462\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2805 - accuracy: 0.9279 - val_loss: 0.4662 - val_accuracy: 0.8494\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2949 - accuracy: 0.9222 - val_loss: 0.4649 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2654 - accuracy: 0.9329 - val_loss: 0.4510 - val_accuracy: 0.8622\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2643 - accuracy: 0.9278 - val_loss: 0.4519 - val_accuracy: 0.8462\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2649 - accuracy: 0.9356 - val_loss: 0.4391 - val_accuracy: 0.8494\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2608 - accuracy: 0.9330 - val_loss: 0.4540 - val_accuracy: 0.8558\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2513 - accuracy: 0.9336 - val_loss: 0.4467 - val_accuracy: 0.8494\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2458 - accuracy: 0.9420 - val_loss: 0.4498 - val_accuracy: 0.8590\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 8s 183ms/step - loss: 0.2591 - accuracy: 0.9363 - val_loss: 0.4657 - val_accuracy: 0.8558\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 8s 180ms/step - loss: 0.2616 - accuracy: 0.9389 - val_loss: 0.4503 - val_accuracy: 0.8558\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2436 - accuracy: 0.9355 - val_loss: 0.4509 - val_accuracy: 0.8526\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 8s 181ms/step - loss: 0.2527 - accuracy: 0.9341 - val_loss: 0.4429 - val_accuracy: 0.8558\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.2531 - accuracy: 0.9397 - val_loss: 0.4490 - val_accuracy: 0.8494\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4391 - accuracy: 0.8494\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4391 - accuracy: 0.8494\n",
      "\n",
      "K-fold cross validation Auc: ['0.8562', '0.8466', '0.8722', '0.8307', '0.8626', '0.8365', '0.8558', '0.8654', '0.8526', '0.8494']\n",
      "\n",
      "K-fold cross validation loss: ['0.5350', '0.4758', '0.4192', '0.6061', '0.4471', '0.6112', '0.4553', '0.4249', '0.4369', '0.4391']\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, random_state = 2021, shuffle = True)\n",
    "reLR = ReduceLROnPlateau(patience = 4,verbose = 1,factor = 0.5) \n",
    "es =EarlyStopping(monitor='val_loss', patience=8, mode='min')\n",
    "\n",
    "accuracy = []\n",
    "losss=[]\n",
    "models=[]\n",
    "\n",
    "for i, (train, validation) in enumerate(skf.split(X, y.argmax(1))) :\n",
    "    mc = ModelCheckpoint(f'./model_kf/cv_study{i + 1}.h5',save_best_only=True, verbose=0, monitor = 'val_loss', mode = 'min', save_weights_only=True)\n",
    "    print(\"-\" * 20 +\"Fold_\"+str(i+1)+ \"-\" * 20)\n",
    "    model = cnn_model((600,18),61)\n",
    "    history = model.fit(X[train], y[train], epochs = 100, validation_data= (X[validation], y[validation]), \n",
    "                        verbose=1,batch_size=64,callbacks=[es,mc,reLR])\n",
    "    model.load_weights(f'./model_kf/cv_study{i + 1}.h5')\n",
    "    \n",
    "    k_accuracy = '%.4f' % (model.evaluate(X[validation], y[validation])[1])\n",
    "    k_loss = '%.4f' % (model.evaluate(X[validation], y[validation])[0])\n",
    "    \n",
    "    accuracy.append(k_accuracy)\n",
    "    losss.append(k_loss)\n",
    "    models.append(model)\n",
    "\n",
    "print('\\nK-fold cross validation Auc: {}'.format(accuracy))\n",
    "print('\\nK-fold cross validation loss: {}'.format(losss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 성능 확인 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8528\n",
      "\n",
      "0.48506\n"
     ]
    }
   ],
   "source": [
    "print(sum([float(i) for i in accuracy])/10)\n",
    "print()\n",
    "print(sum([float(i) for i in losss])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 600, 18)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X=np.array(test_sc.iloc[:,2:]).reshape(782, 600, -1)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.11362115e-05, 2.25996450e-06, 2.11680984e-07, ...,\n",
       "        6.39248919e-03, 1.50766409e-05, 3.40101496e-06],\n",
       "       [4.13231173e-04, 1.99007154e-05, 1.22563812e-04, ...,\n",
       "        8.93750075e-06, 2.06302539e-05, 1.22722922e-05],\n",
       "       [1.86802447e-03, 3.26019563e-02, 1.62000761e-05, ...,\n",
       "        8.92708835e-04, 1.20446784e-02, 2.22884724e-03],\n",
       "       ...,\n",
       "       [4.16979339e-04, 3.25313522e-06, 1.09946477e-05, ...,\n",
       "        1.80470997e-05, 1.36069389e-06, 7.54901499e-04],\n",
       "       [3.85870408e-06, 8.71700991e-04, 9.34987668e-07, ...,\n",
       "        1.06083007e-07, 1.25161705e-05, 4.91666885e-09],\n",
       "       [9.32284092e-05, 4.08958658e-06, 1.09709674e-06, ...,\n",
       "        9.87853055e-05, 9.04675460e-07, 1.53993984e-04]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    pred = model.predict(test_X)\n",
    "    preds.append(pred)\n",
    "pred = np.mean(preds, axis=0)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>...</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3125</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.116810e-07</td>\n",
       "      <td>4.708937e-08</td>\n",
       "      <td>1.865657e-04</td>\n",
       "      <td>1.124369e-07</td>\n",
       "      <td>3.247155e-04</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.424222e-07</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.100756</td>\n",
       "      <td>4.794668e-01</td>\n",
       "      <td>1.128225e-04</td>\n",
       "      <td>3.902865e-01</td>\n",
       "      <td>2.294270e-03</td>\n",
       "      <td>3.994864e-06</td>\n",
       "      <td>2.203607e-06</td>\n",
       "      <td>7.347716e-07</td>\n",
       "      <td>4.205005e-07</td>\n",
       "      <td>9.210566e-08</td>\n",
       "      <td>3.008443e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>7.120603e-07</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>9.638647e-04</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>1.705775e-05</td>\n",
       "      <td>2.177705e-07</td>\n",
       "      <td>6.212062e-09</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>3.137485e-05</td>\n",
       "      <td>8.554223e-04</td>\n",
       "      <td>3.529570e-05</td>\n",
       "      <td>1.155294e-07</td>\n",
       "      <td>3.284435e-06</td>\n",
       "      <td>2.174506e-06</td>\n",
       "      <td>3.665759e-08</td>\n",
       "      <td>4.478506e-04</td>\n",
       "      <td>4.650679e-04</td>\n",
       "      <td>1.211206e-05</td>\n",
       "      <td>2.320302e-06</td>\n",
       "      <td>1.255611e-07</td>\n",
       "      <td>2.459125e-07</td>\n",
       "      <td>8.899527e-10</td>\n",
       "      <td>6.392489e-03</td>\n",
       "      <td>1.507664e-05</td>\n",
       "      <td>3.401015e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3126</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1.225638e-04</td>\n",
       "      <td>1.330648e-03</td>\n",
       "      <td>5.290742e-05</td>\n",
       "      <td>6.115026e-04</td>\n",
       "      <td>2.825133e-06</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>7.107466e-06</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1.578770e-06</td>\n",
       "      <td>1.445539e-08</td>\n",
       "      <td>2.135061e-06</td>\n",
       "      <td>1.027968e-06</td>\n",
       "      <td>2.940293e-04</td>\n",
       "      <td>2.704988e-05</td>\n",
       "      <td>1.144425e-05</td>\n",
       "      <td>2.083481e-05</td>\n",
       "      <td>4.048173e-06</td>\n",
       "      <td>2.643172e-05</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>2.340468e-03</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>4.793614e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.656085e-08</td>\n",
       "      <td>4.648645e-04</td>\n",
       "      <td>3.747970e-04</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.712846e-06</td>\n",
       "      <td>4.472250e-06</td>\n",
       "      <td>8.808629e-06</td>\n",
       "      <td>1.643756e-05</td>\n",
       "      <td>1.192999e-04</td>\n",
       "      <td>2.324675e-03</td>\n",
       "      <td>6.748711e-04</td>\n",
       "      <td>7.841913e-06</td>\n",
       "      <td>1.797475e-08</td>\n",
       "      <td>9.208373e-05</td>\n",
       "      <td>1.928369e-04</td>\n",
       "      <td>1.234607e-04</td>\n",
       "      <td>4.037772e-06</td>\n",
       "      <td>1.639472e-04</td>\n",
       "      <td>8.937501e-06</td>\n",
       "      <td>2.063025e-05</td>\n",
       "      <td>1.227229e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3127</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.032602</td>\n",
       "      <td>1.620008e-05</td>\n",
       "      <td>1.283705e-05</td>\n",
       "      <td>3.405659e-05</td>\n",
       "      <td>6.504179e-04</td>\n",
       "      <td>1.054489e-01</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>1.890009e-05</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>4.135342e-04</td>\n",
       "      <td>3.376489e-05</td>\n",
       "      <td>2.495863e-03</td>\n",
       "      <td>1.967650e-02</td>\n",
       "      <td>8.969253e-04</td>\n",
       "      <td>1.574834e-04</td>\n",
       "      <td>1.690211e-04</td>\n",
       "      <td>4.927005e-05</td>\n",
       "      <td>2.873521e-04</td>\n",
       "      <td>2.336423e-04</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>5.826922e-07</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>4.887589e-02</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>3.705476e-04</td>\n",
       "      <td>2.303389e-04</td>\n",
       "      <td>1.743668e-04</td>\n",
       "      <td>0.063292</td>\n",
       "      <td>0.195926</td>\n",
       "      <td>7.786207e-03</td>\n",
       "      <td>4.333588e-01</td>\n",
       "      <td>2.320479e-04</td>\n",
       "      <td>2.547552e-03</td>\n",
       "      <td>2.427308e-03</td>\n",
       "      <td>4.264420e-03</td>\n",
       "      <td>3.207233e-06</td>\n",
       "      <td>4.141561e-05</td>\n",
       "      <td>2.466323e-05</td>\n",
       "      <td>1.434178e-06</td>\n",
       "      <td>8.685702e-04</td>\n",
       "      <td>1.100743e-06</td>\n",
       "      <td>3.044588e-03</td>\n",
       "      <td>6.567472e-08</td>\n",
       "      <td>8.927088e-04</td>\n",
       "      <td>1.204468e-02</td>\n",
       "      <td>2.228847e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3128</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.345231e-05</td>\n",
       "      <td>1.176274e-04</td>\n",
       "      <td>1.757207e-05</td>\n",
       "      <td>1.440693e-04</td>\n",
       "      <td>5.411918e-06</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>1.369785e-03</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>1.052319e-04</td>\n",
       "      <td>2.757647e-06</td>\n",
       "      <td>2.441343e-06</td>\n",
       "      <td>1.215714e-05</td>\n",
       "      <td>7.255679e-04</td>\n",
       "      <td>1.776193e-06</td>\n",
       "      <td>4.248767e-06</td>\n",
       "      <td>1.051885e-05</td>\n",
       "      <td>8.755162e-06</td>\n",
       "      <td>5.609869e-06</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>1.119547e-04</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>2.999797e-06</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>1.534645e-06</td>\n",
       "      <td>1.452966e-05</td>\n",
       "      <td>1.911514e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>2.528368e-05</td>\n",
       "      <td>4.395631e-05</td>\n",
       "      <td>1.043876e-05</td>\n",
       "      <td>6.607959e-05</td>\n",
       "      <td>3.738505e-03</td>\n",
       "      <td>8.310549e-03</td>\n",
       "      <td>6.330538e-04</td>\n",
       "      <td>1.585062e-04</td>\n",
       "      <td>5.253386e-06</td>\n",
       "      <td>2.855355e-05</td>\n",
       "      <td>1.086101e-04</td>\n",
       "      <td>3.191976e-05</td>\n",
       "      <td>3.284602e-06</td>\n",
       "      <td>5.761672e-05</td>\n",
       "      <td>2.509627e-05</td>\n",
       "      <td>5.781790e-06</td>\n",
       "      <td>7.223898e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3129</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>2.595292e-06</td>\n",
       "      <td>2.950884e-04</td>\n",
       "      <td>2.631509e-04</td>\n",
       "      <td>2.281394e-05</td>\n",
       "      <td>7.910090e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.282780e-05</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>6.662969e-06</td>\n",
       "      <td>1.271762e-06</td>\n",
       "      <td>3.870370e-07</td>\n",
       "      <td>1.917479e-05</td>\n",
       "      <td>4.299077e-04</td>\n",
       "      <td>9.659639e-06</td>\n",
       "      <td>2.665155e-06</td>\n",
       "      <td>5.571738e-05</td>\n",
       "      <td>2.121639e-06</td>\n",
       "      <td>4.022555e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.395142e-04</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>4.269365e-07</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>5.161713e-06</td>\n",
       "      <td>6.367538e-07</td>\n",
       "      <td>5.421267e-06</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>8.105874e-08</td>\n",
       "      <td>1.194872e-06</td>\n",
       "      <td>3.231482e-06</td>\n",
       "      <td>9.941075e-07</td>\n",
       "      <td>3.176446e-04</td>\n",
       "      <td>9.502537e-04</td>\n",
       "      <td>5.820632e-04</td>\n",
       "      <td>5.014890e-06</td>\n",
       "      <td>8.111192e-07</td>\n",
       "      <td>1.425431e-05</td>\n",
       "      <td>2.082707e-06</td>\n",
       "      <td>2.616074e-06</td>\n",
       "      <td>4.458506e-07</td>\n",
       "      <td>2.302143e-03</td>\n",
       "      <td>8.492467e-05</td>\n",
       "      <td>2.439178e-06</td>\n",
       "      <td>3.807932e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>3902</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>2.692790e-07</td>\n",
       "      <td>1.392818e-04</td>\n",
       "      <td>3.632546e-04</td>\n",
       "      <td>3.666391e-05</td>\n",
       "      <td>2.516073e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5.034942e-05</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>5.608847e-07</td>\n",
       "      <td>8.443988e-08</td>\n",
       "      <td>4.424959e-08</td>\n",
       "      <td>3.241649e-06</td>\n",
       "      <td>5.664256e-04</td>\n",
       "      <td>1.966830e-05</td>\n",
       "      <td>4.358430e-06</td>\n",
       "      <td>1.377685e-04</td>\n",
       "      <td>1.753900e-06</td>\n",
       "      <td>9.672356e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.599161e-04</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.128838e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.651635e-07</td>\n",
       "      <td>1.488061e-06</td>\n",
       "      <td>1.090116e-05</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8.845261e-08</td>\n",
       "      <td>5.673692e-07</td>\n",
       "      <td>2.185301e-06</td>\n",
       "      <td>7.444148e-07</td>\n",
       "      <td>2.017738e-04</td>\n",
       "      <td>3.374573e-04</td>\n",
       "      <td>1.702022e-03</td>\n",
       "      <td>2.183509e-07</td>\n",
       "      <td>3.351120e-08</td>\n",
       "      <td>2.855817e-06</td>\n",
       "      <td>4.701814e-07</td>\n",
       "      <td>2.122393e-07</td>\n",
       "      <td>2.414436e-07</td>\n",
       "      <td>1.602291e-02</td>\n",
       "      <td>2.339808e-05</td>\n",
       "      <td>2.259146e-06</td>\n",
       "      <td>5.595793e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>3903</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2.418103e-06</td>\n",
       "      <td>2.474411e-04</td>\n",
       "      <td>7.793277e-05</td>\n",
       "      <td>3.450090e-05</td>\n",
       "      <td>3.203852e-08</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.091073e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.682169e-07</td>\n",
       "      <td>1.466343e-08</td>\n",
       "      <td>1.076064e-07</td>\n",
       "      <td>2.007611e-07</td>\n",
       "      <td>1.504268e-04</td>\n",
       "      <td>3.400251e-05</td>\n",
       "      <td>3.794539e-06</td>\n",
       "      <td>1.547392e-04</td>\n",
       "      <td>1.618792e-06</td>\n",
       "      <td>2.004600e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.284377e-03</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.070109e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.374649e-08</td>\n",
       "      <td>1.038312e-05</td>\n",
       "      <td>3.036143e-05</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.591797e-08</td>\n",
       "      <td>4.240455e-07</td>\n",
       "      <td>1.551022e-06</td>\n",
       "      <td>1.363590e-07</td>\n",
       "      <td>1.432590e-05</td>\n",
       "      <td>1.126615e-04</td>\n",
       "      <td>8.413609e-05</td>\n",
       "      <td>4.325434e-07</td>\n",
       "      <td>7.853837e-09</td>\n",
       "      <td>1.182301e-05</td>\n",
       "      <td>1.135056e-06</td>\n",
       "      <td>1.329223e-06</td>\n",
       "      <td>6.008780e-07</td>\n",
       "      <td>3.952625e-03</td>\n",
       "      <td>5.511012e-06</td>\n",
       "      <td>6.515145e-07</td>\n",
       "      <td>3.573783e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>3904</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.099465e-05</td>\n",
       "      <td>9.337877e-05</td>\n",
       "      <td>1.634578e-05</td>\n",
       "      <td>8.988440e-05</td>\n",
       "      <td>1.229814e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>2.575660e-05</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>7.194992e-06</td>\n",
       "      <td>3.232979e-07</td>\n",
       "      <td>5.068840e-07</td>\n",
       "      <td>7.551430e-06</td>\n",
       "      <td>1.616150e-04</td>\n",
       "      <td>8.896222e-07</td>\n",
       "      <td>4.917642e-07</td>\n",
       "      <td>3.038611e-06</td>\n",
       "      <td>8.085279e-07</td>\n",
       "      <td>4.941950e-06</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.765404e-04</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>4.938819e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.427270e-07</td>\n",
       "      <td>2.785574e-06</td>\n",
       "      <td>5.802608e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>8.832764e-07</td>\n",
       "      <td>4.378901e-06</td>\n",
       "      <td>1.285613e-06</td>\n",
       "      <td>8.524936e-06</td>\n",
       "      <td>4.783742e-04</td>\n",
       "      <td>2.380906e-03</td>\n",
       "      <td>1.889450e-04</td>\n",
       "      <td>1.027952e-05</td>\n",
       "      <td>3.110814e-07</td>\n",
       "      <td>1.133624e-05</td>\n",
       "      <td>1.826907e-05</td>\n",
       "      <td>1.181306e-05</td>\n",
       "      <td>3.889178e-07</td>\n",
       "      <td>7.185910e-05</td>\n",
       "      <td>1.804710e-05</td>\n",
       "      <td>1.360694e-06</td>\n",
       "      <td>7.549015e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>3905</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>9.349877e-07</td>\n",
       "      <td>2.897043e-09</td>\n",
       "      <td>1.360930e-08</td>\n",
       "      <td>2.737833e-08</td>\n",
       "      <td>1.765427e-02</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.666289e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.614539e-05</td>\n",
       "      <td>3.251577e-09</td>\n",
       "      <td>2.272466e-04</td>\n",
       "      <td>7.495165e-08</td>\n",
       "      <td>4.059368e-07</td>\n",
       "      <td>1.741021e-05</td>\n",
       "      <td>4.617530e-07</td>\n",
       "      <td>1.403474e-09</td>\n",
       "      <td>3.055238e-08</td>\n",
       "      <td>4.119734e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.097873e-09</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>9.801980e-01</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.178972e-07</td>\n",
       "      <td>1.237006e-05</td>\n",
       "      <td>4.006409e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2.805889e-05</td>\n",
       "      <td>3.204079e-04</td>\n",
       "      <td>2.950669e-07</td>\n",
       "      <td>4.707098e-04</td>\n",
       "      <td>5.880752e-07</td>\n",
       "      <td>8.497389e-07</td>\n",
       "      <td>5.535009e-10</td>\n",
       "      <td>4.641344e-07</td>\n",
       "      <td>5.805668e-09</td>\n",
       "      <td>7.169170e-09</td>\n",
       "      <td>2.958758e-05</td>\n",
       "      <td>7.718703e-08</td>\n",
       "      <td>1.964780e-06</td>\n",
       "      <td>6.251173e-12</td>\n",
       "      <td>1.060830e-07</td>\n",
       "      <td>1.251617e-05</td>\n",
       "      <td>4.916669e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>3906</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.097097e-06</td>\n",
       "      <td>1.333163e-04</td>\n",
       "      <td>3.200663e-05</td>\n",
       "      <td>2.775637e-05</td>\n",
       "      <td>9.644702e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.585497e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.416702e-07</td>\n",
       "      <td>6.410163e-08</td>\n",
       "      <td>1.649266e-07</td>\n",
       "      <td>4.274219e-06</td>\n",
       "      <td>4.257003e-05</td>\n",
       "      <td>8.205339e-06</td>\n",
       "      <td>7.160952e-07</td>\n",
       "      <td>1.318424e-05</td>\n",
       "      <td>4.357608e-07</td>\n",
       "      <td>7.028999e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>7.395596e-04</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.079032e-08</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.189728e-08</td>\n",
       "      <td>4.557222e-06</td>\n",
       "      <td>2.222234e-05</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>8.976022e-08</td>\n",
       "      <td>8.475187e-07</td>\n",
       "      <td>1.047357e-06</td>\n",
       "      <td>1.614747e-07</td>\n",
       "      <td>2.474583e-05</td>\n",
       "      <td>1.472671e-04</td>\n",
       "      <td>8.589283e-05</td>\n",
       "      <td>1.351910e-07</td>\n",
       "      <td>1.652482e-08</td>\n",
       "      <td>2.854900e-06</td>\n",
       "      <td>9.722719e-07</td>\n",
       "      <td>3.358905e-07</td>\n",
       "      <td>1.054960e-07</td>\n",
       "      <td>2.254390e-03</td>\n",
       "      <td>9.878531e-05</td>\n",
       "      <td>9.046755e-07</td>\n",
       "      <td>1.539940e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         0         1             2             3             4  \\\n",
       "0    3125  0.000011  0.000002  2.116810e-07  4.708937e-08  1.865657e-04   \n",
       "1    3126  0.000413  0.000020  1.225638e-04  1.330648e-03  5.290742e-05   \n",
       "2    3127  0.001868  0.032602  1.620008e-05  1.283705e-05  3.405659e-05   \n",
       "3    3128  0.000691  0.000010  3.345231e-05  1.176274e-04  1.757207e-05   \n",
       "4    3129  0.004222  0.000031  2.595292e-06  2.950884e-04  2.631509e-04   \n",
       "..    ...       ...       ...           ...           ...           ...   \n",
       "777  3902  0.007722  0.000045  2.692790e-07  1.392818e-04  3.632546e-04   \n",
       "778  3903  0.000128  0.000006  2.418103e-06  2.474411e-04  7.793277e-05   \n",
       "779  3904  0.000417  0.000003  1.099465e-05  9.337877e-05  1.634578e-05   \n",
       "780  3905  0.000004  0.000872  9.349877e-07  2.897043e-09  1.360930e-08   \n",
       "781  3906  0.000093  0.000004  1.097097e-06  1.333163e-04  3.200663e-05   \n",
       "\n",
       "                5             6         7             8         9        10  \\\n",
       "0    1.124369e-07  3.247155e-04  0.000002  1.424222e-07  0.005938  0.100756   \n",
       "1    6.115026e-04  2.825133e-06  0.000066  7.107466e-06  0.000011  0.000024   \n",
       "2    6.504179e-04  1.054489e-01  0.000152  1.890009e-05  0.000041  0.002011   \n",
       "3    1.440693e-04  5.411918e-06  0.000118  1.369785e-03  0.000056  0.000156   \n",
       "4    2.281394e-05  7.910090e-07  0.000003  2.282780e-05  0.000076  0.000025   \n",
       "..            ...           ...       ...           ...       ...       ...   \n",
       "777  3.666391e-05  2.516073e-07  0.000004  5.034942e-05  0.000009  0.000006   \n",
       "778  3.450090e-05  3.203852e-08  0.000003  1.091073e-06  0.000002  0.000001   \n",
       "779  8.988440e-05  1.229814e-06  0.000017  2.575660e-05  0.000022  0.000035   \n",
       "780  2.737833e-08  1.765427e-02  0.000002  1.666289e-08  0.000002  0.000002   \n",
       "781  2.775637e-05  9.644702e-08  0.000002  2.585497e-07  0.000002  0.000010   \n",
       "\n",
       "               11            12            13            14            15  \\\n",
       "0    4.794668e-01  1.128225e-04  3.902865e-01  2.294270e-03  3.994864e-06   \n",
       "1    1.578770e-06  1.445539e-08  2.135061e-06  1.027968e-06  2.940293e-04   \n",
       "2    4.135342e-04  3.376489e-05  2.495863e-03  1.967650e-02  8.969253e-04   \n",
       "3    1.052319e-04  2.757647e-06  2.441343e-06  1.215714e-05  7.255679e-04   \n",
       "4    6.662969e-06  1.271762e-06  3.870370e-07  1.917479e-05  4.299077e-04   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "777  5.608847e-07  8.443988e-08  4.424959e-08  3.241649e-06  5.664256e-04   \n",
       "778  2.682169e-07  1.466343e-08  1.076064e-07  2.007611e-07  1.504268e-04   \n",
       "779  7.194992e-06  3.232979e-07  5.068840e-07  7.551430e-06  1.616150e-04   \n",
       "780  1.614539e-05  3.251577e-09  2.272466e-04  7.495165e-08  4.059368e-07   \n",
       "781  3.416702e-07  6.410163e-08  1.649266e-07  4.274219e-06  4.257003e-05   \n",
       "\n",
       "               16            17            18            19            20  \\\n",
       "0    2.203607e-06  7.347716e-07  4.205005e-07  9.210566e-08  3.008443e-05   \n",
       "1    2.704988e-05  1.144425e-05  2.083481e-05  4.048173e-06  2.643172e-05   \n",
       "2    1.574834e-04  1.690211e-04  4.927005e-05  2.873521e-04  2.336423e-04   \n",
       "3    1.776193e-06  4.248767e-06  1.051885e-05  8.755162e-06  5.609869e-06   \n",
       "4    9.659639e-06  2.665155e-06  5.571738e-05  2.121639e-06  4.022555e-06   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "777  1.966830e-05  4.358430e-06  1.377685e-04  1.753900e-06  9.672356e-07   \n",
       "778  3.400251e-05  3.794539e-06  1.547392e-04  1.618792e-06  2.004600e-06   \n",
       "779  8.896222e-07  4.917642e-07  3.038611e-06  8.085279e-07  4.941950e-06   \n",
       "780  1.741021e-05  4.617530e-07  1.403474e-09  3.055238e-08  4.119734e-07   \n",
       "781  8.205339e-06  7.160952e-07  1.318424e-05  4.357608e-07  7.028999e-07   \n",
       "\n",
       "           21            22        23  ...        36            37        38  \\\n",
       "0    0.000004  7.120603e-07  0.005244  ...  0.001091  9.638647e-04  0.000089   \n",
       "1    0.000180  2.340468e-03  0.000217  ...  0.000310  4.793614e-06  0.000002   \n",
       "2    0.000258  5.826922e-07  0.000569  ...  0.000390  4.887589e-02  0.006375   \n",
       "3    0.000086  1.119547e-04  0.000133  ...  0.000285  2.999797e-06  0.000039   \n",
       "4    0.000007  3.395142e-04  0.000008  ...  0.000016  4.269365e-07  0.000011   \n",
       "..        ...           ...       ...  ...       ...           ...       ...   \n",
       "777  0.000002  2.599161e-04  0.000001  ...  0.000004  1.128838e-07  0.000002   \n",
       "778  0.000002  1.284377e-03  0.000007  ...  0.000003  7.070109e-08  0.000002   \n",
       "779  0.000040  1.765404e-04  0.000032  ...  0.000110  4.938819e-07  0.000003   \n",
       "780  0.000003  1.097873e-09  0.000007  ...  0.000006  9.801980e-01  0.000001   \n",
       "781  0.000001  7.395596e-04  0.000014  ...  0.000004  9.079032e-08  0.000003   \n",
       "\n",
       "               39            40            41        42        43  \\\n",
       "0    1.705775e-05  2.177705e-07  6.212062e-09  0.000386  0.000093   \n",
       "1    3.656085e-08  4.648645e-04  3.747970e-04  0.000015  0.000007   \n",
       "2    3.705476e-04  2.303389e-04  1.743668e-04  0.063292  0.195926   \n",
       "3    1.534645e-06  1.452966e-05  1.911514e-05  0.000014  0.000024   \n",
       "4    5.161713e-06  6.367538e-07  5.421267e-06  0.000096  0.000048   \n",
       "..            ...           ...           ...       ...       ...   \n",
       "777  2.651635e-07  1.488061e-06  1.090116e-05  0.000015  0.000004   \n",
       "778  2.374649e-08  1.038312e-05  3.036143e-05  0.000024  0.000007   \n",
       "779  5.427270e-07  2.785574e-06  5.802608e-06  0.000009  0.000012   \n",
       "780  3.178972e-07  1.237006e-05  4.006409e-08  0.000007  0.000009   \n",
       "781  2.189728e-08  4.557222e-06  2.222234e-05  0.000060  0.000014   \n",
       "\n",
       "               44            45            46            47            48  \\\n",
       "0    3.137485e-05  8.554223e-04  3.529570e-05  1.155294e-07  3.284435e-06   \n",
       "1    1.712846e-06  4.472250e-06  8.808629e-06  1.643756e-05  1.192999e-04   \n",
       "2    7.786207e-03  4.333588e-01  2.320479e-04  2.547552e-03  2.427308e-03   \n",
       "3    2.528368e-05  4.395631e-05  1.043876e-05  6.607959e-05  3.738505e-03   \n",
       "4    8.105874e-08  1.194872e-06  3.231482e-06  9.941075e-07  3.176446e-04   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "777  8.845261e-08  5.673692e-07  2.185301e-06  7.444148e-07  2.017738e-04   \n",
       "778  3.591797e-08  4.240455e-07  1.551022e-06  1.363590e-07  1.432590e-05   \n",
       "779  8.832764e-07  4.378901e-06  1.285613e-06  8.524936e-06  4.783742e-04   \n",
       "780  2.805889e-05  3.204079e-04  2.950669e-07  4.707098e-04  5.880752e-07   \n",
       "781  8.976022e-08  8.475187e-07  1.047357e-06  1.614747e-07  2.474583e-05   \n",
       "\n",
       "               49            50            51            52            53  \\\n",
       "0    2.174506e-06  3.665759e-08  4.478506e-04  4.650679e-04  1.211206e-05   \n",
       "1    2.324675e-03  6.748711e-04  7.841913e-06  1.797475e-08  9.208373e-05   \n",
       "2    4.264420e-03  3.207233e-06  4.141561e-05  2.466323e-05  1.434178e-06   \n",
       "3    8.310549e-03  6.330538e-04  1.585062e-04  5.253386e-06  2.855355e-05   \n",
       "4    9.502537e-04  5.820632e-04  5.014890e-06  8.111192e-07  1.425431e-05   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "777  3.374573e-04  1.702022e-03  2.183509e-07  3.351120e-08  2.855817e-06   \n",
       "778  1.126615e-04  8.413609e-05  4.325434e-07  7.853837e-09  1.182301e-05   \n",
       "779  2.380906e-03  1.889450e-04  1.027952e-05  3.110814e-07  1.133624e-05   \n",
       "780  8.497389e-07  5.535009e-10  4.641344e-07  5.805668e-09  7.169170e-09   \n",
       "781  1.472671e-04  8.589283e-05  1.351910e-07  1.652482e-08  2.854900e-06   \n",
       "\n",
       "               54            55            56            57            58  \\\n",
       "0    2.320302e-06  1.255611e-07  2.459125e-07  8.899527e-10  6.392489e-03   \n",
       "1    1.928369e-04  1.234607e-04  4.037772e-06  1.639472e-04  8.937501e-06   \n",
       "2    8.685702e-04  1.100743e-06  3.044588e-03  6.567472e-08  8.927088e-04   \n",
       "3    1.086101e-04  3.191976e-05  3.284602e-06  5.761672e-05  2.509627e-05   \n",
       "4    2.082707e-06  2.616074e-06  4.458506e-07  2.302143e-03  8.492467e-05   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "777  4.701814e-07  2.122393e-07  2.414436e-07  1.602291e-02  2.339808e-05   \n",
       "778  1.135056e-06  1.329223e-06  6.008780e-07  3.952625e-03  5.511012e-06   \n",
       "779  1.826907e-05  1.181306e-05  3.889178e-07  7.185910e-05  1.804710e-05   \n",
       "780  2.958758e-05  7.718703e-08  1.964780e-06  6.251173e-12  1.060830e-07   \n",
       "781  9.722719e-07  3.358905e-07  1.054960e-07  2.254390e-03  9.878531e-05   \n",
       "\n",
       "               59            60  \n",
       "0    1.507664e-05  3.401015e-06  \n",
       "1    2.063025e-05  1.227229e-05  \n",
       "2    1.204468e-02  2.228847e-03  \n",
       "3    5.781790e-06  7.223898e-03  \n",
       "4    2.439178e-06  3.807932e-03  \n",
       "..            ...           ...  \n",
       "777  2.259146e-06  5.595793e-03  \n",
       "778  6.515145e-07  3.573783e-05  \n",
       "779  1.360694e-06  7.549015e-04  \n",
       "780  1.251617e-05  4.916669e-09  \n",
       "781  9.046755e-07  1.539940e-04  \n",
       "\n",
       "[782 rows x 62 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission=pd.read_csv('./data/sample_submission.csv')\n",
    "submission.iloc[:,1:]=pred\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub_kfold_stratified_10_adam_fft_0.5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
